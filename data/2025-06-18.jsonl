{"id": "2506.13796", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.13796", "abs": "https://arxiv.org/abs/2506.13796", "authors": ["Zhou Chen", "Xiao Wang", "Yuanhong Liao", "Ming Lin", "Yuqi Bai"], "title": "ClimateChat: Designing Data and Methods for Instruction Tuning LLMs to Answer Climate Change Queries", "comment": "ICLR 2025 camera ready, 13 pages, 4 figures, 4 tables", "summary": "As the issue of global climate change becomes increasingly severe, the demand\nfor research in climate science continues to grow. Natural language processing\ntechnologies, represented by Large Language Models (LLMs), have been widely\napplied to climate change-specific research, providing essential information\nsupport for decision-makers and the public. Some studies have improved model\nperformance on relevant tasks by constructing climate change-related\ninstruction data and instruction-tuning LLMs. However, current research remains\ninadequate in efficiently producing large volumes of high-precision instruction\ndata for climate change, which limits further development of climate change\nLLMs. This study introduces an automated method for constructing instruction\ndata. The method generates instructions using facts and background knowledge\nfrom documents and enhances the diversity of the instruction data through web\nscraping and the collection of seed instructions. Using this method, we\nconstructed a climate change instruction dataset, named ClimateChat-Corpus,\nwhich was used to fine-tune open-source LLMs, resulting in an LLM named\nClimateChat. Evaluation results show that ClimateChat significantly improves\nperformance on climate change question-and-answer tasks. Additionally, we\nevaluated the impact of different base models and instruction data on LLM\nperformance and demonstrated its capability to adapt to a wide range of climate\nchange scientific discovery tasks, emphasizing the importance of selecting an\nappropriate base model for instruction tuning. This research provides valuable\nreferences and empirical support for constructing climate change instruction\ndata and training climate change-specific LLMs."}
{"id": "2506.13886", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.13886", "abs": "https://arxiv.org/abs/2506.13886", "authors": ["Antara Raaghavi Bhattacharya", "Isabel Papadimitriou", "Kathryn Davidson", "David Alvarez-Melis"], "title": "Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles", "comment": null, "summary": "Across languages, numeral systems vary widely in how they construct and\ncombine numbers. While humans consistently learn to navigate this diversity,\nlarge language models (LLMs) struggle with linguistic-mathematical puzzles\ninvolving cross-linguistic numeral systems, which humans can learn to solve\nsuccessfully. We investigate why this task is difficult for LLMs through a\nseries of experiments that untangle the linguistic and mathematical aspects of\nnumbers in language. Our experiments establish that models cannot consistently\nsolve such problems unless the mathematical operations in the problems are\nexplicitly marked using known symbols ($+$, $\\times$, etc, as in \"twenty +\nthree\"). In further ablation studies, we probe how individual parameters of\nnumeral construction and combination affect performance. While humans use their\nlinguistic understanding of numbers to make inferences about the implicit\ncompositional structure of numerals, LLMs seem to lack this notion of implicit\nnumeral structure. We conclude that the ability to flexibly infer compositional\nrules from implicit patterns in human-scale data remains an open challenge for\ncurrent reasoning models."}
{"id": "2506.13888", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.13888", "abs": "https://arxiv.org/abs/2506.13888", "authors": ["Jipeng Zhang", "Kehao Miao", "Renjie Pi", "Zhaowei Wang", "Runtao Liu", "Rui Pan", "Tong Zhang"], "title": "VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative Training", "comment": null, "summary": "Reinforcement Fine-Tuning (RFT) with verifiable rewards has advanced large\nlanguage models but remains underexplored for Vision-Language (VL) models. The\nVision-Language Reward Model (VL-RM) is key to aligning VL models by providing\nstructured feedback, yet training effective VL-RMs faces two major challenges.\nFirst, the bootstrapping dilemma arises as high-quality training data depends\non already strong VL models, creating a cycle where self-generated supervision\nreinforces existing biases. Second, modality bias and negative example\namplification occur when VL models hallucinate incorrect visual attributes,\nleading to flawed preference data that further misguides training. To address\nthese issues, we propose an iterative training framework leveraging vision\nexperts, Chain-of-Thought (CoT) rationales, and Margin-based Rejection\nSampling. Our approach refines preference datasets, enhances structured\ncritiques, and iteratively improves reasoning. Experiments across VL-RM\nbenchmarks demonstrate superior performance in hallucination detection and\nmultimodal reasoning, advancing VL model alignment with reinforcement learning."}
{"id": "2506.13894", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.13894", "abs": "https://arxiv.org/abs/2506.13894", "authors": ["Ryuki Matsuura", "Shikhar Bharadwaj", "Jiarui Liu", "Dhatchi Kunde Govindarajan"], "title": "EmoNews: A Spoken Dialogue System for Expressive News Conversations", "comment": null, "summary": "We develop a task-oriented spoken dialogue system (SDS) that regulates\nemotional speech based on contextual cues to enable more empathetic news\nconversations. Despite advancements in emotional text-to-speech (TTS)\ntechniques, task-oriented emotional SDSs remain underexplored due to the\ncompartmentalized nature of SDS and emotional TTS research, as well as the lack\nof standardized evaluation metrics for social goals. We address these\nchallenges by developing an emotional SDS for news conversations that utilizes\na large language model (LLM)-based sentiment analyzer to identify appropriate\nemotions and PromptTTS to synthesize context-appropriate emotional speech. We\nalso propose subjective evaluation scale for emotional SDSs and judge the\nemotion regulation performance of the proposed and baseline systems.\nExperiments showed that our emotional SDS outperformed a baseline system in\nterms of the emotion regulation and engagement. These results suggest the\ncritical role of speech emotion for more engaging conversations. All our source\ncode is open-sourced at\nhttps://github.com/dhatchi711/espnet-emotional-news/tree/emo-sds/egs2/emo_news_sds/sds1"}
{"id": "2506.13769", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.13769", "abs": "https://arxiv.org/abs/2506.13769", "authors": ["Filippo Leveni"], "title": "Non-planar Object Detection and Identification by Features Matching and Triangulation Growth", "comment": "Master's thesis at Politecnico di Milano", "summary": "Object detection and identification is surely a fundamental topic in the\ncomputer vision field; it plays a crucial role in many applications such as\nobject tracking, industrial robots control, image retrieval, etc. We propose a\nfeature-based approach for detecting and identifying distorted occurrences of a\ngiven template in a scene image by incremental grouping of feature matches\nbetween the image and the template. For this purpose, we consider the Delaunay\ntriangulation of template features as an useful tool through which to be guided\nin this iterative approach. The triangulation is treated as a graph and,\nstarting from a single triangle, neighboring nodes are considered and the\ncorresponding features are identified; then matches related to them are\nevaluated to determine if they are worthy to be grouped. This evaluation is\nbased on local consistency criteria derived from geometric and photometric\nproperties of local features. Our solution allows the identification of the\nobject in situations where geometric models (e.g. homography) does not hold,\nthus enable the detection of objects such that the template is non planar or\nwhen it is planar but appears distorted in the image. We show that our approach\nperforms just as well or better than application of homography-based RANSAC in\nscenarios in which distortion is nearly absent, while when the deformation\nbecomes relevant our method shows better description performance."}
{"id": "2506.13901", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.13901", "abs": "https://arxiv.org/abs/2506.13901", "authors": ["Abhilekh Borah", "Chhavi Sharma", "Danush Khanna", "Utkarsh Bhatt", "Gurpreet Singh", "Hasnat Md Abdullah", "Raghav Kaushik Ravi", "Vinija Jain", "Jyoti Patel", "Shubham Singh", "Vasu Sharma", "Arpita Vats", "Rahul Raja", "Aman Chadha", "Amitava Das"], "title": "Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise Pooled Representations", "comment": null, "summary": "Alignment is no longer a luxury, it is a necessity. As large language models\n(LLMs) enter high-stakes domains like education, healthcare, governance, and\nlaw, their behavior must reliably reflect human-aligned values and safety\nconstraints. Yet current evaluations rely heavily on behavioral proxies such as\nrefusal rates, G-Eval scores, and toxicity classifiers, all of which have\ncritical blind spots. Aligned models are often vulnerable to jailbreaking,\nstochasticity of generation, and alignment faking.\n  To address this issue, we introduce the Alignment Quality Index (AQI). This\nnovel geometric and prompt-invariant metric empirically assesses LLM alignment\nby analyzing the separation of safe and unsafe activations in latent space. By\ncombining measures such as the Davies-Bouldin Score (DBS), Dunn Index (DI),\nXie-Beni Index (XBI), and Calinski-Harabasz Index (CHI) across various\nformulations, AQI captures clustering quality to detect hidden misalignments\nand jailbreak risks, even when outputs appear compliant. AQI also serves as an\nearly warning signal for alignment faking, offering a robust, decoding\ninvariant tool for behavior agnostic safety auditing.\n  Additionally, we propose the LITMUS dataset to facilitate robust evaluation\nunder these challenging conditions. Empirical tests on LITMUS across different\nmodels trained under DPO, GRPO, and RLHF conditions demonstrate AQI's\ncorrelation with external judges and ability to reveal vulnerabilities missed\nby refusal metrics. We make our implementation publicly available to foster\nfuture research in this area."}
{"id": "2506.13770", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.13770", "abs": "https://arxiv.org/abs/2506.13770", "authors": ["Shiwen Zhang", "Zhuowei Chen", "Lang Chen", "Yanze Wu"], "title": "CDST: Color Disentangled Style Transfer for Universal Style Reference Customization", "comment": "codes and models will be released if the paper is accepted", "summary": "We introduce Color Disentangled Style Transfer (CDST), a novel and efficient\ntwo-stream style transfer training paradigm which completely isolates color\nfrom style and forces the style stream to be color-blinded. With one same\nmodel, CDST unlocks universal style transfer capabilities in a tuning-free\nmanner during inference. Especially, the characteristics-preserved style\ntransfer with style and content references is solved in the tuning-free way for\nthe first time. CDST significantly improves the style similarity by\nmulti-feature image embeddings compression and preserves strong editing\ncapability via our new CDST style definition inspired by Diffusion UNet\ndisentanglement law. By conducting thorough qualitative and quantitative\nexperiments and human evaluations, we demonstrate that CDST achieves\nstate-of-the-art results on various style transfer tasks."}
{"id": "2506.13956", "categories": ["cs.CL", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.13956", "abs": "https://arxiv.org/abs/2506.13956", "authors": ["Shang-Chi Tsai", "Seiya Kawano", "Angel Garcia Contreras", "Koichiro Yoshino", "Yun-Nung Chen"], "title": "ASMR: Augmenting Life Scenario using Large Generative Models for Robotic Action Reflection", "comment": "IWSDS 2024 Best Paper Award", "summary": "When designing robots to assist in everyday human activities, it is crucial\nto enhance user requests with visual cues from their surroundings for improved\nintent understanding. This process is defined as a multimodal classification\ntask. However, gathering a large-scale dataset encompassing both visual and\nlinguistic elements for model training is challenging and time-consuming. To\naddress this issue, our paper introduces a novel framework focusing on data\naugmentation in robotic assistance scenarios, encompassing both dialogues and\nrelated environmental imagery. This approach involves leveraging a\nsophisticated large language model to simulate potential conversations and\nenvironmental contexts, followed by the use of a stable diffusion model to\ncreate images depicting these environments. The additionally generated data\nserves to refine the latest multimodal models, enabling them to more accurately\ndetermine appropriate actions in response to user interactions with the limited\ntarget data. Our experimental results, based on a dataset collected from\nreal-world scenarios, demonstrate that our methodology significantly enhances\nthe robot's action selection capabilities, achieving the state-of-the-art\nperformance."}
{"id": "2506.13780", "categories": ["cs.CV", "cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.13780", "abs": "https://arxiv.org/abs/2506.13780", "authors": ["Sedat Porikli", "Vedat Porikli"], "title": "Hidden Bias in the Machine: Stereotypes in Text-to-Image Models", "comment": "Equal contribution by both authors, Published at CVPR 2025 Workshop\n  on Experimental Model Auditing via Controllable Synthesis (EMACS) and\n  Workshop on Demographic Diversity in Computer Vision (DemoDiv)", "summary": "Text-to-Image (T2I) models have transformed visual content creation,\nproducing highly realistic images from natural language prompts. However,\nconcerns persist around their potential to replicate and magnify existing\nsocietal biases. To investigate these issues, we curated a diverse set of\nprompts spanning thematic categories such as occupations, traits, actions,\nideologies, emotions, family roles, place descriptions, spirituality, and life\nevents. For each of the 160 unique topics, we crafted multiple prompt\nvariations to reflect a wide range of meanings and perspectives. Using Stable\nDiffusion 1.5 (UNet-based) and Flux-1 (DiT-based) models with original\ncheckpoints, we generated over 16,000 images under consistent settings.\nAdditionally, we collected 8,000 comparison images from Google Image Search.\nAll outputs were filtered to exclude abstract, distorted, or nonsensical\nresults. Our analysis reveals significant disparities in the representation of\ngender, race, age, somatotype, and other human-centric factors across generated\nimages. These disparities often mirror and reinforce harmful stereotypes\nembedded in societal narratives. We discuss the implications of these findings\nand emphasize the need for more inclusive datasets and development practices to\nfoster fairness in generative visual systems."}
{"id": "2506.13965", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.13965", "abs": "https://arxiv.org/abs/2506.13965", "authors": ["Aleksander Smywiński-Pohl", "Tomer Libal", "Adam Kaczmarczyk", "Magdalena Król"], "title": "Are manual annotations necessary for statutory interpretations retrieval?", "comment": null, "summary": "One of the elements of legal research is looking for cases where judges have\nextended the meaning of a legal concept by providing interpretations of what a\nconcept means or does not mean. This allow legal professionals to use such\ninterpretations as precedents as well as laymen to better understand the legal\nconcept. The state-of-the-art approach for retrieving the most relevant\ninterpretations for these concepts currently depends on the ranking of\nsentences and the training of language models over annotated examples. That\nmanual annotation process can be quite expensive and need to be repeated for\neach such concept, which prompted recent research in trying to automate this\nprocess. In this paper, we highlight the results of various experiments\nconducted to determine the volume, scope and even the need for manual\nannotation. First of all, we check what is the optimal number of annotations\nper a legal concept. Second, we check if we can draw the sentences for\nannotation randomly or there is a gain in the performance of the model, when\nonly the best candidates are annotated. As the last question we check what is\nthe outcome of automating the annotation process with the help of an LLM."}
{"id": "2506.13846", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.13846", "abs": "https://arxiv.org/abs/2506.13846", "authors": ["Runtao Liu", "Jiahao Zhan", "Yingqing He", "Chen Wei", "Alan Yuille", "Qifeng Chen"], "title": "Fake it till You Make it: Reward Modeling as Discriminative Prediction", "comment": null, "summary": "An effective reward model plays a pivotal role in reinforcement learning for\npost-training enhancement of visual generative models. However, current\napproaches of reward modeling suffer from implementation complexity due to\ntheir reliance on extensive human-annotated preference data or meticulously\nengineered quality dimensions that are often incomplete and\nengineering-intensive. Inspired by adversarial training in generative\nadversarial networks (GANs), this paper proposes GAN-RM, an efficient reward\nmodeling framework that eliminates manual preference annotation and explicit\nquality dimension engineering. Our method trains the reward model through\ndiscrimination between a small set of representative, unpaired target\nsamples(denoted as Preference Proxy Data) and model-generated ordinary outputs,\nrequiring only a few hundred target samples. Comprehensive experiments\ndemonstrate our GAN-RM's effectiveness across multiple key applications\nincluding test-time scaling implemented as Best-of-N sample filtering,\npost-training approaches like Supervised Fine-Tuning (SFT) and Direct\nPreference Optimization (DPO)."}
{"id": "2506.13978", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.13978", "abs": "https://arxiv.org/abs/2506.13978", "authors": ["Xiuwen Wu", "Hao Wang", "Zhiang Yan", "Xiaohan Tang", "Pengfei Xu", "Wai-Ting Siok", "Ping Li", "Jia-Hong Gao", "Bingjiang Lyu", "Lang Qin"], "title": "AI shares emotion with humans across languages and cultures", "comment": null, "summary": "Effective and safe human-machine collaboration requires the regulated and\nmeaningful exchange of emotions between humans and artificial intelligence\n(AI). Current AI systems based on large language models (LLMs) can provide\nfeedback that makes people feel heard. Yet it remains unclear whether LLMs\nrepresent emotion in language as humans do, or whether and how the emotional\ntone of their output can be controlled. We assess human-AI emotional alignment\nacross linguistic-cultural groups and model-families, using interpretable LLM\nfeatures translated from concept-sets for over twenty nuanced emotion\ncategories (including six basic emotions). Our analyses reveal that LLM-derived\nemotion spaces are structurally congruent with human perception, underpinned by\nthe fundamental affective dimensions of valence and arousal. Furthermore, these\nemotion-related features also accurately predict large-scale behavioural data\non word ratings along these two core dimensions, reflecting both universal and\nlanguage-specific patterns. Finally, by leveraging steering vectors derived\nsolely from human-centric emotion concepts, we show that model expressions can\nbe stably and naturally modulated across distinct emotion categories, which\nprovides causal evidence that human emotion concepts can be used to\nsystematically induce LLMs to produce corresponding affective states when\nconveying content. These findings suggest AI not only shares emotional\nrepresentations with humans but its affective outputs can be precisely guided\nusing psychologically grounded emotion concepts."}
{"id": "2506.13897", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.13897", "abs": "https://arxiv.org/abs/2506.13897", "authors": ["Thomas Kreutz", "Max Mühlhäuser", "Alejandro Sanchez Guinea"], "title": "DeSPITE: Exploring Contrastive Deep Skeleton-Pointcloud-IMU-Text Embeddings for Advanced Point Cloud Human Activity Understanding", "comment": "This work is currently under review at ICCV 2025", "summary": "Despite LiDAR (Light Detection and Ranging) being an effective\nprivacy-preserving alternative to RGB cameras to perceive human activities, it\nremains largely underexplored in the context of multi-modal contrastive\npre-training for human activity understanding (e.g., human activity recognition\n(HAR), retrieval, or person re-identification (RE-ID)). To close this gap, our\nwork explores learning the correspondence between LiDAR point clouds, human\nskeleton poses, IMU data, and text in a joint embedding space. More\nspecifically, we present DeSPITE, a Deep Skeleton-Pointcloud-IMU-Text Embedding\nmodel, which effectively learns a joint embedding space across these four\nmodalities through noise contrastive estimation. At the heart of our empirical\nexploration, we have combined the existing LIPD and Babel datasets, which\nenabled us to synchronize data of all four modalities, allowing us to explore\nthe learning of a new joint embedding space. Our experiments demonstrate novel\nhuman activity understanding tasks for point cloud sequences enabled through\nDeSPITE, including Skeleton<->Pointcloud<->IMU matching, retrieval, and\ntemporal moment retrieval. Furthermore, we show that DeSPITE is an effective\npre-training strategy for point cloud HAR through experiments in MSR-Action3D\nand HMPEAR."}
{"id": "2506.14012", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14012", "abs": "https://arxiv.org/abs/2506.14012", "authors": ["Amr Mohamed", "Yang Zhang", "Michalis Vazirgiannis", "Guokan Shang"], "title": "Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text", "comment": null, "summary": "Code-switching (CSW) is the act of alternating between two or more languages\nwithin a single discourse. This phenomenon is widespread in multilingual\ncommunities, and increasingly prevalent in online content, where users\nnaturally mix languages in everyday communication. As a result, Large Language\nModels (LLMs), now central to content processing and generation, are frequently\nexposed to code-switched inputs. Given their widespread use, it is crucial to\nunderstand how LLMs process and reason about such mixed-language text. This\npaper presents a systematic evaluation of LLM comprehension under\ncode-switching by generating CSW variants of established reasoning and\ncomprehension benchmarks. While degradation is evident when foreign tokens\ndisrupt English text$\\unicode{x2013}$even under linguistic\nconstraints$\\unicode{x2013}$embedding English into other languages often\nimproves comprehension. Though prompting yields mixed results, fine-tuning\noffers a more stable path to degradation mitigation."}
{"id": "2506.13902", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.13902", "abs": "https://arxiv.org/abs/2506.13902", "authors": ["Raymond Yu", "Paul Han", "Josh Myers-Dean", "Piper Wolters", "Favyen Bastani"], "title": "OPTIMUS: Observing Persistent Transformations in Multi-temporal Unlabeled Satellite-data", "comment": "WACV 2025", "summary": "In the face of pressing environmental issues in the 21st century, monitoring\nsurface changes on Earth is more important than ever. Large-scale remote\nsensing, such as satellite imagery, is an important tool for this task.\nHowever, using supervised methods to detect changes is difficult because of the\nlack of satellite data annotated with change labels, especially for rare\ncategories of change. Annotation proves challenging due to the sparse\noccurrence of changes in satellite images. Even within a vast collection of\nimages, only a small fraction may exhibit persistent changes of interest. To\naddress this challenge, we introduce OPTIMUS, a self-supervised learning method\nbased on an intuitive principle: if a model can recover information about the\nrelative order of images in the time series, then that implies that there are\nlong-lasting changes in the images. OPTIMUS demonstrates this principle by\nusing change point detection methods on model outputs in a time series. We\ndemonstrate that OPTIMUS can directly detect interesting changes in satellite\nimages, achieving an improvement in AUROC score from 56.3% to 87.6% at\ndistinguishing changed time series from unchanged ones compared to baselines.\nOur code and dataset are available at\nhttps://huggingface.co/datasets/optimus-change/optimus-dataset/."}
{"id": "2506.14028", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14028", "abs": "https://arxiv.org/abs/2506.14028", "authors": ["Xueqing Peng", "Lingfei Qian", "Yan Wang", "Ruoyu Xiang", "Yueru He", "Yang Ren", "Mingyang Jiang", "Jeff Zhao", "Huan He", "Yi Han", "Yun Feng", "Yuechen Jiang", "Yupeng Cao", "Haohang Li", "Yangyang Yu", "Xiaoyu Wang", "Penglei Gao", "Shengyuan Lin", "Keyi Wang", "Shanshan Yang", "Yilun Zhao", "Zhiwei Liu", "Peng Lu", "Jerry Huang", "Suyuchen Wang", "Triantafillos Papadopoulos", "Polydoros Giannouris", "Efstathia Soufleri", "Nuo Chen", "Guojun Xiong", "Zhiyang Deng", "Yijia Zhao", "Mingquan Lin", "Meikang Qiu", "Kaleb E Smith", "Arman Cohan", "Xiao-Yang Liu", "Jimin Huang", "Alejandro Lopez-Lira", "Xi Chen", "Junichi Tsujii", "Jian-Yun Nie", "Sophia Ananiadou", "Qianqian Xie"], "title": "MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark for Financial LLM Evaluation", "comment": null, "summary": "Recent advances in large language models (LLMs) have accelerated progress in\nfinancial NLP and applications, yet existing benchmarks remain limited to\nmonolingual and unimodal settings, often over-relying on simple tasks and\nfailing to reflect the complexity of real-world financial communication. We\nintroduce MultiFinBen, the first multilingual and multimodal benchmark tailored\nto the global financial domain, evaluating LLMs across modalities (text,\nvision, audio) and linguistic settings (monolingual, bilingual, multilingual)\non domain-specific tasks. We introduce two novel tasks, including PolyFiQA-Easy\nand PolyFiQA-Expert, the first multilingual financial benchmarks requiring\nmodels to perform complex reasoning over mixed-language inputs; and EnglishOCR\nand SpanishOCR, the first OCR-embedded financial QA tasks challenging models to\nextract and reason over information from visual-text financial documents.\nMoreover, we propose a dynamic, difficulty-aware selection mechanism and curate\na compact, balanced benchmark rather than simple aggregation existing datasets.\nExtensive evaluation of 22 state-of-the-art models reveals that even the\nstrongest models, despite their general multimodal and multilingual\ncapabilities, struggle dramatically when faced with complex cross-lingual and\nmultimodal tasks in financial domain. MultiFinBen is publicly released to\nfoster transparent, reproducible, and inclusive progress in financial studies\nand applications."}
{"id": "2506.13910", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.13910", "abs": "https://arxiv.org/abs/2506.13910", "authors": ["Aritra Dutta", "Pushpita Boral", "G Suseela"], "title": "Intelligent Image Sensing for Crime Analysis: A ML Approach towards Enhanced Violence Detection and Investigation", "comment": null, "summary": "The increasing global crime rate, coupled with substantial human and property\nlosses, highlights the limitations of traditional surveillance methods in\npromptly detecting diverse and unexpected acts of violence. Addressing this\npressing need for automatic violence detection, we leverage Machine Learning to\ndetect and categorize violent events in video streams. This paper introduces a\ncomprehensive framework for violence detection and classification, employing\nSupervised Learning for both binary and multi-class violence classification.\nThe detection model relies on 3D Convolutional Neural Networks, while the\nclassification model utilizes the separable convolutional 3D model for feature\nextraction and bidirectional LSTM for temporal processing. Training is\nconducted on a diverse customized datasets with frame-level annotations,\nincorporating videos from surveillance cameras, human recordings, hockey fight,\nsohas and wvd dataset across various platforms. Additionally, a camera module\nintegrated with raspberry pi is used to capture live video feed, which is sent\nto the ML model for processing. Thus, demonstrating improved performance in\nterms of computational resource efficiency and accuracy."}
{"id": "2506.14040", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.14040", "abs": "https://arxiv.org/abs/2506.14040", "authors": ["Md Nazmus Sakib"], "title": "An Interdisciplinary Review of Commonsense Reasoning and Intent Detection", "comment": null, "summary": "This review explores recent advances in commonsense reasoning and intent\ndetection, two key challenges in natural language understanding. We analyze 28\npapers from ACL, EMNLP, and CHI (2020-2025), organizing them by methodology and\napplication. Commonsense reasoning is reviewed across zero-shot learning,\ncultural adaptation, structured evaluation, and interactive contexts. Intent\ndetection is examined through open-set models, generative formulations,\nclustering, and human-centered systems. By bridging insights from NLP and HCI,\nwe highlight emerging trends toward more adaptive, multilingual, and\ncontext-aware models, and identify key gaps in grounding, generalization, and\nbenchmark design."}
{"id": "2506.13925", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.13925", "abs": "https://arxiv.org/abs/2506.13925", "authors": ["Numair Nadeem", "Saeed Anwar", "Muhammad Hamza Asad", "Abdul Bais"], "title": "HierVL: Semi-Supervised Segmentation leveraging Hierarchical Vision-Language Synergy with Dynamic Text-Spatial Query Alignment", "comment": null, "summary": "Semi-supervised semantic segmentation remains challenging under severe label\nscarcity and domain variability. Vision-only methods often struggle to\ngeneralize, resulting in pixel misclassification between similar classes, poor\ngeneralization and boundary localization. Vision-Language Models offer robust,\ndomain-invariant semantics but lack the spatial grounding required for dense\nprediction. We introduce HierVL, a unified framework that bridges this gap by\nintegrating abstract text embeddings into a mask-transformer architecture\ntailored for semi-supervised segmentation. HierVL features three novel\ncomponents: a Hierarchical Semantic Query Generator that filters and projects\nabstract class embeddings into multi-scale queries to suppress irrelevant\nclasses and handle intra-class variability; a Cross-Modal Spatial Alignment\nModule that aligns semantic queries with pixel features for sharper boundaries\nunder sparse supervision; and a Dual-Query Transformer Decoder that fuses\nsemantic and instance-level queries to prevent instance collapse. We also\nintroduce targeted regularization losses that maintain vision-language\nalignment throughout training to reinforce semantic grounding. HierVL\nestablishes a new state-of-the-art by achieving a +4.4% mean improvement of the\nintersection over the union on COCO (with 232 labeled images), +3.1% on Pascal\nVOC (with 92 labels), +5.9% on ADE20 (with 158 labels) and +1.8% on Cityscapes\n(with 100 labels), demonstrating better performance under 1% supervision on\nfour benchmark datasets. Our results show that language-guided segmentation\ncloses the label efficiency gap and unlocks new levels of fine-grained,\ninstance-aware generalization."}
{"id": "2506.14046", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14046", "abs": "https://arxiv.org/abs/2506.14046", "authors": ["David Kogan", "Max Schumacher", "Sam Nguyen", "Masanori Suzuki", "Melissa Smith", "Chloe Sophia Bellows", "Jared Bernstein"], "title": "Ace-CEFR -- A Dataset for Automated Evaluation of the Linguistic Difficulty of Conversational Texts for LLM Applications", "comment": null, "summary": "There is an unmet need to evaluate the language difficulty of short,\nconversational passages of text, particularly for training and filtering Large\nLanguage Models (LLMs). We introduce Ace-CEFR, a dataset of English\nconversational text passages expert-annotated with their corresponding level of\ntext difficulty. We experiment with several models on Ace-CEFR, including\nTransformer-based models and LLMs. We show that models trained on Ace-CEFR can\nmeasure text difficulty more accurately than human experts and have latency\nappropriate to production environments. Finally, we release the Ace-CEFR\ndataset to the public for research and development."}
{"id": "2506.13993", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.13993", "abs": "https://arxiv.org/abs/2506.13993", "authors": ["Michelangelo Conserva", "Alex Wilson", "Charlotte Stanton", "Vishal Batchu", "Varun Gulshan"], "title": "Mapping Farmed Landscapes from Remote Sensing", "comment": null, "summary": "Effective management of agricultural landscapes is critical for meeting\nglobal biodiversity targets, but efforts are hampered by the absence of\ndetailed, large-scale ecological maps. To address this, we introduce\nFarmscapes, the first large-scale (covering most of England), high-resolution\n(25cm) map of rural landscape features, including ecologically vital elements\nlike hedgerows, woodlands, and stone walls. This map was generated using a deep\nlearning segmentation model trained on a novel, dataset of 942 manually\nannotated tiles derived from aerial imagery. Our model accurately identifies\nkey habitats, achieving high f1-scores for woodland (96\\%) and farmed land\n(95\\%), and demonstrates strong capability in segmenting linear features, with\nan F1-score of 72\\% for hedgerows. By releasing the England-wide map on Google\nEarth Engine, we provide a powerful, open-access tool for ecologists and\npolicymakers. This work enables data-driven planning for habitat restoration,\nsupports the monitoring of initiatives like the EU Biodiversity Strategy, and\nlays the foundation for advanced analysis of landscape connectivity."}
{"id": "2506.14064", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14064", "abs": "https://arxiv.org/abs/2506.14064", "authors": ["Iona Carslaw", "Sivan Milton", "Nicolas Navarre", "Ciyang Qing", "Wataru Uegaki"], "title": "Automatic Extraction of Clausal Embedding Based on Large-Scale English Text Data", "comment": "Accepted in the Society for Computation in Linguistics", "summary": "For linguists, embedded clauses have been of special interest because of\ntheir intricate distribution of syntactic and semantic features. Yet, current\nresearch relies on schematically created language examples to investigate these\nconstructions, missing out on statistical information and naturally-occurring\nexamples that can be gained from large language corpora. Thus, we present a\nmethodological approach for detecting and annotating naturally-occurring\nexamples of English embedded clauses in large-scale text data using\nconstituency parsing and a set of parsing heuristics. Our tool has been\nevaluated on our dataset Golden Embedded Clause Set (GECS), which includes\nhand-annotated examples of naturally-occurring English embedded clause\nsentences. Finally, we present a large-scale dataset of naturally-occurring\nEnglish embedded clauses which we have extracted from the open-source corpus\nDolma using our extraction tool."}
{"id": "2506.14008", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14008", "abs": "https://arxiv.org/abs/2506.14008", "authors": ["Daniel Montoya", "Aymen Bouguerra", "Alexandra Gomez-Villa", "Fabio Arnez"], "title": "FindMeIfYouCan: Bringing Open Set metrics to $\\textit{near} $, $ \\textit{far} $ and $\\textit{farther}$ Out-of-Distribution Object Detection", "comment": "Preprint", "summary": "State-of-the-art Object Detection (OD) methods predominantly operate under a\nclosed-world assumption, where test-time categories match those encountered\nduring training. However, detecting and localizing unknown objects is crucial\nfor safety-critical applications in domains such as autonomous driving and\nmedical imaging. Recently, Out-Of-Distribution (OOD) detection has emerged as a\nvital research direction for OD, focusing on identifying incorrect predictions\ntypically associated with unknown objects. This paper shows that the current\nevaluation protocol for OOD-OD violates the assumption of non-overlapping\nobjects with respect to the In-Distribution (ID) datasets, and obscures crucial\nsituations such as ignoring unknown objects, potentially leading to\noverconfidence in deployment scenarios where truly novel objects might be\nencountered. To address these limitations, we manually curate, and enrich the\nexisting benchmark by exploiting semantic similarity to create new evaluation\nsplits categorized as $\\textit{near}$, $\\textit{far}$, and $\\textit{farther}$\nfrom ID distributions. Additionally, we incorporate established metrics from\nthe Open Set community, providing deeper insights into how effectively methods\ndetect unknowns, when they ignore them, and when they mistakenly classify OOD\nobjects as ID. Our comprehensive evaluation demonstrates that semantically and\nvisually close OOD objects are easier to localize than far ones, but are also\nmore easily confounded with ID objects. $\\textit{Far}$ and $\\textit{farther}$\nobjects are harder to localize but less prone to be taken for an ID object."}
{"id": "2506.14101", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14101", "abs": "https://arxiv.org/abs/2506.14101", "authors": ["Paul Landes", "Sitara Rao", "Aaron Jeremy Chaise", "Barbara Di Eugenio"], "title": "Abstract Meaning Representation for Hospital Discharge Summarization", "comment": null, "summary": "The Achilles heel of Large Language Models (LLMs) is hallucination, which has\ndrastic consequences for the clinical domain. This is particularly important\nwith regards to automatically generating discharge summaries (a lengthy medical\ndocument that summarizes a hospital in-patient visit). Automatically generating\nthese summaries would free physicians to care for patients and reduce\ndocumentation burden. The goal of this work is to discover new methods that\ncombine language-based graphs and deep learning models to address provenance of\ncontent and trustworthiness in automatic summarization. Our method shows\nimpressive reliability results on the publicly available Medical Information\nMart for Intensive III (MIMIC-III) corpus and clinical notes written by\nphysicians at Anonymous Hospital. rovide our method, generated discharge ary\noutput examples, source code and trained models."}
{"id": "2506.14015", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14015", "abs": "https://arxiv.org/abs/2506.14015", "authors": ["Nick Yiwen Huang", "Akin Caliskan", "Berkay Kicanaoglu", "James Tompkin", "Hyeongwoo Kim"], "title": "Disentangling 3D from Large Vision-Language Models for Controlled Portrait Generation", "comment": null, "summary": "We consider the problem of disentangling 3D from large vision-language\nmodels, which we show on generative 3D portraits. This allows free-form text\ncontrol of appearance attributes like age, hair style, and glasses, and 3D\ngeometry control of face expression and camera pose. In this setting, we assume\nwe use a pre-trained large vision-language model (LVLM; CLIP) to generate from\na smaller 2D dataset with no additional paired labels and with a pre-defined 3D\nmorphable model (FLAME). First, we disentangle using canonicalization to a 2D\nreference frame from a deformable neural 3D triplane representation. But\nanother form of entanglement arises from the significant noise in the LVLM's\nembedding space that describes irrelevant features. This damages output quality\nand diversity, but we overcome this with a Jacobian regularization that can be\ncomputed efficiently with a stochastic approximator. Compared to existing\nmethods, our approach produces portraits with added text and 3D control, where\nportraits remain consistent when either control is changed. Broadly, this\napproach lets creators control 3D generators on their own 2D face data without\nneeding resources to label large data or train large models."}
{"id": "2506.14111", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.14111", "abs": "https://arxiv.org/abs/2506.14111", "authors": ["Essential AI", ":", "Andrew Hojel", "Michael Pust", "Tim Romanski", "Yash Vanjani", "Ritvik Kapila", "Mohit Parmar", "Adarsh Chaluvaraju", "Alok Tripathy", "Anil Thomas", "Ashish Tanwer", "Darsh J Shah", "Ishaan Shah", "Karl Stratos", "Khoi Nguyen", "Kurt Smith", "Michael Callahan", "Peter Rushton", "Philip Monk", "Platon Mazarakis", "Saad Jamal", "Saurabh Srivastava", "Somanshu Singla", "Ashish Vaswani"], "title": "Essential-Web v1.0: 24T tokens of organized web data", "comment": null, "summary": "Data plays the most prominent role in how language models acquire skills and\nknowledge. The lack of massive, well-organized pre-training datasets results in\ncostly and inaccessible data pipelines. We present Essential-Web v1.0, a\n24-trillion-token dataset in which every document is annotated with a\ntwelve-category taxonomy covering topic, format, content complexity, and\nquality. Taxonomy labels are produced by EAI-Distill-0.5b, a fine-tuned\n0.5b-parameter model that achieves an annotator agreement within 3% of\nQwen2.5-32B-Instruct. With nothing more than SQL-style filters, we obtain\ncompetitive web-curated datasets in math (-8.0% relative to SOTA), web code\n(+14.3%), STEM (+24.5%) and medical (+8.6%). Essential-Web v1.0 is available on\nHuggingFace: https://huggingface.co/datasets/EssentialAI/essential-web-v1.0"}
{"id": "2506.14035", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14035", "abs": "https://arxiv.org/abs/2506.14035", "authors": ["Chelsi Jain", "Yiran Wu", "Yifan Zeng", "Jiale Liu", "S hengyu Dai", "Zhenwen Shao", "Qingyun Wu", "Huazheng Wang"], "title": "SimpleDoc: Multi-Modal Document Understanding with Dual-Cue Page Retrieval and Iterative Refinement", "comment": null, "summary": "Document Visual Question Answering (DocVQA) is a practical yet challenging\ntask, which is to ask questions based on documents while referring to multiple\npages and different modalities of information, e.g, images and tables. To\nhandle multi-modality, recent methods follow a similar Retrieval Augmented\nGeneration (RAG) pipeline, but utilize Visual Language Models (VLMs) based\nembedding model to embed and retrieve relevant pages as images, and generate\nanswers with VLMs that can accept an image as input. In this paper, we\nintroduce SimpleDoc, a lightweight yet powerful retrieval - augmented framework\nfor DocVQA. It boosts evidence page gathering by first retrieving candidates\nthrough embedding similarity and then filtering and re-ranking these candidates\nbased on page summaries. A single VLM-based reasoner agent repeatedly invokes\nthis dual-cue retriever, iteratively pulling fresh pages into a working memory\nuntil the question is confidently answered. SimpleDoc outperforms previous\nbaselines by 3.2% on average on 4 DocVQA datasets with much fewer pages\nretrieved. Our code is available at https://github.com/ag2ai/SimpleDoc."}
{"id": "2506.14123", "categories": ["cs.CL", "cs.FL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.14123", "abs": "https://arxiv.org/abs/2506.14123", "authors": ["Jonathan Hayase", "Alisa Liu", "Noah A. Smith", "Sewoong Oh"], "title": "Sampling from Your Language Model One Byte at a Time", "comment": "23 pages, 8 figures", "summary": "Tokenization is used almost universally by modern language models, enabling\nefficient text representation using multi-byte or multi-character tokens.\nHowever, prior work has shown that tokenization can introduce distortion into\nthe model's generations. For example, users are often advised not to end their\nprompts with a space because it prevents the model from including the space as\npart of the next token. This Prompt Boundary Problem (PBP) also arises in\nlanguages such as Chinese and in code generation, where tokens often do not\nline up with syntactic boundaries. Additionally mismatching tokenizers often\nhinder model composition and interoperability. For example, it is not possible\nto directly ensemble models with different tokenizers due to their mismatching\nvocabularies. To address these issues, we present an inference-time method to\nconvert any autoregressive LM with a BPE tokenizer into a character-level or\nbyte-level LM, without changing its generative distribution at the text level.\nOur method efficient solves the PBP and is also able to unify the vocabularies\nof language models with different tokenizers, allowing one to ensemble LMs with\ndifferent tokenizers at inference time as well as transfer the post-training\nfrom one model to another using proxy-tuning. We demonstrate in experiments\nthat the ensemble and proxy-tuned models outperform their constituents on\ndownstream evals."}
{"id": "2506.14096", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14096", "abs": "https://arxiv.org/abs/2506.14096", "authors": ["Sanjeda Akter", "Ibne Farabi Shihab", "Anuj Sharma"], "title": "Image Segmentation with Large Language Models: A Survey with Perspectives for Intelligent Transportation Systems", "comment": null, "summary": "The integration of Large Language Models (LLMs) with computer vision is\nprofoundly transforming perception tasks like image segmentation. For\nintelligent transportation systems (ITS), where accurate scene understanding is\ncritical for safety and efficiency, this new paradigm offers unprecedented\ncapabilities. This survey systematically reviews the emerging field of\nLLM-augmented image segmentation, focusing on its applications, challenges, and\nfuture directions within ITS. We provide a taxonomy of current approaches based\non their prompting mechanisms and core architectures, and we highlight how\nthese innovations can enhance road scene understanding for autonomous driving,\ntraffic monitoring, and infrastructure maintenance. Finally, we identify key\nchallenges, including real-time performance and safety-critical reliability,\nand outline a perspective centered on explainable, human-centric AI as a\nprerequisite for the successful deployment of this technology in\nnext-generation transportation systems."}
{"id": "2506.14157", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14157", "abs": "https://arxiv.org/abs/2506.14157", "authors": ["Chengyu Huang", "Tanya Goyal"], "title": "DCRM: A Heuristic to Measure Response Pair Quality in Preference Optimization", "comment": null, "summary": "Recent research has attempted to associate preference optimization (PO)\nperformance with the underlying preference datasets. In this work, our\nobservation is that the differences between the preferred response $y^+$ and\ndispreferred response $y^-$ influence what LLMs can learn, which may not match\nthe desirable differences to learn. Therefore, we use distance and reward\nmargin to quantify these differences, and combine them to get Distance\nCalibrated Reward Margin (DCRM), a metric that measures the quality of a\nresponse pair for PO. Intuitively, DCRM encourages minimal noisy differences\nand maximal desired differences. With this, we study 3 types of commonly used\npreference datasets, classified along two axes: the source of the responses and\nthe preference labeling function. We establish a general correlation between\nhigher DCRM of the training set and better learning outcome. Inspired by this,\nwe propose a best-of-$N^2$ pairing method that selects response pairs with the\nhighest DCRM. Empirically, in various settings, our method produces training\ndatasets that can further improve models' performance on AlpacaEval, MT-Bench,\nand Arena-Hard over the existing training sets."}
{"id": "2506.14121", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14121", "abs": "https://arxiv.org/abs/2506.14121", "authors": ["Siyu Xu", "Wenjie Li", "Guangwei Gao", "Jian Yang", "Guo-Jun Qi", "Chia-Wen Lin"], "title": "FADPNet: Frequency-Aware Dual-Path Network for Face Super-Resolution", "comment": "12 pages, 11 figures, 6 tales", "summary": "Face super-resolution (FSR) under limited computational costs remains an open\nproblem. Existing approaches typically treat all facial pixels equally,\nresulting in suboptimal allocation of computational resources and degraded FSR\nperformance. CNN is relatively sensitive to high-frequency facial features,\nsuch as component contours and facial outlines. Meanwhile, Mamba excels at\ncapturing low-frequency features like facial color and fine-grained texture,\nand does so with lower complexity than Transformers. Motivated by these\nobservations, we propose FADPNet, a Frequency-Aware Dual-Path Network that\ndecomposes facial features into low- and high-frequency components and\nprocesses them via dedicated branches. For low-frequency regions, we introduce\na Mamba-based Low-Frequency Enhancement Block (LFEB), which combines\nstate-space attention with squeeze-and-excitation operations to extract\nlow-frequency global interactions and emphasize informative channels. For\nhigh-frequency regions, we design a CNN-based Deep Position-Aware Attention\n(DPA) module to enhance spatially-dependent structural details, complemented by\na lightweight High-Frequency Refinement (HFR) module that further refines\nfrequency-specific representations. Through the above designs, our method\nachieves an excellent balance between FSR quality and model efficiency,\noutperforming existing approaches."}
{"id": "2506.14158", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14158", "abs": "https://arxiv.org/abs/2506.14158", "authors": ["Tao He", "Guang Huang", "Yu Yang", "Tianshi Xu", "Sicheng Zhao", "Guiguang Ding", "Pengyang Wang", "Feng Tian"], "title": "S$^4$C: Speculative Sampling with Syntactic and Semantic Coherence for Efficient Inference of Large Language Models", "comment": null, "summary": "Large language models (LLMs) exhibit remarkable reasoning capabilities across\ndiverse downstream tasks. However, their autoregressive nature leads to\nsubstantial inference latency, posing challenges for real-time applications.\nSpeculative sampling mitigates this issue by introducing a drafting phase\nfollowed by a parallel validation phase, enabling faster token generation and\nverification. Existing approaches, however, overlook the inherent coherence in\ntext generation, limiting their efficiency. To address this gap, we propose a\nSpeculative Sampling with Syntactic and Semantic Coherence (S$^4$C) framework,\nwhich extends speculative sampling by leveraging multi-head drafting for rapid\ntoken generation and a continuous verification tree for efficient candidate\nvalidation and feature reuse. Experimental results demonstrate that S$^4$C\nsurpasses baseline methods across mainstream tasks, offering enhanced\nefficiency, parallelism, and the ability to generate more valid tokens with\nfewer computational resources. On Spec-bench benchmarks, S$^4$C achieves an\nacceleration ratio of 2.26x-2.60x, outperforming state-of-the-art methods."}
{"id": "2506.14130", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.14130", "abs": "https://arxiv.org/abs/2506.14130", "authors": ["Chunyu Cao", "Jintao Cheng", "Zeyu Chen", "Linfan Zhan", "Rui Fan", "Zhijian He", "Xiaoyu Tang"], "title": "KDMOS:Knowledge Distillation for Motion Segmentation", "comment": null, "summary": "Motion Object Segmentation (MOS) is crucial for autonomous driving, as it\nenhances localization, path planning, map construction, scene flow estimation,\nand future state prediction. While existing methods achieve strong performance,\nbalancing accuracy and real-time inference remains a challenge. To address\nthis, we propose a logits-based knowledge distillation framework for MOS,\naiming to improve accuracy while maintaining real-time efficiency.\nSpecifically, we adopt a Bird's Eye View (BEV) projection-based model as the\nstudent and a non-projection model as the teacher. To handle the severe\nimbalance between moving and non-moving classes, we decouple them and apply\ntailored distillation strategies, allowing the teacher model to better learn\nkey motion-related features. This approach significantly reduces false\npositives and false negatives. Additionally, we introduce dynamic upsampling,\noptimize the network architecture, and achieve a 7.69% reduction in parameter\ncount, mitigating overfitting. Our method achieves a notable IoU of 78.8% on\nthe hidden test set of the SemanticKITTI-MOS dataset and delivers competitive\nresults on the Apollo dataset. The KDMOS implementation is available at\nhttps://github.com/SCNU-RISLAB/KDMOS."}
{"id": "2506.14161", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14161", "abs": "https://arxiv.org/abs/2506.14161", "authors": ["Yanlin Li", "Hao Liu", "Huimin Liu", "Yinwei Wei", "Yupeng Hu"], "title": "MIST: Towards Multi-dimensional Implicit Bias and Stereotype Evaluation of LLMs via Theory of Mind", "comment": null, "summary": "Theory of Mind (ToM) in Large Language Models (LLMs) refers to their capacity\nfor reasoning about mental states, yet failures in this capacity often manifest\nas systematic implicit bias. Evaluating this bias is challenging, as\nconventional direct-query methods are susceptible to social desirability\neffects and fail to capture its subtle, multi-dimensional nature. To this end,\nwe propose an evaluation framework that leverages the Stereotype Content Model\n(SCM) to reconceptualize bias as a multi-dimensional failure in ToM across\nCompetence, Sociability, and Morality. The framework introduces two indirect\ntasks: the Word Association Bias Test (WABT) to assess implicit lexical\nassociations and the Affective Attribution Test (AAT) to measure covert\naffective leanings, both designed to probe latent stereotypes without\ntriggering model avoidance. Extensive experiments on 8 State-of-the-Art LLMs\ndemonstrate our framework's capacity to reveal complex bias structures,\nincluding pervasive sociability bias, multi-dimensional divergence, and\nasymmetric stereotype amplification, thereby providing a more robust\nmethodology for identifying the structural nature of implicit bias."}
{"id": "2506.14136", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14136", "abs": "https://arxiv.org/abs/2506.14136", "authors": ["Nafiz Sadman", "Farhana Zulkernine", "Benjamin Kwan"], "title": "Interpreting Biomedical VLMs on High-Imbalance Out-of-Distributions: An Insight into BiomedCLIP on Radiology", "comment": "GitHub: https://github.com/Nafiz95/BioVLM_Eval_CXR", "summary": "In this paper, we construct two research objectives: i) explore the learned\nembedding space of BiomedCLIP, an open-source large vision language model, to\nanalyse meaningful class separations, and ii) quantify the limitations of\nBiomedCLIP when applied to a highly imbalanced, out-of-distribution multi-label\nmedical dataset. We experiment on IU-xray dataset, which exhibits the\naforementioned criteria, and evaluate BiomedCLIP in classifying images\n(radiographs) in three contexts: zero-shot inference, full finetuning, and\nlinear probing. The results show that the model under zero-shot settings\nover-predicts all labels, leading to poor precision and inter-class\nseparability. Full fine-tuning improves classification of distinct diseases,\nwhile linear probing detects overlapping features. We demonstrate visual\nunderstanding of the model using Grad-CAM heatmaps and compare with 15\nannotations by a radiologist. We highlight the need for careful adaptations of\nthe models to foster reliability and applicability in a real-world setting. The\ncode for the experiments in this work is available and maintained on GitHub."}
{"id": "2506.14175", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14175", "abs": "https://arxiv.org/abs/2506.14175", "authors": ["Chenglong Wang", "Yang Gan", "Yifu Huo", "Yongyu Mu", "Qiaozhi He", "Murun Yang", "Bei Li", "Tong Xiao", "Chunliang Zhang", "Tongran Liu", "Jingbo Zhu"], "title": "GRAM: A Generative Foundation Reward Model for Reward Generalization", "comment": "Accepted by ICML 2025", "summary": "In aligning large language models (LLMs), reward models have played an\nimportant role, but are standardly trained as discriminative models and rely\nonly on labeled human preference data. In this paper, we explore methods that\ntrain reward models using both unlabeled and labeled data. Building on the\ngenerative models in LLMs, we develop a generative reward model that is first\ntrained via large-scale unsupervised learning and then fine-tuned via\nsupervised learning. We also show that by using label smoothing, we are in fact\noptimizing a regularized pairwise ranking loss. This result, in turn, provides\na new view of training reward models, which links generative models and\ndiscriminative models under the same class of training objectives. The outcome\nof these techniques is a foundation reward model, which can be applied to a\nwide range of tasks with little or no further fine-tuning effort. Extensive\nexperiments show that this model generalizes well across several tasks,\nincluding response ranking, reinforcement learning from human feedback, and\ntask adaptation with fine-tuning, achieving significant performance\nimprovements over several strong baseline models."}
{"id": "2506.14142", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14142", "abs": "https://arxiv.org/abs/2506.14142", "authors": ["Wenting Chen", "Yi Dong", "Zhaojun Ding", "Yucheng Shi", "Yifan Zhou", "Fang Zeng", "Yijun Luo", "Tianyu Lin", "Yihang Su", "Yichen Wu", "Kai Zhang", "Zhen Xiang", "Tianming Liu", "Ninghao Liu", "Lichao Sun", "Yixuan Yuan", "Xiang Li"], "title": "RadFabric: Agentic AI System with Reasoning Capability for Radiology", "comment": "4 figures, 2 tables", "summary": "Chest X ray (CXR) imaging remains a critical diagnostic tool for thoracic\nconditions, but current automated systems face limitations in pathology\ncoverage, diagnostic accuracy, and integration of visual and textual reasoning.\nTo address these gaps, we propose RadFabric, a multi agent, multimodal\nreasoning framework that unifies visual and textual analysis for comprehensive\nCXR interpretation. RadFabric is built on the Model Context Protocol (MCP),\nenabling modularity, interoperability, and scalability for seamless integration\nof new diagnostic agents. The system employs specialized CXR agents for\npathology detection, an Anatomical Interpretation Agent to map visual findings\nto precise anatomical structures, and a Reasoning Agent powered by large\nmultimodal reasoning models to synthesize visual, anatomical, and clinical data\ninto transparent and evidence based diagnoses. RadFabric achieves significant\nperformance improvements, with near-perfect detection of challenging\npathologies like fractures (1.000 accuracy) and superior overall diagnostic\naccuracy (0.799) compared to traditional systems (0.229 to 0.527). By\nintegrating cross modal feature alignment and preference-driven reasoning,\nRadFabric advances AI-driven radiology toward transparent, anatomically\nprecise, and clinically actionable CXR analysis."}
{"id": "2506.14177", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.14177", "abs": "https://arxiv.org/abs/2506.14177", "authors": ["Tuan Nguyen", "Huy-Dat Tran"], "title": "Can we train ASR systems on Code-switch without real code-switch data? Case study for Singapore's languages", "comment": "Accepted by Interspeech 2025", "summary": "Code-switching (CS), common in multilingual settings, presents challenges for\nASR due to scarce and costly transcribed data caused by linguistic complexity.\nThis study investigates building CS-ASR using synthetic CS data. We propose a\nphrase-level mixing method to generate synthetic CS data that mimics natural\npatterns. Utilizing monolingual augmented with synthetic phrase-mixed CS data\nto fine-tune large pretrained ASR models (Whisper, MMS, SeamlessM4T). This\npaper focuses on three under-resourced Southeast Asian language pairs:\nMalay-English (BM-EN), Mandarin-Malay (ZH-BM), and Tamil-English (TA-EN),\nestablishing a new comprehensive benchmark for CS-ASR to evaluate the\nperformance of leading ASR models. Experimental results show that the proposed\ntraining strategy enhances ASR performance on monolingual and CS tests, with\nBM-EN showing highest gains, then TA-EN and ZH-BM. This finding offers a\ncost-effective approach for CS-ASR development, benefiting research and\nindustry."}
{"id": "2506.14144", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14144", "abs": "https://arxiv.org/abs/2506.14144", "authors": ["Juho Bai", "Inwook Shim"], "title": "SceneAware: Scene-Constrained Pedestrian Trajectory Prediction with LLM-Guided Walkability", "comment": null, "summary": "Accurate prediction of pedestrian trajectories is essential for applications\nin robotics and surveillance systems. While existing approaches primarily focus\non social interactions between pedestrians, they often overlook the rich\nenvironmental context that significantly shapes human movement patterns. In\nthis paper, we propose SceneAware, a novel framework that explicitly\nincorporates scene understanding to enhance trajectory prediction accuracy. Our\nmethod leverages a Vision Transformer~(ViT) scene encoder to process\nenvironmental context from static scene images, while Multi-modal Large\nLanguage Models~(MLLMs) generate binary walkability masks that distinguish\nbetween accessible and restricted areas during training. We combine a\nTransformer-based trajectory encoder with the ViT-based scene encoder,\ncapturing both temporal dynamics and spatial constraints. The framework\nintegrates collision penalty mechanisms that discourage predicted trajectories\nfrom violating physical boundaries, ensuring physically plausible predictions.\nSceneAware is implemented in both deterministic and stochastic variants.\nComprehensive experiments on the ETH/UCY benchmark datasets show that our\napproach outperforms state-of-the-art methods, with more than 50\\% improvement\nover previous models. Our analysis based on different trajectory categories\nshows that the model performs consistently well across various types of\npedestrian movement. This highlights the importance of using explicit scene\ninformation and shows that our scene-aware approach is both effective and\nreliable in generating accurate and physically plausible predictions. Code is\navailable at: https://github.com/juho127/SceneAware."}
{"id": "2506.14190", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.14190", "abs": "https://arxiv.org/abs/2506.14190", "authors": ["Tuan Nguyen", "Huy-Dat Tran"], "title": "AsyncSwitch: Asynchronous Text-Speech Adaptation for Code-Switched ASR", "comment": "This work has been submitted to the IEEE for possible publication.\n  This paper is a preprint version submitted to the 2025 IEEE Automatic Speech\n  Recognition and Understanding Workshop (ASRU 2025)", "summary": "Developing code-switched ASR systems is challenging due to language ambiguity\nand limited exposure to multilingual, code-switched data, while collecting such\nspeech is costly. Prior work generates synthetic audio from text, but these\nmethods are computationally intensive and hard to scale. We introduce\nAsyncSwitch, a novel asynchronous adaptation framework that leverages\nlarge-scale, text-rich web data to pre-expose ASR models to diverse\ncode-switched domains before fine-tuning on paired speech-text corpora. Our\nthree-stage process (1) trains decoder self-attention and feedforward layers on\ncode-switched text, (2) aligns decoder and encoder via cross-attention using\nlimited speech-text data, and (3) fully fine-tunes the entire model.\nExperiments with Whisper on Malay-English code-switching demonstrate a 9.02%\nrelative WER reduction, while improving monolingual performance in Singlish,\nMalay, and other English variants."}
{"id": "2506.14168", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14168", "abs": "https://arxiv.org/abs/2506.14168", "authors": ["Hu Yu", "Biao Gong", "Hangjie Yuan", "DanDan Zheng", "Weilong Chai", "Jingdong Chen", "Kecheng Zheng", "Feng Zhao"], "title": "VideoMAR: Autoregressive Video Generatio with Continuous Tokens", "comment": "Submitted to NeurIPS 2025", "summary": "Masked-based autoregressive models have demonstrated promising image\ngeneration capability in continuous space. However, their potential for video\ngeneration remains under-explored. In this paper, we propose \\textbf{VideoMAR},\na concise and efficient decoder-only autoregressive image-to-video model with\ncontinuous tokens, composing temporal frame-by-frame and spatial masked\ngeneration. We first identify temporal causality and spatial bi-directionality\nas the first principle of video AR models, and propose the next-frame diffusion\nloss for the integration of mask and video generation. Besides, the huge cost\nand difficulty of long sequence autoregressive modeling is a basic but crucial\nissue. To this end, we propose the temporal short-to-long curriculum learning\nand spatial progressive resolution training, and employ progressive temperature\nstrategy at inference time to mitigate the accumulation error. Furthermore,\nVideoMAR replicates several unique capacities of language models to video\ngeneration. It inherently bears high efficiency due to simultaneous\ntemporal-wise KV cache and spatial-wise parallel generation, and presents the\ncapacity of spatial and temporal extrapolation via 3D rotary embeddings. On the\nVBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos\nI2V) while requiring significantly fewer parameters ($9.3\\%$), training data\n($0.5\\%$), and GPU resources ($0.2\\%$)."}
{"id": "2506.14199", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14199", "abs": "https://arxiv.org/abs/2506.14199", "authors": ["Junghwan Kim", "Kieun Park", "Sohee Park", "Hyunggug Kim", "Bongwon Suh"], "title": "MAS-LitEval : Multi-Agent System for Literary Translation Quality Assessment", "comment": "4 Pages, 2 tables, EMNLP submitted", "summary": "Literary translation requires preserving cultural nuances and stylistic\nelements, which traditional metrics like BLEU and METEOR fail to assess due to\ntheir focus on lexical overlap. This oversight neglects the narrative\nconsistency and stylistic fidelity that are crucial for literary works. To\naddress this, we propose MAS-LitEval, a multi-agent system using Large Language\nModels (LLMs) to evaluate translations based on terminology, narrative, and\nstyle. We tested MAS-LitEval on translations of The Little Prince and A\nConnecticut Yankee in King Arthur's Court, generated by various LLMs, and\ncompared it to traditional metrics. \\textbf{MAS-LitEval} outperformed these\nmetrics, with top models scoring up to 0.890 in capturing literary nuances.\nThis work introduces a scalable, nuanced framework for Translation Quality\nAssessment (TQA), offering a practical tool for translators and researchers."}
{"id": "2506.14170", "categories": ["cs.CV", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2506.14170", "abs": "https://arxiv.org/abs/2506.14170", "authors": ["Shulong Zhang", "Mingyuan Yao", "Jiayin Zhao", "Xiao Liu", "Haihua Wang"], "title": "A multi-stage augmented multimodal interaction network for fish feeding intensity quantification", "comment": null, "summary": "In recirculating aquaculture systems, accurate and effective assessment of\nfish feeding intensity is crucial for reducing feed costs and calculating\noptimal feeding times. However, current studies have limitations in modality\nselection, feature extraction and fusion, and co-inference for decision making,\nwhich restrict further improvement in the accuracy, applicability and\nreliability of multimodal fusion models. To address this problem, this study\nproposes a Multi-stage Augmented Multimodal Interaction Network (MAINet) for\nquantifying fish feeding intensity. Firstly, a general feature extraction\nframework is proposed to efficiently extract feature information from input\nimage, audio and water wave datas. Second, an Auxiliary-modality Reinforcement\nPrimary-modality Mechanism (ARPM) is designed for inter-modal interaction and\ngenerate enhanced features, which consists of a Channel Attention Fusion\nNetwork (CAFN) and a Dual-mode Attention Fusion Network (DAFN). Finally, an\nEvidence Reasoning (ER) rule is introduced to fuse the output results of each\nmodality and make decisions, thereby completing the quantification of fish\nfeeding intensity. The experimental results show that the constructed MAINet\nreaches 96.76%, 96.78%, 96.79% and 96.79% in accuracy, precision, recall and\nF1-Score respectively, and its performance is significantly higher than the\ncomparison models. Compared with models that adopt single-modality,\ndual-modality fusion and different decision-making fusion methods, it also has\nobvious advantages. Meanwhile, the ablation experiments further verified the\nkey role of the proposed improvement strategy in improving the robustness and\nfeature utilization efficiency of model, which can effectively improve the\naccuracy of the quantitative results of fish feeding intensity."}
{"id": "2506.14200", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.14200", "abs": "https://arxiv.org/abs/2506.14200", "authors": ["Brihi Joshi", "Keyu He", "Sahana Ramnath", "Sadra Sabouri", "Kaitlyn Zhou", "Souti Chattopadhyay", "Swabha Swayamdipta", "Xiang Ren"], "title": "ELI-Why: Evaluating the Pedagogical Utility of Language Model Explanations", "comment": "Findings of ACL 2025", "summary": "Language models today are widely used in education, yet their ability to\ntailor responses for learners with varied informational needs and knowledge\nbackgrounds remains under-explored. To this end, we introduce ELI-Why, a\nbenchmark of 13.4K \"Why\" questions to evaluate the pedagogical capabilities of\nlanguage models. We then conduct two extensive human studies to assess the\nutility of language model-generated explanatory answers (explanations) on our\nbenchmark, tailored to three distinct educational grades: elementary,\nhigh-school and graduate school. In our first study, human raters assume the\nrole of an \"educator\" to assess model explanations' fit to different\neducational grades. We find that GPT-4-generated explanations match their\nintended educational background only 50% of the time, compared to 79% for lay\nhuman-curated explanations. In our second study, human raters assume the role\nof a learner to assess if an explanation fits their own informational needs.\nAcross all educational backgrounds, users deemed GPT-4-generated explanations\n20% less suited on average to their informational needs, when compared to\nexplanations curated by lay people. Additionally, automated evaluation metrics\nreveal that explanations generated across different language model families for\ndifferent informational needs remain indistinguishable in their grade-level,\nlimiting their pedagogical effectiveness."}
{"id": "2506.14176", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14176", "abs": "https://arxiv.org/abs/2506.14176", "authors": ["Renao Yan"], "title": "One-Shot Neural Architecture Search with Network Similarity Directed Initialization for Pathological Image Classification", "comment": null, "summary": "Deep learning-based pathological image analysis presents unique challenges\ndue to the practical constraints of network design. Most existing methods apply\ncomputer vision models directly to medical tasks, neglecting the distinct\ncharacteristics of pathological images. This mismatch often leads to\ncomputational inefficiencies, particularly in edge-computing scenarios. To\naddress this, we propose a novel Network Similarity Directed Initialization\n(NSDI) strategy to improve the stability of neural architecture search (NAS).\nFurthermore, we introduce domain adaptation into one-shot NAS to better handle\nvariations in staining and semantic scale across pathology datasets.\nExperiments on the BRACS dataset demonstrate that our method outperforms\nexisting approaches, delivering both superior classification performance and\nclinically relevant feature localization."}
{"id": "2506.14203", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14203", "abs": "https://arxiv.org/abs/2506.14203", "authors": ["Jongho Kim", "Romain Storaï", "Seung-won Hwang"], "title": "Intended Target Identification for Anomia Patients with Gradient-based Selective Augmentation", "comment": "EMNLP 2024 Findings (long)", "summary": "In this study, we investigate the potential of language models (LMs) in\naiding patients experiencing anomia, a difficulty identifying the names of\nitems. Identifying the intended target item from patient's circumlocution\ninvolves the two challenges of term failure and error: (1) The terms relevant\nto identifying the item remain unseen. (2) What makes the challenge unique is\ninherent perturbed terms by semantic paraphasia, which are not exactly related\nto the target item, hindering the identification process. To address each, we\npropose robustifying the model from semantically paraphasic errors and\nenhancing the model with unseen terms with gradient-based selective\naugmentation. Specifically, the gradient value controls augmented data quality\namid semantic errors, while the gradient variance guides the inclusion of\nunseen but relevant terms. Due to limited domain-specific datasets, we evaluate\nthe model on the Tip-of-the-Tongue dataset as an intermediary task and then\napply our findings to real patient data from AphasiaBank. Our results\ndemonstrate strong performance against baselines, aiding anomia patients by\naddressing the outlined challenges."}
{"id": "2506.14181", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14181", "abs": "https://arxiv.org/abs/2506.14181", "authors": ["Yufei Li", "Jirui Wu", "Long Tian", "Liming Wang", "Xiaonan Liu", "Zijun Liu", "Xiyang Liu"], "title": "Meta-SurDiff: Classification Diffusion Model Optimized by Meta Learning is Reliable for Online Surgical Phase Recognition", "comment": "15 pages, 5 figures", "summary": "Online surgical phase recognition has drawn great attention most recently due\nto its potential downstream applications closely related to human life and\nhealth. Despite deep models have made significant advances in capturing the\ndiscriminative long-term dependency of surgical videos to achieve improved\nrecognition, they rarely account for exploring and modeling the uncertainty in\nsurgical videos, which should be crucial for reliable online surgical phase\nrecognition. We categorize the sources of uncertainty into two types, frame\nambiguity in videos and unbalanced distribution among surgical phases, which\nare inevitable in surgical videos. To address this pivot issue, we introduce a\nmeta-learning-optimized classification diffusion model (Meta-SurDiff), to take\nfull advantage of the deep generative model and meta-learning in achieving\nprecise frame-level distribution estimation for reliable online surgical phase\nrecognition. For coarse recognition caused by ambiguous video frames, we employ\na classification diffusion model to assess the confidence of recognition\nresults at a finer-grained frame-level instance. For coarse recognition caused\nby unbalanced phase distribution, we use a meta-learning based objective to\nlearn the diffusion model, thus enhancing the robustness of classification\nboundaries for different surgical phases.We establish effectiveness of\nMeta-SurDiff in online surgical phase recognition through extensive experiments\non five widely used datasets using more than four practical metrics. The\ndatasets include Cholec80, AutoLaparo, M2Cai16, OphNet, and NurViD, where\nOphNet comes from ophthalmic surgeries, NurViD is the daily care dataset, while\nthe others come from laparoscopic surgeries. We will release the code upon\nacceptance."}
{"id": "2506.14205", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14205", "abs": "https://arxiv.org/abs/2506.14205", "authors": ["Jingxu Xie", "Dylan Xu", "Xuandong Zhao", "Dawn Song"], "title": "AgentSynth: Scalable Task Generation for Generalist Computer-Use Agents", "comment": null, "summary": "We introduce AgentSynth, a scalable and cost-efficient pipeline for\nautomatically synthesizing high-quality tasks and trajectory datasets for\ngeneralist computer-use agents. Leveraging information asymmetry, AgentSynth\nconstructs subtasks that are simple during generation but significantly more\nchallenging when composed into long-horizon tasks, enabling the creation of\nover 6,000 diverse and realistic tasks. Our pipeline begins with an LLM-based\ntask proposer guided by a persona, followed by an execution agent that\ncompletes the task and logs the trajectory. This process is repeated\niteratively to form a sequence of subtasks, which are then summarized by a\nseparate agent into a composite task of controllable difficulty. A key strength\nof AgentSynth is its ability to precisely modulate task complexity by varying\nthe number of subtasks. Empirical evaluations show that state-of-the-art LLM\nagents suffer a steep performance drop, from 18% success at difficulty level 1\nto just 4% at level 6, highlighting the benchmark's difficulty and\ndiscriminative power. Moreover, our pipeline achieves a low average cost of\n\\$0.60 per trajectory, orders of magnitude cheaper than human annotations. Our\ncode and data are publicly available at\nhttps://github.com/sunblaze-ucb/AgentSynth"}
{"id": "2506.14189", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14189", "abs": "https://arxiv.org/abs/2506.14189", "authors": ["Kunyuan Deng", "Yi Wang", "Lap-Pui Chau"], "title": "Egocentric Human-Object Interaction Detection: A New Benchmark and Method", "comment": null, "summary": "Understanding the interaction between humans and objects has gained much\nattention in recent years. Existing human-object interaction (HOI) detection\nmethods mainly focus on the third-person perspectives, overlooking a more\nintuitive way from the egocentric view of HOI, namely Ego-HOI. This paper\nintroduces an Ego-HOIBench, a new dataset to promote the benchmarking and\ndevelopment of Ego-HOI detection. Our Ego-HOIBench comprises more than 27K\negocentric images with high-quality hand-verb-object triplet annotations across\n123 fine-grained interaction categories and locations, covering a rich\ndiversity of scenarios, object types, and hand configurations in daily\nactivities. In addition, we explore and adapt third-person HOI detection\nmethods to Ego-HOIBench and illustrate the challenges of hand-occluded objects\nand the complexity of single- and two-hand interactions. To build a new\nbaseline, we propose a Hand Geometry and Interactivity Refinement (HGIR)\nscheme, which leverages hand pose and geometric information as valuable cues\nfor interpreting interactions. Specifically, the HGIR scheme explicitly\nextracts global hand geometric features from the estimated hand pose proposals\nand refines the interaction-specific features using pose-interaction attention.\nThis scheme enables the model to obtain a robust and powerful interaction\nrepresentation, significantly improving the Ego-HOI detection capability. Our\napproach is lightweight and effective, and it can be easily applied to HOI\nbaselines in a plug-and-play manner to achieve state-of-the-art results on\nEgo-HOIBench. Our project is available at:\nhttps://dengkunyuan.github.io/EgoHOIBench/"}
{"id": "2506.14206", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14206", "abs": "https://arxiv.org/abs/2506.14206", "authors": ["Jia-Chen Zhang", "Zheng Zhou", "Yu-Jie Xiong", "Chun-Ming Xia", "Fei Dai"], "title": "CausalDiffTab: Mixed-Type Causal-Aware Diffusion for Tabular Data Generation", "comment": null, "summary": "Training data has been proven to be one of the most critical components in\ntraining generative AI. However, obtaining high-quality data remains\nchallenging, with data privacy issues presenting a significant hurdle. To\naddress the need for high-quality data. Synthesize data has emerged as a\nmainstream solution, demonstrating impressive performance in areas such as\nimages, audio, and video. Generating mixed-type data, especially high-quality\ntabular data, still faces significant challenges. These primarily include its\ninherent heterogeneous data types, complex inter-variable relationships, and\nintricate column-wise distributions. In this paper, we introduce CausalDiffTab,\na diffusion model-based generative model specifically designed to handle mixed\ntabular data containing both numerical and categorical features, while being\nmore flexible in capturing complex interactions among variables. We further\npropose a hybrid adaptive causal regularization method based on the principle\nof Hierarchical Prior Fusion. This approach adaptively controls the weight of\ncausal regularization, enhancing the model's performance without compromising\nits generative capabilities. Comprehensive experiments conducted on seven\ndatasets demonstrate that CausalDiffTab outperforms baseline methods across all\nmetrics. Our code is publicly available at:\nhttps://github.com/Godz-z/CausalDiffTab."}
{"id": "2506.14229", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14229", "abs": "https://arxiv.org/abs/2506.14229", "authors": ["Changbai Li", "Haodong Zhu", "Hanlin Chen", "Juan Zhang", "Tongfei Chen", "Shuo Yang", "Shuwei Shao", "Wenhao Dong", "Baochang Zhang"], "title": "HRGS: Hierarchical Gaussian Splatting for Memory-Efficient High-Resolution 3D Reconstruction", "comment": null, "summary": "3D Gaussian Splatting (3DGS) has made significant strides in real-time 3D\nscene reconstruction, but faces memory scalability issues in high-resolution\nscenarios. To address this, we propose Hierarchical Gaussian Splatting (HRGS),\na memory-efficient framework with hierarchical block-level optimization. First,\nwe generate a global, coarse Gaussian representation from low-resolution data.\nThen, we partition the scene into multiple blocks, refining each block with\nhigh-resolution data. The partitioning involves two steps: Gaussian\npartitioning, where irregular scenes are normalized into a bounded cubic space\nwith a uniform grid for task distribution, and training data partitioning,\nwhere only relevant observations are retained for each block. By guiding block\nrefinement with the coarse Gaussian prior, we ensure seamless Gaussian fusion\nacross adjacent blocks. To reduce computational demands, we introduce\nImportance-Driven Gaussian Pruning (IDGP), which computes importance scores for\neach Gaussian and removes those with minimal contribution, speeding up\nconvergence and reducing memory usage. Additionally, we incorporate normal\npriors from a pretrained model to enhance surface reconstruction quality. Our\nmethod enables high-quality, high-resolution 3D scene reconstruction even under\nmemory constraints. Extensive experiments on three benchmarks show that HRGS\nachieves state-of-the-art performance in high-resolution novel view synthesis\n(NVS) and surface reconstruction tasks."}
{"id": "2506.14211", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14211", "abs": "https://arxiv.org/abs/2506.14211", "authors": ["Sina Abdidizaji", "Md Kowsher", "Niloofar Yousefi", "Ivan Garibay"], "title": "Explainable Detection of Implicit Influential Patterns in Conversations via Data Augmentation", "comment": "Accepted at the HCI International conference 2025", "summary": "In the era of digitalization, as individuals increasingly rely on digital\nplatforms for communication and news consumption, various actors employ\nlinguistic strategies to influence public perception. While models have become\nproficient at detecting explicit patterns, which typically appear in texts as\nsingle remarks referred to as utterances, such as social media posts, malicious\nactors have shifted toward utilizing implicit influential verbal patterns\nembedded within conversations. These verbal patterns aim to mentally penetrate\nthe victim's mind in order to influence them, enabling the actor to obtain the\ndesired information through implicit means. This paper presents an improved\napproach for detecting such implicit influential patterns. Furthermore, the\nproposed model is capable of identifying the specific locations of these\ninfluential elements within a conversation. To achieve this, the existing\ndataset was augmented using the reasoning capabilities of state-of-the-art\nlanguage models. Our designed framework resulted in a 6% improvement in the\ndetection of implicit influential patterns in conversations. Moreover, this\napproach improved the multi-label classification tasks related to both the\ntechniques used for influence and the vulnerability of victims by 33% and 43%,\nrespectively."}
{"id": "2506.14238", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14238", "abs": "https://arxiv.org/abs/2506.14238", "authors": ["Yinuo Zheng", "Lipeng Gu", "Honghua Chen", "Liangliang Nan", "Mingqiang Wei"], "title": "Unified Representation Space for 3D Visual Grounding", "comment": null, "summary": "3D visual grounding (3DVG) is a critical task in scene understanding that\naims to identify objects in 3D scenes based on text descriptions. However,\nexisting methods rely on separately pre-trained vision and text encoders,\nresulting in a significant gap between the two modalities in terms of spatial\ngeometry and semantic categories. This discrepancy often causes errors in\nobject positioning and classification. The paper proposes UniSpace-3D, which\ninnovatively introduces a unified representation space for 3DVG, effectively\nbridging the gap between visual and textual features. Specifically, UniSpace-3D\nincorporates three innovative designs: i) a unified representation encoder that\nleverages the pre-trained CLIP model to map visual and textual features into a\nunified representation space, effectively bridging the gap between the two\nmodalities; ii) a multi-modal contrastive learning module that further reduces\nthe modality gap; iii) a language-guided query selection module that utilizes\nthe positional and semantic information to identify object candidate points\naligned with textual descriptions. Extensive experiments demonstrate that\nUniSpace-3D outperforms baseline models by at least 2.24% on the ScanRefer and\nNr3D/Sr3D datasets. The code will be made available upon acceptance of the\npaper."}
{"id": "2506.14213", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14213", "abs": "https://arxiv.org/abs/2506.14213", "authors": ["Jongho Kim", "Dohyeon Lee", "Minsoo Kim", "Seung-won Hwang"], "title": "Chaining Event Spans for Temporal Relation Grounding", "comment": "In Proceedings of the 18th Conference of the European Chapter of the\n  Association for Computational Linguistics (Volume 1: Long Papers), pages\n  1689-1700", "summary": "Accurately understanding temporal relations between events is a critical\nbuilding block of diverse tasks, such as temporal reading comprehension (TRC)\nand relation extraction (TRE). For example in TRC, we need to understand the\ntemporal semantic differences between the following two questions that are\nlexically near-identical: \"What finished right before the decision?\" or \"What\nfinished right after the decision?\". To discern the two questions, existing\nsolutions have relied on answer overlaps as a proxy label to contrast similar\nand dissimilar questions. However, we claim that answer overlap can lead to\nunreliable results, due to spurious overlaps of two dissimilar questions with\ncoincidentally identical answers. To address the issue, we propose a novel\napproach that elicits proper reasoning behaviors through a module for\npredicting time spans of events. We introduce the Timeline Reasoning Network\n(TRN) operating in a two-step inductive reasoning process: In the first step\nmodel initially answers each question with semantic and syntactic information.\nThe next step chains multiple questions on the same event to predict a\ntimeline, which is then used to ground the answers. Results on the TORQUE and\nTB-dense, TRC and TRE tasks respectively, demonstrate that TRN outperforms\nprevious methods by effectively resolving the spurious overlaps using the\npredicted timeline."}
{"id": "2506.14243", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14243", "abs": "https://arxiv.org/abs/2506.14243", "authors": ["Xiaohui Jiang", "Haijiang Zhu", "Chadei Li", "Fulin Tang", "Ning An"], "title": "Cross-Modal Geometric Hierarchy Fusion: An Implicit-Submap Driven Framework for Resilient 3D Place Recognition", "comment": null, "summary": "LiDAR-based place recognition serves as a crucial enabler for long-term\nautonomy in robotics and autonomous driving systems. Yet, prevailing\nmethodologies relying on handcrafted feature extraction face dual challenges:\n(1) Inconsistent point cloud density, induced by ego-motion dynamics and\nenvironmental disturbances during repeated traversals, leads to descriptor\ninstability, and (2) Representation fragility stems from reliance on\nsingle-level geometric abstractions that lack discriminative power in\nstructurally complex scenarios. To address these limitations, we propose a\nnovel framework that redefines 3D place recognition through density-agnostic\ngeometric reasoning. Specifically, we introduce an implicit 3D representation\nbased on elastic points, which is immune to the interference of original scene\npoint cloud density and achieves the characteristic of uniform distribution.\nSubsequently, we derive the occupancy grid and normal vector information of the\nscene from this implicit representation. Finally, with the aid of these two\ntypes of information, we obtain descriptors that fuse geometric information\nfrom both bird's-eye view (capturing macro-level spatial layouts) and 3D\nsegment (encoding micro-scale surface geometries) perspectives. We conducted\nextensive experiments on numerous datasets (KITTI, KITTI-360, MulRan, NCLT)\nacross diverse environments. The experimental results demonstrate that our\nmethod achieves state-of-the-art performance. Moreover, our approach strikes an\noptimal balance between accuracy, runtime, and memory optimization for\nhistorical maps, showcasing excellent Resilient and scalability. Our code will\nbe open-sourced in the future."}
{"id": "2506.14234", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14234", "abs": "https://arxiv.org/abs/2506.14234", "authors": ["Md Tanzib Hosain", "Salman Rahman", "Md Kishor Morol", "Md Rizwan Parvez"], "title": "Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just Like an Olympiad Team", "comment": null, "summary": "Despite impressive progress on complex reasoning, current large language\nmodels (LLMs) typically operate in isolation - treating each problem as an\nindependent attempt, without accumulating or integrating experiential\nknowledge. In contrast, expert problem solvers - such as Olympiad or\nprogramming contest teams - leverage a rich tapestry of experiences: absorbing\nmentorship from coaches, developing intuition from past problems, leveraging\nknowledge of tool usage and library functionality, adapting strategies based on\nthe expertise and experiences of peers, continuously refining their reasoning\nthrough trial and error, and learning from other related problems even during\ncompetition. We introduce Xolver, a training-free multi-agent reasoning\nframework that equips a black-box LLM with a persistent, evolving memory of\nholistic experience. Xolver integrates diverse experience modalities, including\nexternal and self-retrieval, tool use, collaborative interactions, agent-driven\nevaluation, and iterative refinement. By learning from relevant strategies,\ncode fragments, and abstract reasoning patterns at inference time, Xolver\navoids generating solutions from scratch - marking a transition from isolated\ninference toward experience-aware language agents. Built on both open-weight\nand proprietary models, Xolver consistently outperforms specialized reasoning\nagents. Even with lightweight backbones (e.g., QWQ-32B), it often surpasses\nadvanced models including Qwen3-235B, Gemini 2.5 Pro, o3, and o4-mini-high.\nWith o3-mini-high, it achieves new best results on GSM8K (98.1%), AIME'24\n(94.4%), AIME'25 (93.7%), Math-500 (99.8%), and LiveCodeBench-V5 (91.6%) -\nhighlighting holistic experience learning as a key step toward generalist\nagents capable of expert-level reasoning. Code and data are available at\nhttps://kagnlp.github.io/xolver.github.io/."}
{"id": "2506.14255", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14255", "abs": "https://arxiv.org/abs/2506.14255", "authors": ["Johannes Flotzinger", "Fabian Deuser", "Achref Jaziri", "Heiko Neumann", "Norbert Oswald", "Visvanathan Ramesh", "Thomas Braml"], "title": "synth-dacl: Does Synthetic Defect Data Enhance Segmentation Accuracy and Robustness for Real-World Bridge Inspections?", "comment": null, "summary": "Adequate bridge inspection is increasingly challenging in many countries due\nto growing ailing stocks, compounded with a lack of staff and financial\nresources. Automating the key task of visual bridge inspection, classification\nof defects and building components on pixel level, improves efficiency,\nincreases accuracy and enhances safety in the inspection process and resulting\nbuilding assessment. Models overtaking this task must cope with an assortment\nof real-world conditions. They must be robust to variations in image quality,\nas well as background texture, as defects often appear on surfaces of diverse\ntexture and degree of weathering. dacl10k is the largest and most diverse\ndataset for real-world concrete bridge inspections. However, the dataset\nexhibits class imbalance, which leads to notably poor model performance\nparticularly when segmenting fine-grained classes such as cracks and cavities.\nThis work introduces \"synth-dacl\", a compilation of three novel dataset\nextensions based on synthetic concrete textures. These extensions are designed\nto balance class distribution in dacl10k and enhance model performance,\nespecially for crack and cavity segmentation. When incorporating the synth-dacl\nextensions, we observe substantial improvements in model robustness across 15\nperturbed test sets. Notably, on the perturbed test set, a model trained on\ndacl10k combined with all synthetic extensions achieves a 2% increase in mean\nIoU, F1 score, Recall, and Precision compared to the same model trained solely\non dacl10k."}
{"id": "2506.14235", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14235", "abs": "https://arxiv.org/abs/2506.14235", "authors": ["Yimin Deng", "Yuxia Wu", "Yejing Wang", "Guoshuai Zhao", "Li Zhu", "Qidong Liu", "Derong Xu", "Zichuan Fu", "Xian Wu", "Yefeng Zheng", "Xiangyu Zhao", "Xueming Qian"], "title": "A Multi-Expert Structural-Semantic Hybrid Framework for Unveiling Historical Patterns in Temporal Knowledge Graphs", "comment": "ACL25 findings", "summary": "Temporal knowledge graph reasoning aims to predict future events with\nknowledge of existing facts and plays a key role in various downstream tasks.\nPrevious methods focused on either graph structure learning or semantic\nreasoning, failing to integrate dual reasoning perspectives to handle different\nprediction scenarios. Moreover, they lack the capability to capture the\ninherent differences between historical and non-historical events, which limits\ntheir generalization across different temporal contexts. To this end, we\npropose a Multi-Expert Structural-Semantic Hybrid (MESH) framework that employs\nthree kinds of expert modules to integrate both structural and semantic\ninformation, guiding the reasoning process for different events. Extensive\nexperiments on three datasets demonstrate the effectiveness of our approach."}
{"id": "2506.14256", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14256", "abs": "https://arxiv.org/abs/2506.14256", "authors": ["Deepak Ghimire", "Joonwhoan Lee"], "title": "Comparison of Two Methods for Stationary Incident Detection Based on Background Image", "comment": "8 pages, 6 figures", "summary": "In general, background subtraction-based methods are used to detect moving\nobjects in visual tracking applications. In this paper, we employed a\nbackground subtraction-based scheme to detect the temporarily stationary\nobjects. We proposed two schemes for stationary object detection, and we\ncompare those in terms of detection performance and computational complexity.\nIn the first approach, we used a single background, and in the second approach,\nwe used dual backgrounds, generated with different learning rates, in order to\ndetect temporarily stopped objects. Finally, we used normalized cross\ncorrelation (NCC) based image comparison to monitor and track the detected\nstationary object in a video scene. The proposed method is robust with partial\nocclusion, short-time fully occlusion, and illumination changes, and it can\noperate in real time."}
{"id": "2506.14248", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14248", "abs": "https://arxiv.org/abs/2506.14248", "authors": ["Chenghao Li", "Liu Liu", "Baosheng Yu", "Jiayan Qiu", "Yibing Zhan"], "title": "Re-Initialization Token Learning for Tool-Augmented Large Language Models", "comment": null, "summary": "Large language models have demonstrated exceptional performance, yet struggle\nwith complex tasks such as numerical reasoning, plan generation. Integrating\nexternal tools, such as calculators and databases, into large language models\n(LLMs) is crucial for enhancing problem-solving capabilities. Current methods\nassign a unique token to each tool, enabling LLMs to call tools through token\nprediction-similar to word generation. However, this approach fails to account\nfor the relationship between tool and word tokens, limiting adaptability within\npre-trained LLMs. To address this issue, we propose a novel token learning\nmethod that aligns tool tokens with the existing word embedding space from the\nperspective of initialization, thereby enhancing model performance. We begin by\nconstructing prior token embeddings for each tool based on the tool's name or\ndescription, which are used to initialize and regularize the learnable tool\ntoken embeddings. This ensures the learned embeddings are well-aligned with the\nword token space, improving tool call accuracy. We evaluate the method on tasks\nsuch as numerical reasoning, knowledge-based question answering, and embodied\nplan generation using GSM8K-XL, FuncQA, KAMEL, and VirtualHome datasets. The\nresults demonstrate clear improvements over recent baselines, including CoT,\nREACT, ICL, and ToolkenGPT, indicating that our approach effectively augments\nLLMs with tools through relevant tokens across diverse domains."}
{"id": "2506.14265", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14265", "abs": "https://arxiv.org/abs/2506.14265", "authors": ["Siran Dai", "Qianqian Xu", "Peisong Wen", "Yang Liu", "Qingming Huang"], "title": "Exploring Non-contrastive Self-supervised Representation Learning for Image-based Profiling", "comment": "CVPR 2025 Computer Vision for Drug Discovery", "summary": "Image-based cell profiling aims to create informative representations of cell\nimages. This technique is critical in drug discovery and has greatly advanced\nwith recent improvements in computer vision. Inspired by recent developments in\nnon-contrastive Self-Supervised Learning (SSL), this paper provides an initial\nexploration into training a generalizable feature extractor for cell images\nusing such methods. However, there are two major challenges: 1) There is a\nlarge difference between the distributions of cell images and natural images,\ncausing the view-generation process in existing SSL methods to fail; and 2)\nUnlike typical scenarios where each representation is based on a single image,\ncell profiling often involves multiple input images, making it difficult to\neffectively combine all available information. To overcome these challenges, we\npropose SSLProfiler, a non-contrastive SSL framework specifically designed for\ncell profiling. We introduce specialized data augmentation and representation\npost-processing methods tailored to cell images, which effectively address the\nissues mentioned above and result in a robust feature extractor. With these\nimprovements, SSLProfiler won the Cell Line Transferability challenge at CVPR\n2025."}
{"id": "2506.14285", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14285", "abs": "https://arxiv.org/abs/2506.14285", "authors": ["Seongbo Jang", "Minjin Jeon", "Jaehoon Lee", "Seonghyeon Lee", "Dongha Lee", "Hwanjo Yu"], "title": "From What to Respond to When to Respond: Timely Response Generation for Open-domain Dialogue Agents", "comment": "Work in progress", "summary": "While research on dialogue response generation has primarily focused on\ngenerating coherent responses conditioning on textual context, the critical\nquestion of when to respond grounded on the temporal context remains\nunderexplored. To bridge this gap, we propose a novel task called timely\ndialogue response generation and introduce the TimelyChat benchmark, which\nevaluates the capabilities of language models to predict appropriate time\nintervals and generate time-conditioned responses. Additionally, we construct a\nlarge-scale training dataset by leveraging unlabeled event knowledge from a\ntemporal commonsense knowledge graph and employing a large language model (LLM)\nto synthesize 55K event-driven dialogues. We then train Timer, a dialogue agent\ndesigned to proactively predict time intervals and generate timely responses\nthat align with those intervals. Experimental results show that Timer\noutperforms prompting-based LLMs and other fine-tuned baselines in both\nturn-level and dialogue-level evaluations. We publicly release our data, model,\nand code."}
{"id": "2506.14271", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14271", "abs": "https://arxiv.org/abs/2506.14271", "authors": ["Weiming Zhang", "Dingwen Xiao", "Aobotao Dai", "Yexin Liu", "Tianbo Pan", "Shiqi Wen", "Lei Chen", "Lin Wang"], "title": "Leader360V: The Large-scale, Real-world 360 Video Dataset for Multi-task Learning in Diverse Environment", "comment": "23 pages, 16 figures", "summary": "360 video captures the complete surrounding scenes with the ultra-large field\nof view of 360X180. This makes 360 scene understanding tasks, eg, segmentation\nand tracking, crucial for appications, such as autonomous driving, robotics.\nWith the recent emergence of foundation models, the community is, however,\nimpeded by the lack of large-scale, labelled real-world datasets. This is\ncaused by the inherent spherical properties, eg, severe distortion in polar\nregions, and content discontinuities, rendering the annotation costly yet\ncomplex. This paper introduces Leader360V, the first large-scale, labeled\nreal-world 360 video datasets for instance segmentation and tracking. Our\ndatasets enjoy high scene diversity, ranging from indoor and urban settings to\nnatural and dynamic outdoor scenes. To automate annotation, we design an\nautomatic labeling pipeline, which subtly coordinates pre-trained 2D segmentors\nand large language models to facilitate the labeling. The pipeline operates in\nthree novel stages. Specifically, in the Initial Annotation Phase, we introduce\na Semantic- and Distortion-aware Refinement module, which combines object mask\nproposals from multiple 2D segmentors with LLM-verified semantic labels. These\nare then converted into mask prompts to guide SAM2 in generating\ndistortion-aware masks for subsequent frames. In the Auto-Refine Annotation\nPhase, missing or incomplete regions are corrected either by applying the SDR\nagain or resolving the discontinuities near the horizontal borders. The Manual\nRevision Phase finally incorporates LLMs and human annotators to further refine\nand validate the annotations. Extensive user studies and evaluations\ndemonstrate the effectiveness of our labeling pipeline. Meanwhile, experiments\nconfirm that Leader360V significantly enhances model performance for 360 video\nsegmentation and tracking, paving the way for more scalable 360 scene\nunderstanding."}
{"id": "2506.14302", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14302", "abs": "https://arxiv.org/abs/2506.14302", "authors": ["Xueyang Feng", "Jingsen Zhang", "Jiakai Tang", "Wei Li", "Guohao Cai", "Xu Chen", "Quanyu Dai", "Yue Zhu", "Zhenhua Dong"], "title": "Expectation Confirmation Preference Optimization for Multi-Turn Conversational Recommendation Agent", "comment": "Accepted to Findings of ACL 2025", "summary": "Recent advancements in Large Language Models (LLMs) have significantly\npropelled the development of Conversational Recommendation Agents (CRAs).\nHowever, these agents often generate short-sighted responses that fail to\nsustain user guidance and meet expectations. Although preference optimization\nhas proven effective in aligning LLMs with user expectations, it remains costly\nand performs poorly in multi-turn dialogue. To address this challenge, we\nintroduce a novel multi-turn preference optimization (MTPO) paradigm ECPO,\nwhich leverages Expectation Confirmation Theory to explicitly model the\nevolution of user satisfaction throughout multi-turn dialogues, uncovering the\nunderlying causes of dissatisfaction. These causes can be utilized to support\ntargeted optimization of unsatisfactory responses, thereby achieving turn-level\npreference optimization. ECPO ingeniously eliminates the significant sampling\noverhead of existing MTPO methods while ensuring the optimization process\ndrives meaningful improvements. To support ECPO, we introduce an LLM-based user\nsimulator, AILO, to simulate user feedback and perform expectation confirmation\nduring conversational recommendations. Experimental results show that ECPO\nsignificantly enhances CRA's interaction capabilities, delivering notable\nimprovements in both efficiency and effectiveness over existing MTPO methods."}
{"id": "2506.14322", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.14322", "abs": "https://arxiv.org/abs/2506.14322", "authors": ["Avigail Cohen Rimon", "Mirela Ben-Chen", "Or Litany"], "title": "FRIDU: Functional Map Refinement with Guided Image Diffusion", "comment": "Accepted to SGP 2025 (Symposium on Geometry Processing)", "summary": "We propose a novel approach for refining a given correspondence map between\ntwo shapes. A correspondence map represented as a functional map, namely a\nchange of basis matrix, can be additionally treated as a 2D image. With this\nperspective, we train an image diffusion model directly in the space of\nfunctional maps, enabling it to generate accurate maps conditioned on an\ninaccurate initial map. The training is done purely in the functional space,\nand thus is highly efficient. At inference time, we use the pointwise map\ncorresponding to the current functional map as guidance during the diffusion\nprocess. The guidance can additionally encourage different functional map\nobjectives, such as orthogonality and commutativity with the Laplace-Beltrami\noperator. We show that our approach is competitive with state-of-the-art\nmethods of map refinement and that guided diffusion models provide a promising\npathway to functional map processing."}
{"id": "2506.14335", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14335", "abs": "https://arxiv.org/abs/2506.14335", "authors": ["Silvia Casola", "Yang Janet Liu", "Siyao Peng", "Oliver Kraus", "Albert Gatt", "Barbara Plank"], "title": "Evaluation Should Not Ignore Variation: On the Impact of Reference Set Choice on Summarization Metrics", "comment": "17 pages, 13 figures", "summary": "Human language production exhibits remarkable richness and variation,\nreflecting diverse communication styles and intents. However, this variation is\noften overlooked in summarization evaluation. While having multiple reference\nsummaries is known to improve correlation with human judgments, the impact of\nusing different reference sets on reference-based metrics has not been\nsystematically investigated. This work examines the sensitivity of widely used\nreference-based metrics in relation to the choice of reference sets, analyzing\nthree diverse multi-reference summarization datasets: SummEval, GUMSum, and\nDUC2004. We demonstrate that many popular metrics exhibit significant\ninstability. This instability is particularly concerning for n-gram-based\nmetrics like ROUGE, where model rankings vary depending on the reference sets,\nundermining the reliability of model comparisons. We also collect human\njudgments on LLM outputs for genre-diverse data and examine their correlation\nwith metrics to supplement existing findings beyond newswire summaries, finding\nweak-to-no correlation. Taken together, we recommend incorporating reference\nset variation into summarization evaluation to enhance consistency alongside\ncorrelation with human judgments, especially when evaluating LLMs."}
{"id": "2506.14350", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.14350", "abs": "https://arxiv.org/abs/2506.14350", "authors": ["Zoubida Ameur", "Frédéric Lefebvre", "Philippe De Lagrange", "Miloš Radosavljević"], "title": "FGA-NN: Film Grain Analysis Neural Network", "comment": null, "summary": "Film grain, once a by-product of analog film, is now present in most\ncinematographic content for aesthetic reasons. However, when such content is\ncompressed at medium to low bitrates, film grain is lost due to its random\nnature. To preserve artistic intent while compressing efficiently, film grain\nis analyzed and modeled before encoding and synthesized after decoding. This\npaper introduces FGA-NN, the first learning-based film grain analysis method to\nestimate conventional film grain parameters compatible with conventional\nsynthesis. Quantitative and qualitative results demonstrate FGA-NN's superior\nbalance between analysis accuracy and synthesis complexity, along with its\nrobustness and applicability."}
{"id": "2506.14345", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.14345", "abs": "https://arxiv.org/abs/2506.14345", "authors": ["Bruno Martins", "Piotr Szymański", "Piotr Gramacki"], "title": "A Vision for Geo-Temporal Deep Research Systems: Towards Comprehensive, Transparent, and Reproducible Geo-Temporal Information Synthesis", "comment": null, "summary": "The emergence of Large Language Models (LLMs) has transformed information\naccess, with current LLMs also powering deep research systems that can generate\ncomprehensive report-style answers, through planned iterative search,\nretrieval, and reasoning. Still, current deep research systems lack the\ngeo-temporal capabilities that are essential for answering context-rich\nquestions involving geographic and/or temporal constraints, frequently\noccurring in domains like public health, environmental science, or\nsocio-economic analysis. This paper reports our vision towards next generation\nsystems, identifying important technical, infrastructural, and evaluative\nchallenges in integrating geo-temporal reasoning into deep research pipelines.\nWe argue for augmenting retrieval and synthesis processes with the ability to\nhandle geo-temporal constraints, supported by open and reproducible\ninfrastructures and rigorous evaluation protocols. Our vision outlines a path\ntowards more advanced and geo-temporally aware deep research systems, of\npotential impact to the future of AI-driven information access."}
{"id": "2506.14356", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14356", "abs": "https://arxiv.org/abs/2506.14356", "authors": ["Xiaoqi Wang", "Yi Wang", "Lap-Pui Chau"], "title": "EVA02-AT: Egocentric Video-Language Understanding with Spatial-Temporal Rotary Positional Embeddings and Symmetric Optimization", "comment": null, "summary": "Egocentric video-language understanding demands both high efficiency and\naccurate spatial-temporal modeling. Existing approaches face three key\nchallenges: 1) Excessive pre-training cost arising from multi-stage\npre-training pipelines, 2) Ineffective spatial-temporal encoding due to\nmanually split 3D rotary positional embeddings that hinder feature\ninteractions, and 3) Imprecise learning objectives in soft-label multi-instance\nretrieval, which neglect negative pair correlations. In this paper, we\nintroduce EVA02-AT, a suite of EVA02-based video-language foundation models\ntailored to egocentric video understanding tasks. EVA02-AT first efficiently\ntransfers an image-based CLIP model into a unified video encoder via a\nsingle-stage pretraining. Second, instead of applying rotary positional\nembeddings to isolated dimensions, we introduce spatial-temporal rotary\npositional embeddings along with joint attention, which can effectively encode\nboth spatial and temporal information on the entire hidden dimension. This\njoint encoding of spatial-temporal features enables the model to learn\ncross-axis relationships, which are crucial for accurately modeling motion and\ninteraction in videos. Third, focusing on multi-instance video-language\nretrieval tasks, we introduce the Symmetric Multi-Similarity (SMS) loss and a\nnovel training framework that advances all soft labels for both positive and\nnegative pairs, providing a more precise learning objective. Extensive\nexperiments on Ego4D, EPIC-Kitchens-100, and Charades-Ego under zero-shot and\nfine-tuning settings demonstrate that EVA02-AT achieves state-of-the-art\nperformance across diverse egocentric video-language tasks with fewer\nparameters. Models with our SMS loss also show significant performance gains on\nmulti-instance retrieval benchmarks. Our code and models are publicly available\nat https://github.com/xqwang14/EVA02-AT ."}
{"id": "2506.14370", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14370", "abs": "https://arxiv.org/abs/2506.14370", "authors": ["Amrit Poudel", "Yifan Ding", "Jurgen Pfeffer", "Tim Weninger"], "title": "Digital Gatekeepers: Google's Role in Curating Hashtags and Subreddits", "comment": "Accepted to ACL 2025 Main", "summary": "Search engines play a crucial role as digital gatekeepers, shaping the\nvisibility of Web and social media content through algorithmic curation. This\nstudy investigates how search engines like Google selectively promotes or\nsuppresses certain hashtags and subreddits, impacting the information users\nencounter. By comparing search engine results with nonsampled data from Reddit\nand Twitter/X, we reveal systematic biases in content visibility. Google's\nalgorithms tend to suppress subreddits and hashtags related to sexually\nexplicit material, conspiracy theories, advertisements, and cryptocurrencies,\nwhile promoting content associated with higher engagement. These findings\nsuggest that Google's gatekeeping practices influence public discourse by\ncurating the social media narratives available to users."}
{"id": "2506.14362", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14362", "abs": "https://arxiv.org/abs/2506.14362", "authors": ["Daniele Rege Cambrin", "Eleonora Poeta", "Eliana Pastor", "Isaac Corley", "Tania Cerquitelli", "Elena Baralis", "Paolo Garza"], "title": "HydroChronos: Forecasting Decades of Surface Water Change", "comment": null, "summary": "Forecasting surface water dynamics is crucial for water resource management\nand climate change adaptation. However, the field lacks comprehensive datasets\nand standardized benchmarks. In this paper, we introduce HydroChronos, a\nlarge-scale, multi-modal spatiotemporal dataset for surface water dynamics\nforecasting designed to address this gap. We couple the dataset with three\nforecasting tasks. The dataset includes over three decades of aligned Landsat 5\nand Sentinel-2 imagery, climate data, and Digital Elevation Models for diverse\nlakes and rivers across Europe, North America, and South America. We also\npropose AquaClimaTempo UNet, a novel spatiotemporal architecture with a\ndedicated climate data branch, as a strong benchmark baseline. Our model\nsignificantly outperforms a Persistence baseline for forecasting future water\ndynamics by +14% and +11% F1 across change detection and direction of change\nclassification tasks, and by +0.1 MAE on the magnitude of change regression.\nFinally, we conduct an Explainable AI analysis to identify the key climate\nvariables and input channels that influence surface water change, providing\ninsights to inform and guide future modeling efforts."}
{"id": "2506.14371", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.14371", "abs": "https://arxiv.org/abs/2506.14371", "authors": ["Lucile Favero", "Daniel Frases", "Juan Antonio Pérez-Ortiz", "Tanja Käser", "Nuria Oliver"], "title": "ELLIS Alicante at CQs-Gen 2025: Winning the critical thinking questions shared task: LLM-based question generation and selection", "comment": "Proceedings of the 12th Workshop on Argument Mining", "summary": "The widespread adoption of chat interfaces based on Large Language Models\n(LLMs) raises concerns about promoting superficial learning and undermining the\ndevelopment of critical thinking skills. Instead of relying on LLMs purely for\nretrieving factual information, this work explores their potential to foster\ndeeper reasoning by generating critical questions that challenge unsupported or\nvague claims in debate interventions. This study is part of a shared task of\nthe 12th Workshop on Argument Mining, co-located with ACL 2025, focused on\nautomatic critical question generation. We propose a two-step framework\ninvolving two small-scale open source language models: a Questioner that\ngenerates multiple candidate questions and a Judge that selects the most\nrelevant ones. Our system ranked first in the shared task competition,\ndemonstrating the potential of the proposed LLM-based approach to encourage\ncritical engagement with argumentative texts."}
{"id": "2506.14367", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14367", "abs": "https://arxiv.org/abs/2506.14367", "authors": ["Sumshun Nahar Eity", "Mahin Montasir Afif", "Tanisha Fairooz", "Md. Mortuza Ahmmed", "Md Saef Ullah Miah"], "title": "DGG-XNet: A Hybrid Deep Learning Framework for Multi-Class Brain Disease Classification with Explainable AI", "comment": null, "summary": "Accurate diagnosis of brain disorders such as Alzheimer's disease and brain\ntumors remains a critical challenge in medical imaging. Conventional methods\nbased on manual MRI analysis are often inefficient and error-prone. To address\nthis, we propose DGG-XNet, a hybrid deep learning model integrating VGG16 and\nDenseNet121 to enhance feature extraction and classification. DenseNet121\npromotes feature reuse and efficient gradient flow through dense connectivity,\nwhile VGG16 contributes strong hierarchical spatial representations. Their\nfusion enables robust multiclass classification of neurological conditions.\nGrad-CAM is applied to visualize salient regions, enhancing model transparency.\nTrained on a combined dataset from BraTS 2021 and Kaggle, DGG-XNet achieved a\ntest accuracy of 91.33\\%, with precision, recall, and F1-score all exceeding\n91\\%. These results highlight DGG-XNet's potential as an effective and\ninterpretable tool for computer-aided diagnosis (CAD) of neurodegenerative and\noncological brain disorders."}
{"id": "2506.14397", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14397", "abs": "https://arxiv.org/abs/2506.14397", "authors": ["Yeonkyoung So", "Gyuseong Lee", "Sungmok Jung", "Joonhak Lee", "JiA Kang", "Sangho Kim", "Jaejin Lee"], "title": "Thunder-NUBench: A Benchmark for LLMs' Sentence-Level Negation Understanding", "comment": null, "summary": "Negation is a fundamental linguistic phenomenon that poses persistent\nchallenges for Large Language Models (LLMs), particularly in tasks requiring\ndeep semantic understanding. Existing benchmarks often treat negation as a side\ncase within broader tasks like natural language inference, resulting in a lack\nof benchmarks that exclusively target negation understanding. In this work, we\nintroduce \\textbf{Thunder-NUBench}, a novel benchmark explicitly designed to\nassess sentence-level negation understanding in LLMs. Thunder-NUBench goes\nbeyond surface-level cue detection by contrasting standard negation with\nstructurally diverse alternatives such as local negation, contradiction, and\nparaphrase. The benchmark consists of manually curated sentence-negation pairs\nand a multiple-choice dataset that enables in-depth evaluation of models'\nnegation understanding."}
{"id": "2506.14373", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14373", "abs": "https://arxiv.org/abs/2506.14373", "authors": ["Junyeob Baek", "Hosung Lee", "Christopher Hoang", "Mengye Ren", "Sungjin Ahn"], "title": "Discrete JEPA: Learning Discrete Token Representations without Reconstruction", "comment": null, "summary": "The cornerstone of cognitive intelligence lies in extracting hidden patterns\nfrom observations and leveraging these principles to systematically predict\nfuture outcomes. However, current image tokenization methods demonstrate\nsignificant limitations in tasks requiring symbolic abstraction and logical\nreasoning capabilities essential for systematic inference. To address this\nchallenge, we propose Discrete-JEPA, extending the latent predictive coding\nframework with semantic tokenization and novel complementary objectives to\ncreate robust tokenization for symbolic reasoning tasks. Discrete-JEPA\ndramatically outperforms baselines on visual symbolic prediction tasks, while\nstriking visual evidence reveals the spontaneous emergence of deliberate\nsystematic patterns within the learned semantic token space. Though an initial\nmodel, our approach promises a significant impact for advancing Symbolic world\nmodeling and planning capabilities in artificial intelligence systems."}
{"id": "2506.14407", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14407", "abs": "https://arxiv.org/abs/2506.14407", "authors": ["Zeinab Sadat Taghavi", "Ali Modarressi", "Yunpu Ma", "Hinrich Schütze"], "title": "ImpliRet: Benchmarking the Implicit Fact Retrieval Challenge", "comment": null, "summary": "Retrieval systems are central to many NLP pipelines, but often rely on\nsurface-level cues such as keyword overlap and lexical semantic similarity. To\nevaluate retrieval beyond these shallow signals, recent benchmarks introduce\nreasoning-heavy queries; however, they primarily shift the burden to query-side\nprocessing techniques -- like prompting or multi-hop retrieval -- that can help\nresolve complexity. In contrast, we present ImpliRet, a benchmark that shifts\nthe reasoning challenge to document-side processing: The queries are simple,\nbut relevance depends on facts stated implicitly in documents through temporal\n(e.g., resolving \"two days ago\"), arithmetic, and world knowledge\nrelationships. We evaluate a range of sparse and dense retrievers, all of which\nstruggle in this setting: the best nDCG@10 is only 15.07%. We also test whether\nlong-context models can overcome this limitation. But even with a short context\nof only ten documents, including the positive document, GPT-4.1 scores only\n35.06%, showing that document-side reasoning remains a challenge. Our codes are\navailable at github.com/ZeinabTaghavi/IMPLIRET.Contribution."}
{"id": "2506.14382", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14382", "abs": "https://arxiv.org/abs/2506.14382", "authors": ["Ning Zhou", "Shanxiong Chen", "Mingting Zhou", "Haigang Sui", "Lieyun Hu", "Han Li", "Li Hua", "Qiming Zhou"], "title": "DepthSeg: Depth prompting in remote sensing semantic segmentation", "comment": null, "summary": "Remote sensing semantic segmentation is crucial for extracting detailed land\nsurface information, enabling applications such as environmental monitoring,\nland use planning, and resource assessment. In recent years, advancements in\nartificial intelligence have spurred the development of automatic remote\nsensing semantic segmentation methods. However, the existing semantic\nsegmentation methods focus on distinguishing spectral characteristics of\ndifferent objects while ignoring the differences in the elevation of the\ndifferent targets. This results in land cover misclassification in complex\nscenarios involving shadow occlusion and spectral confusion. In this paper, we\nintroduce a depth prompting two-dimensional (2D) remote sensing semantic\nsegmentation framework (DepthSeg). It automatically models depth/height\ninformation from 2D remote sensing images and integrates it into the semantic\nsegmentation framework to mitigate the effects of spectral confusion and shadow\nocclusion. During the feature extraction phase of DepthSeg, we introduce a\nlightweight adapter to enable cost-effective fine-tuning of the large-parameter\nvision transformer encoder pre-trained by natural images. In the depth\nprompting phase, we propose a depth prompter to model depth/height features\nexplicitly. In the semantic prediction phase, we introduce a semantic\nclassification decoder that couples the depth prompts with high-dimensional\nland-cover features, enabling accurate extraction of land-cover types.\nExperiments on the LiuZhou dataset validate the advantages of the DepthSeg\nframework in land cover mapping tasks. Detailed ablation studies further\nhighlight the significance of the depth prompts in remote sensing semantic\nsegmentation."}
{"id": "2506.14429", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14429", "abs": "https://arxiv.org/abs/2506.14429", "authors": ["Xiaoran Liu", "Zhigeng Liu", "Zengfeng Huang", "Qipeng Guo", "Ziwei He", "Xipeng Qiu"], "title": "LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs", "comment": "16 pages, 12 figures, work in progress", "summary": "Large Language Diffusion Models, or diffusion LLMs, have emerged as a\nsignificant focus in NLP research, with substantial effort directed toward\nunderstanding their scalability and downstream task performance. However, their\nlong-context capabilities remain unexplored, lacking systematic analysis or\nmethods for context extension. In this work, we present the first systematic\ninvestigation comparing the long-context performance of diffusion LLMs and\ntraditional auto-regressive LLMs. We first identify a unique characteristic of\ndiffusion LLMs, unlike auto-regressive LLMs, they maintain remarkably\n\\textbf{\\textit{stable perplexity}} during direct context extrapolation.\nFurthermore, where auto-regressive models fail outright during the\nNeedle-In-A-Haystack task with context exceeding their pretrained length, we\ndiscover diffusion LLMs exhibit a distinct \\textbf{\\textit{local perception}}\nphenomenon, enabling successful retrieval from recent context segments. We\nexplain both phenomena through the lens of Rotary Position Embedding (RoPE)\nscaling theory. Building on these observations, we propose LongLLaDA, a\ntraining-free method that integrates LLaDA with the NTK-based RoPE\nextrapolation. Our results validate that established extrapolation scaling laws\nremain effective for extending the context windows of diffusion LLMs.\nFurthermore, we identify long-context tasks where diffusion LLMs outperform\nauto-regressive LLMs and others where they fall short. Consequently, this study\nestablishes the first context extrapolation method for diffusion LLMs while\nproviding essential theoretical insights and empirical benchmarks critical for\nadvancing future research on long-context diffusion LLMs."}
{"id": "2506.14384", "categories": ["cs.CV", "I.4"], "pdf": "https://arxiv.org/pdf/2506.14384", "abs": "https://arxiv.org/abs/2506.14384", "authors": ["Huan Kang", "Hui Li", "Xiao-Jun Wu", "Tianyang Xu", "Rui Wang", "Chunyang Cheng", "Josef Kittler"], "title": "GrFormer: A Novel Transformer on Grassmann Manifold for Infrared and Visible Image Fusion", "comment": "16 pages, 11 figures", "summary": "In the field of image fusion, promising progress has been made by modeling\ndata from different modalities as linear subspaces.\n  However, in practice, the source images are often located in a non-Euclidean\nspace, where the Euclidean methods usually cannot\n  encapsulate the intrinsic topological structure. Typically, the inner product\nperformed in the Euclidean space calculates the algebraic\n  similarity rather than the semantic similarity, which results in undesired\nattention output and a decrease in fusion performance.\n  While the balance of low-level details and high-level semantics should be\nconsidered in infrared and visible image fusion task. To\n  address this issue, in this paper, we propose a novel attention mechanism\nbased on Grassmann manifold for infrared and visible\n  image fusion (GrFormer). Specifically, our method constructs a low-rank\nsubspace mapping through projection constraints on the\n  Grassmann manifold, compressing attention features into subspaces of varying\nrank levels. This forces the features to decouple into\n  high-frequency details (local low-rank) and low-frequency semantics (global\nlow-rank), thereby achieving multi-scale semantic\n  fusion. Additionally, to effectively integrate the significant information,\nwe develop a cross-modal fusion strategy (CMS) based on\n  a covariance mask to maximise the complementary properties between different\nmodalities and to suppress the features with high\n  correlation, which are deemed redundant. The experimental results demonstrate\nthat our network outperforms SOTA methods both\n  qualitatively and quantitatively on multiple image fusion benchmarks. The\ncodes are available at https://github.com/Shaoyun2023."}
{"id": "2506.14448", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14448", "abs": "https://arxiv.org/abs/2506.14448", "authors": ["Jiayin Wang", "Zhiquang Guo", "Weizhi Ma", "Min Zhang"], "title": "How Far Can LLMs Improve from Experience? Measuring Test-Time Learning Ability in LLMs with Human Comparison", "comment": null, "summary": "As evaluation designs of large language models may shape our trajectory\ntoward artificial general intelligence, comprehensive and forward-looking\nassessment is essential. Existing benchmarks primarily assess static knowledge,\nwhile intelligence also entails the ability to rapidly learn from experience.\nTo this end, we advocate for the evaluation of Test-time Learning, the capacity\nto improve performance in experience-based, reasoning-intensive tasks during\ntest time. In this work, we propose semantic games as effective testbeds for\nevaluating test-time learning, due to their resistance to saturation and\ninherent demand for strategic reasoning. We introduce an objective evaluation\nframework that compares model performance under both limited and cumulative\nexperience settings, and contains four forms of experience representation. To\nprovide a comparative baseline, we recruit eight human participants to complete\nthe same task. Results show that LLMs exhibit measurable test-time learning\ncapabilities; however, their improvements are less stable under cumulative\nexperience and progress more slowly than those observed in humans. These\nfindings underscore the potential of LLMs as general-purpose learning machines,\nwhile also revealing a substantial intellectual gap between models and humans,\nirrespective of how well LLMs perform on static benchmarks."}
{"id": "2506.14399", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14399", "abs": "https://arxiv.org/abs/2506.14399", "authors": ["Tian Xia", "Fabio De Sousa Ribeiro", "Rajat R Rasal", "Avinash Kori", "Raghav Mehta", "Ben Glocker"], "title": "Decoupled Classifier-Free Guidance for Counterfactual Diffusion Models", "comment": null, "summary": "Counterfactual image generation aims to simulate realistic visual outcomes\nunder specific causal interventions. Diffusion models have recently emerged as\na powerful tool for this task, combining DDIM inversion with conditional\ngeneration via classifier-free guidance (CFG). However, standard CFG applies a\nsingle global weight across all conditioning variables, which can lead to poor\nidentity preservation and spurious attribute changes - a phenomenon known as\nattribute amplification. To address this, we propose Decoupled Classifier-Free\nGuidance (DCFG), a flexible and model-agnostic framework that introduces\ngroup-wise conditioning control. DCFG builds on an attribute-split embedding\nstrategy that disentangles semantic inputs, enabling selective guidance on\nuser-defined attribute groups. For counterfactual generation, we partition\nattributes into intervened and invariant sets based on a causal graph and apply\ndistinct guidance to each. Experiments on CelebA-HQ, MIMIC-CXR, and EMBED show\nthat DCFG improves intervention fidelity, mitigates unintended changes, and\nenhances reversibility, enabling more faithful and interpretable counterfactual\nimage generation."}
{"id": "2506.14474", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.14474", "abs": "https://arxiv.org/abs/2506.14474", "authors": ["Eyal German", "Sagiv Antebi", "Edan Habler", "Asaf Shabtai", "Yuval Elovici"], "title": "LexiMark: Robust Watermarking via Lexical Substitutions to Enhance Membership Verification of an LLM's Textual Training Data", "comment": null, "summary": "Large language models (LLMs) can be trained or fine-tuned on data obtained\nwithout the owner's consent. Verifying whether a specific LLM was trained on\nparticular data instances or an entire dataset is extremely challenging.\nDataset watermarking addresses this by embedding identifiable modifications in\ntraining data to detect unauthorized use. However, existing methods often lack\nstealth, making them relatively easy to detect and remove. In light of these\nlimitations, we propose LexiMark, a novel watermarking technique designed for\ntext and documents, which embeds synonym substitutions for carefully selected\nhigh-entropy words. Our method aims to enhance an LLM's memorization\ncapabilities on the watermarked text without altering the semantic integrity of\nthe text. As a result, the watermark is difficult to detect, blending\nseamlessly into the text with no visible markers, and is resistant to removal\ndue to its subtle, contextually appropriate substitutions that evade automated\nand manual detection. We evaluated our method using baseline datasets from\nrecent studies and seven open-source models: LLaMA-1 7B, LLaMA-3 8B, Mistral\n7B, Pythia 6.9B, as well as three smaller variants from the Pythia family\n(160M, 410M, and 1B). Our evaluation spans multiple training settings,\nincluding continued pretraining and fine-tuning scenarios. The results\ndemonstrate significant improvements in AUROC scores compared to existing\nmethods, underscoring our method's effectiveness in reliably verifying whether\nunauthorized watermarked data was used in LLM training."}
{"id": "2506.14404", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14404", "abs": "https://arxiv.org/abs/2506.14404", "authors": ["Nikos Spyrou", "Athanasios Vlontzos", "Paraskevas Pegios", "Thomas Melistas", "Nefeli Gkouti", "Yannis Panagakis", "Giorgos Papanastasiou", "Sotirios A. Tsaftaris"], "title": "Causally Steered Diffusion for Automated Video Counterfactual Generation", "comment": null, "summary": "Adapting text-to-image (T2I) latent diffusion models for video editing has\nshown strong visual fidelity and controllability, but challenges remain in\nmaintaining causal relationships in video content. Edits affecting causally\ndependent attributes risk generating unrealistic or misleading outcomes if\nthese relationships are ignored. In this work, we propose a causally faithful\nframework for counterfactual video generation, guided by a vision-language\nmodel (VLM). Our method is agnostic to the underlying video editing system and\ndoes not require access to its internal mechanisms or finetuning. Instead, we\nguide the generation by optimizing text prompts based on an assumed causal\ngraph, addressing the challenge of latent space control in LDMs. We evaluate\nour approach using standard video quality metrics and counterfactual-specific\ncriteria, such as causal effectiveness and minimality. Our results demonstrate\nthat causally faithful video counterfactuals can be effectively generated\nwithin the learned distribution of LDMs through prompt-based causal steering.\nWith its compatibility with any black-box video editing system, our method\nholds significant potential for generating realistic \"what-if\" video scenarios\nin diverse areas such as healthcare and digital media."}
{"id": "2506.14493", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.14493", "abs": "https://arxiv.org/abs/2506.14493", "authors": ["Jiyuan Fu", "Kaixun Jiang", "Lingyi Hong", "Jinglun Li", "Haijing Guo", "Dingkang Yang", "Zhaoyu Chen", "Wenqiang Zhang"], "title": "LingoLoop Attack: Trapping MLLMs via Linguistic Context and State Entrapment into Endless Loops", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have shown great promise but require\nsubstantial computational resources during inference. Attackers can exploit\nthis by inducing excessive output, leading to resource exhaustion and service\ndegradation. Prior energy-latency attacks aim to increase generation time by\nbroadly shifting the output token distribution away from the EOS token, but\nthey neglect the influence of token-level Part-of-Speech (POS) characteristics\non EOS and sentence-level structural patterns on output counts, limiting their\nefficacy. To address this, we propose LingoLoop, an attack designed to induce\nMLLMs to generate excessively verbose and repetitive sequences. First, we find\nthat the POS tag of a token strongly affects the likelihood of generating an\nEOS token. Based on this insight, we propose a POS-Aware Delay Mechanism to\npostpone EOS token generation by adjusting attention weights guided by POS\ninformation. Second, we identify that constraining output diversity to induce\nrepetitive loops is effective for sustained generation. We introduce a\nGenerative Path Pruning Mechanism that limits the magnitude of hidden states,\nencouraging the model to produce persistent loops. Extensive experiments\ndemonstrate LingoLoop can increase generated tokens by up to 30 times and\nenergy consumption by a comparable factor on models like Qwen2.5-VL-3B,\nconsistently driving MLLMs towards their maximum generation limits. These\nfindings expose significant MLLMs' vulnerabilities, posing challenges for their\nreliable deployment. The code will be released publicly following the paper's\nacceptance."}
{"id": "2506.14418", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14418", "abs": "https://arxiv.org/abs/2506.14418", "authors": ["Jiayi Chen", "Yanbiao Ma", "Andi Zhang", "Weidong Tang", "Wei Dai", "Bowei Liu"], "title": "Compositional Attribute Imbalance in Vision Datasets", "comment": null, "summary": "Visual attribute imbalance is a common yet underexplored issue in image\nclassification, significantly impacting model performance and generalization.\nIn this work, we first define the first-level and second-level attributes of\nimages and then introduce a CLIP-based framework to construct a visual\nattribute dictionary, enabling automatic evaluation of image attributes. By\nsystematically analyzing both single-attribute imbalance and compositional\nattribute imbalance, we reveal how the rarity of attributes affects model\nperformance. To tackle these challenges, we propose adjusting the sampling\nprobability of samples based on the rarity of their compositional attributes.\nThis strategy is further integrated with various data augmentation techniques\n(such as CutMix, Fmix, and SaliencyMix) to enhance the model's ability to\nrepresent rare attributes. Extensive experiments on benchmark datasets\ndemonstrate that our method effectively mitigates attribute imbalance, thereby\nimproving the robustness and fairness of deep neural networks. Our research\nhighlights the importance of modeling visual attribute distributions and\nprovides a scalable solution for long-tail image classification tasks."}
{"id": "2506.14532", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14532", "abs": "https://arxiv.org/abs/2506.14532", "authors": ["Can Zheng", "Jiguang He", "Chung G. Kang", "Guofa Cai", "Zitong Yu", "Merouane Debbah"], "title": "M2BeamLLM: Multimodal Sensing-empowered mmWave Beam Prediction with Large Language Models", "comment": "13 pages, 20 figures", "summary": "This paper introduces a novel neural network framework called M2BeamLLM for\nbeam prediction in millimeter-wave (mmWave) massive multi-input multi-output\n(mMIMO) communication systems. M2BeamLLM integrates multi-modal sensor data,\nincluding images, radar, LiDAR, and GPS, leveraging the powerful reasoning\ncapabilities of large language models (LLMs) such as GPT-2 for beam prediction.\nBy combining sensing data encoding, multimodal alignment and fusion, and\nsupervised fine-tuning (SFT), M2BeamLLM achieves significantly higher beam\nprediction accuracy and robustness, demonstrably outperforming traditional deep\nlearning (DL) models in both standard and few-shot scenarios. Furthermore, its\nprediction performance consistently improves with increased diversity in\nsensing modalities. Our study provides an efficient and intelligent beam\nprediction solution for vehicle-to-infrastructure (V2I) mmWave communication\nsystems."}
{"id": "2506.14428", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14428", "abs": "https://arxiv.org/abs/2506.14428", "authors": ["Ruihao Xi", "Xuekuan Wang", "Yongcheng Li", "Shuhua Li", "Zichen Wang", "Yiwei Wang", "Feng Wei", "Cairong Zhao"], "title": "Toward Rich Video Human-Motion2D Generation", "comment": null, "summary": "Generating realistic and controllable human motions, particularly those\ninvolving rich multi-character interactions, remains a significant challenge\ndue to data scarcity and the complexities of modeling inter-personal dynamics.\nTo address these limitations, we first introduce a new large-scale rich video\nhuman motion 2D dataset (Motion2D-Video-150K) comprising 150,000 video\nsequences. Motion2D-Video-150K features a balanced distribution of diverse\nsingle-character and, crucially, double-character interactive actions, each\npaired with detailed textual descriptions. Building upon this dataset, we\npropose a novel diffusion-based rich video human motion2D generation (RVHM2D)\nmodel. RVHM2D incorporates an enhanced textual conditioning mechanism utilizing\neither dual text encoders (CLIP-L/B) or T5-XXL with both global and local\nfeatures. We devise a two-stage training strategy: the model is first trained\nwith a standard diffusion objective, and then fine-tuned using reinforcement\nlearning with an FID-based reward to further enhance motion realism and text\nalignment. Extensive experiments demonstrate that RVHM2D achieves leading\nperformance on the Motion2D-Video-150K benchmark in generating both single and\ninteractive double-character scenarios."}
{"id": "2506.14562", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.14562", "abs": "https://arxiv.org/abs/2506.14562", "authors": ["Di He", "Ajay Jaiswal", "Songjun Tu", "Li Shen", "Ganzhao Yuan", "Shiwei Liu", "Lu Yin"], "title": "AlphaDecay:Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs", "comment": null, "summary": "Weight decay is a standard regularization technique for training large\nlanguage models (LLMs). While it is common to assign a uniform decay rate to\nevery layer, this approach overlooks the structural diversity of LLMs and the\nvarying spectral properties across modules. In this paper, we introduce\nAlphaDecay, a simple yet effective method that adaptively assigns different\nweight decay strengths to each module of an LLM. Our approach is guided by\nHeavy-Tailed Self-Regularization (HT-SR) theory, which analyzes the empirical\nspectral density (ESD) of weight correlation matrices to quantify\n\"heavy-tailedness.\" Modules exhibiting more pronounced heavy-tailed ESDs,\nreflecting stronger feature learning, are assigned weaker decay, while modules\nwith lighter-tailed spectra receive stronger decay. Our method leverages\ntailored weight decay assignments to balance the module-wise differences in\nspectral properties, leading to improved performance. Extensive pre-training\ntasks with various model sizes from 60M to 1B demonstrate that AlphaDecay\nachieves better perplexity and generalization than conventional uniform decay\nand other adaptive decay baselines."}
{"id": "2506.14435", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.14435", "abs": "https://arxiv.org/abs/2506.14435", "authors": ["Hongyu Wang", "Jiayu Xu", "Ruiping Wang", "Yan Feng", "Yitao Zhai", "Peng Pei", "Xunliang Cai", "Xilin Chen"], "title": "MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal Models", "comment": "Work in progress", "summary": "Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size\nto boost performance while maintaining fixed active parameters. However,\nprevious works primarily utilized full-precision experts during sparse\nup-cycling. Despite they show superior performance on end tasks, the large\namount of experts introduces higher memory footprint, which poses significant\nchallenges for the deployment on edge devices. In this work, we propose MoTE, a\nscalable and memory-efficient approach to train Mixture-of-Ternary-Experts\nmodels from dense checkpoint. Instead of training fewer high-precision experts,\nwe propose to train more low-precision experts during up-cycling. Specifically,\nwe use the pre-trained FFN as a shared expert and train ternary routed experts\nwith parameters in {-1, 0, 1}. Extensive experiments show that our approach has\npromising scaling trend along model size. MoTE achieves comparable performance\nto full-precision baseline MoE-LLaVA while offering lower memory footprint.\nFurthermore, our approach is compatible with post-training quantization methods\nand the advantage further amplifies when memory-constraint goes lower. Given\nthe same amount of expert memory footprint of 3.4GB and combined with\npost-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3%\naverage accuracy on end tasks, demonstrating its effectiveness and potential\nfor memory-constrained devices."}
{"id": "2506.14580", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14580", "abs": "https://arxiv.org/abs/2506.14580", "authors": ["David Wan", "Eran Hirsch", "Elias Stengel-Eskin", "Ido Dagan", "Mohit Bansal"], "title": "GenerationPrograms: Fine-grained Attribution with Executable Programs", "comment": "27 Pages. Code: https://github.com/meetdavidwan/generationprograms", "summary": "Recent large language models (LLMs) achieve impressive performance in\nsource-conditioned text generation but often fail to correctly provide\nfine-grained attributions for their outputs, undermining verifiability and\ntrust. Moreover, existing attribution methods do not explain how and why models\nleverage the provided source documents to generate their final responses,\nlimiting interpretability. To overcome these challenges, we introduce a modular\ngeneration framework, GenerationPrograms, inspired by recent advancements in\nexecutable \"code agent\" architectures. Unlike conventional generation methods\nthat simultaneously generate outputs and attributions or rely on post-hoc\nattribution, GenerationPrograms decomposes the process into two distinct\nstages: first, creating an executable program plan composed of modular text\noperations (such as paraphrasing, compression, and fusion) explicitly tailored\nto the query, and second, executing these operations following the program's\nspecified instructions to produce the final response. Empirical evaluations\ndemonstrate that GenerationPrograms significantly improves attribution quality\nat both the document level and sentence level across two long-form\nquestion-answering tasks and a multi-document summarization task. We further\ndemonstrate that GenerationPrograms can effectively function as a post-hoc\nattribution method, outperforming traditional techniques in recovering accurate\nattributions. In addition, the interpretable programs generated by\nGenerationPrograms enable localized refinement through modular-level\nimprovements that further enhance overall attribution quality."}
{"id": "2506.14440", "categories": ["cs.CV", "cs.AI", "cs.LG", "68T05, 68T07", "I.2.6; I.4.2; I.4.9"], "pdf": "https://arxiv.org/pdf/2506.14440", "abs": "https://arxiv.org/abs/2506.14440", "authors": ["David E. Hernandez", "Jose Chang", "Torbjörn E. M. Nordling"], "title": "Model compression using knowledge distillation with integrated gradients", "comment": "49 pages, 12 figures", "summary": "Model compression is critical for deploying deep learning models on\nresource-constrained devices. We introduce a novel method enhancing knowledge\ndistillation with integrated gradients (IG) as a data augmentation strategy.\nOur approach overlays IG maps onto input images during training, providing\nstudent models with deeper insights into teacher models' decision-making\nprocesses. Extensive evaluation on CIFAR-10 demonstrates that our IG-augmented\nknowledge distillation achieves 92.6% testing accuracy with a 4.1x compression\nfactor-a significant 1.1 percentage point improvement ($p<0.001$) over\nnon-distilled models (91.5%). This compression reduces inference time from 140\nms to 13 ms. Our method precomputes IG maps before training, transforming\nsubstantial runtime costs into a one-time preprocessing step. Our comprehensive\nexperiments include: (1) comparisons with attention transfer, revealing\ncomplementary benefits when combined with our approach; (2) Monte Carlo\nsimulations confirming statistical robustness; (3) systematic evaluation of\ncompression factor versus accuracy trade-offs across a wide range (2.2x-1122x);\nand (4) validation on an ImageNet subset aligned with CIFAR-10 classes,\ndemonstrating generalisability beyond the initial dataset. These extensive\nablation studies confirm that IG-based knowledge distillation consistently\noutperforms conventional approaches across varied architectures and compression\nratios. Our results establish this framework as a viable compression technique\nfor real-world deployment on edge devices while maintaining competitive\naccuracy."}
{"id": "2506.14606", "categories": ["cs.CL", "cs.AR", "cs.LG", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.14606", "abs": "https://arxiv.org/abs/2506.14606", "authors": ["Ahmed Heakl", "Sarim Hashmi", "Chaimaa Abi", "Celine Lee", "Abdulrahman Mahmoud"], "title": "Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC Transpilation with Testing Guarantees", "comment": "Project page: https://ahmedheakl.github.io/Guaranteed-Guess/", "summary": "The hardware ecosystem is rapidly evolving, with increasing interest in\ntranslating low-level programs across different instruction set architectures\n(ISAs) in a quick, flexible, and correct way to enhance the portability and\nlongevity of existing code. A particularly challenging class of this\ntranspilation problem is translating between complex- (CISC) and reduced-\n(RISC) hardware architectures, due to fundamental differences in instruction\ncomplexity, memory models, and execution paradigms. In this work, we introduce\nGG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the\ntranslation power of pre-trained large language models (LLMs) with the rigor of\nestablished software testing constructs. Our method generates candidate\ntranslations using an LLM from one ISA to another, and embeds such translations\nwithin a software-testing framework to build quantifiable confidence in the\ntranslation. We evaluate our GG approach over two diverse datasets, enforce\nhigh code coverage (>98%) across unit tests, and achieve functional/semantic\ncorrectness of 99% on HumanEval programs and 49% on BringupBench programs,\nrespectively. Further, we compare our approach to the state-of-the-art Rosetta\n2 framework on Apple Silicon, showcasing 1.73x faster runtime performance,\n1.47x better energy efficiency, and 2.41x better memory usage for our\ntranspiled code, demonstrating the effectiveness of GG for real-world\nCISC-to-RISC translation tasks. We will open-source our codes, data, models,\nand benchmarks to establish a common foundation for ISA-level code translation\nresearch."}
{"id": "2506.14451", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14451", "abs": "https://arxiv.org/abs/2506.14451", "authors": ["Aditya Shourya", "Michel Dumontier", "Chang Sun"], "title": "Adapting Lightweight Vision Language Models for Radiological Visual Question Answering", "comment": null, "summary": "Recent advancements in vision-language systems have improved the accuracy of\nRadiological Visual Question Answering (VQA) Models. However, some challenges\nremain across each stage of model development: limited expert-labeled images\nhinders data procurement at scale; the intricate and nuanced patterns of\nradiological images make modeling inherently difficult; and the lack of\nevaluation evaluation efforts makes it difficult to identify cases where the\nmodel might be ill-conditioned. In this study, we fine-tune a lightweight 3B\nparameter vision-language model for Radiological VQA, demonstrating that small\nmodels, when appropriately tuned with curated data, can achieve robust\nperformance across both open- and closed-ended questions. We propose a\ncost-effective training pipeline from synthetic question-answer pair generation\nto multi-stage fine-tuning on specialised radiological domain-targeted datasets\n(e.g., ROCO v2.0, MedPix v2.0). Our results show that despite operating at a\nfraction of the scale of state-of-the-art models such as LLaVA-Med, our model\nachieves promising performance given its small parameter size and the limited\nscale of training data. We introduce a lightweight saliency-based diagnostic\ntool that enables domain experts to inspect VQA model performance and identify\nill-conditioned failure modes through saliency analysis."}
{"id": "2506.14613", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14613", "abs": "https://arxiv.org/abs/2506.14613", "authors": ["Junghyun Min", "Xiulin Yang", "Shira Wein"], "title": "When Does Meaning Backfire? Investigating the Role of AMRs in NLI", "comment": "9 pages, 2 figures", "summary": "Natural Language Inference (NLI) relies heavily on adequately parsing the\nsemantic content of the premise and hypothesis. In this work, we investigate\nwhether adding semantic information in the form of an Abstract Meaning\nRepresentation (AMR) helps pretrained language models better generalize in NLI.\nOur experiments integrating AMR into NLI in both fine-tuning and prompting\nsettings show that the presence of AMR in fine-tuning hinders model\ngeneralization while prompting with AMR leads to slight gains in\n\\texttt{GPT-4o}. However, an ablation study reveals that the improvement comes\nfrom amplifying surface-level differences rather than aiding semantic\nreasoning. This amplification can mislead models to predict non-entailment even\nwhen the core meaning is preserved."}
{"id": "2506.14471", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14471", "abs": "https://arxiv.org/abs/2506.14471", "authors": ["Yikang Zhou", "Tao Zhang", "Dizhe Zhang", "Shunping Ji", "Xiangtai Li", "Lu Qi"], "title": "Dense360: Dense Understanding from Omnidirectional Panoramas", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) require comprehensive visual inputs\nto achieve dense understanding of the physical world. While existing MLLMs\ndemonstrate impressive world understanding capabilities through limited\nfield-of-view (FOV) visual inputs (e.g., 70 degree), we take the first step\ntoward dense understanding from omnidirectional panoramas. We first introduce\nan omnidirectional panoramas dataset featuring a comprehensive suite of\nreliability-scored annotations. Specifically, our dataset contains 160K\npanoramas with 5M dense entity-level captions, 1M unique referring expressions,\nand 100K entity-grounded panoramic scene descriptions. Compared to multi-view\nalternatives, panoramas can provide more complete, compact, and continuous\nscene representations through equirectangular projections (ERP). However, the\nuse of ERP introduces two key challenges for MLLMs: i) spatial continuity along\nthe circle of latitude, and ii) latitude-dependent variation in information\ndensity. We address these challenges through ERP-RoPE, a position encoding\nscheme specifically designed for panoramic ERP. In addition, we introduce\nDense360-Bench, the first benchmark for evaluating MLLMs on omnidirectional\ncaptioning and grounding, establishing a comprehensive framework for advancing\ndense visual-language understanding in panoramic settings."}
{"id": "2506.14625", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14625", "abs": "https://arxiv.org/abs/2506.14625", "authors": ["Chenchen Yuan", "Zheyu Zhang", "Shuo Yang", "Bardh Prenkaj", "Gjergji Kasneci"], "title": "Probabilistic Aggregation and Targeted Embedding Optimization for Collective Moral Reasoning in Large Language Models", "comment": "18 pages", "summary": "Large Language Models (LLMs) have shown impressive moral reasoning abilities.\nYet they often diverge when confronted with complex, multi-factor moral\ndilemmas. To address these discrepancies, we propose a framework that\nsynthesizes multiple LLMs' moral judgments into a collectively formulated moral\njudgment, realigning models that deviate significantly from this consensus. Our\naggregation mechanism fuses continuous moral acceptability scores (beyond\nbinary labels) into a collective probability, weighting contributions by model\nreliability. For misaligned models, a targeted embedding-optimization procedure\nfine-tunes token embeddings for moral philosophical theories, minimizing JS\ndivergence to the consensus while preserving semantic integrity. Experiments on\na large-scale social moral dilemma dataset show our approach builds robust\nconsensus and improves individual model fidelity. These findings highlight the\nvalue of data-driven moral alignment across multiple models and its potential\nfor safer, more consistent AI systems."}
{"id": "2506.14473", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.14473", "abs": "https://arxiv.org/abs/2506.14473", "authors": ["Zhijing Wan", "Zhixiang Wang", "Zheng Wang", "Xin Xu", "Shin'ichi Satoh"], "title": "Foundation Model Insights and a Multi-Model Approach for Superior Fine-Grained One-shot Subset Selection", "comment": "18 pages, 10 figures, accepted by ICML 2025", "summary": "One-shot subset selection serves as an effective tool to reduce deep learning\ntraining costs by identifying an informative data subset based on the\ninformation extracted by an information extractor (IE). Traditional IEs,\ntypically pre-trained on the target dataset, are inherently dataset-dependent.\nFoundation models (FMs) offer a promising alternative, potentially mitigating\nthis limitation. This work investigates two key questions: (1) Can FM-based\nsubset selection outperform traditional IE-based methods across diverse\ndatasets? (2) Do all FMs perform equally well as IEs for subset selection?\nExtensive experiments uncovered surprising insights: FMs consistently\noutperform traditional IEs on fine-grained datasets, whereas their advantage\ndiminishes on coarse-grained datasets with noisy labels. Motivated by these\nfinding, we propose RAM-APL (RAnking Mean-Accuracy of Pseudo-class Labels), a\nmethod tailored for fine-grained image datasets. RAM-APL leverages multiple FMs\nto enhance subset selection by exploiting their complementary strengths. Our\napproach achieves state-of-the-art performance on fine-grained datasets,\nincluding Oxford-IIIT Pet, Food-101, and Caltech-UCSD Birds-200-2011."}
{"id": "2506.14634", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.14634", "abs": "https://arxiv.org/abs/2506.14634", "authors": ["Leah von der Heyde", "Anna-Carolina Haensch", "Bernd Weiß", "Jessika Daikeler"], "title": "AIn't Nothing But a Survey? Using Large Language Models for Coding German Open-Ended Survey Responses on Survey Motivation", "comment": "to appear in Survey Research Methods", "summary": "The recent development and wider accessibility of LLMs have spurred\ndiscussions about how they can be used in survey research, including\nclassifying open-ended survey responses. Due to their linguistic capacities, it\nis possible that LLMs are an efficient alternative to time-consuming manual\ncoding and the pre-training of supervised machine learning models. As most\nexisting research on this topic has focused on English-language responses\nrelating to non-complex topics or on single LLMs, it is unclear whether its\nfindings generalize and how the quality of these classifications compares to\nestablished methods. In this study, we investigate to what extent different\nLLMs can be used to code open-ended survey responses in other contexts, using\nGerman data on reasons for survey participation as an example. We compare\nseveral state-of-the-art LLMs and several prompting approaches, and evaluate\nthe LLMs' performance by using human expert codings. Overall performance\ndiffers greatly between LLMs, and only a fine-tuned LLM achieves satisfactory\nlevels of predictive performance. Performance differences between prompting\napproaches are conditional on the LLM used. Finally, LLMs' unequal\nclassification performance across different categories of reasons for survey\nparticipation results in different categorical distributions when not using\nfine-tuning. We discuss the implications of these findings, both for\nmethodological research on coding open-ended responses and for their\nsubstantive analysis, and for practitioners processing or substantively\nanalyzing such data. Finally, we highlight the many trade-offs researchers need\nto consider when choosing automated methods for open-ended response\nclassification in the age of LLMs. In doing so, our study contributes to the\ngrowing body of research about the conditions under which LLMs can be\nefficiently, accurately, and reliably leveraged in survey research."}
{"id": "2506.14495", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14495", "abs": "https://arxiv.org/abs/2506.14495", "authors": ["Yu Qi", "Lipeng Gu", "Honghua Chen", "Liangliang Nan", "Mingqiang Wei"], "title": "I Speak and You Find: Robust 3D Visual Grounding with Noisy and Ambiguous Speech Inputs", "comment": null, "summary": "Existing 3D visual grounding methods rely on precise text prompts to locate\nobjects within 3D scenes. Speech, as a natural and intuitive modality, offers a\npromising alternative. Real-world speech inputs, however, often suffer from\ntranscription errors due to accents, background noise, and varying speech\nrates, limiting the applicability of existing 3DVG methods. To address these\nchallenges, we propose \\textbf{SpeechRefer}, a novel 3DVG framework designed to\nenhance performance in the presence of noisy and ambiguous speech-to-text\ntranscriptions. SpeechRefer integrates seamlessly with xisting 3DVG models and\nintroduces two key innovations. First, the Speech Complementary Module captures\nacoustic similarities between phonetically related words and highlights subtle\ndistinctions, generating complementary proposal scores from the speech signal.\nThis reduces dependence on potentially erroneous transcriptions. Second, the\nContrastive Complementary Module employs contrastive learning to align\nerroneous text features with corresponding speech features, ensuring robust\nperformance even when transcription errors dominate. Extensive experiments on\nthe SpeechRefer and peechNr3D datasets demonstrate that SpeechRefer improves\nthe performance of existing 3DVG methods by a large margin, which highlights\nSpeechRefer's potential to bridge the gap between noisy speech inputs and\nreliable 3DVG, enabling more intuitive and practical multimodal systems."}
{"id": "2506.14641", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.14641", "abs": "https://arxiv.org/abs/2506.14641", "authors": ["Xiang Cheng", "Chengyan Pan", "Minjun Zhao", "Deyang Li", "Fangchao Liu", "Xinyu Zhang", "Xiao Zhang", "Yong Liu"], "title": "Revisiting Chain-of-Thought Prompting: Zero-shot Can Be Stronger than Few-shot", "comment": "19 pages,22 figures", "summary": "In-Context Learning (ICL) is an essential emergent ability of Large Language\nModels (LLMs), and recent studies introduce Chain-of-Thought (CoT) to exemplars\nof ICL to enhance the reasoning capability, especially in mathematics tasks.\nHowever, given the continuous advancement of model capabilities, it remains\nunclear whether CoT exemplars still benefit recent, stronger models in such\ntasks. Through systematic experiments, we find that for recent strong models\nsuch as the Qwen2.5 series, adding traditional CoT exemplars does not improve\nreasoning performance compared to Zero-Shot CoT. Instead, their primary\nfunction is to align the output format with human expectations. We further\ninvestigate the effectiveness of enhanced CoT exemplars, constructed using\nanswers from advanced models such as \\texttt{Qwen2.5-Max} and\n\\texttt{DeepSeek-R1}. Experimental results indicate that these enhanced\nexemplars still fail to improve the model's reasoning performance. Further\nanalysis reveals that models tend to ignore the exemplars and focus primarily\non the instructions, leading to no observable gain in reasoning ability.\nOverall, our findings highlight the limitations of the current ICL+CoT\nframework in mathematical reasoning, calling for a re-examination of the ICL\nparadigm and the definition of exemplars."}
{"id": "2506.14511", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14511", "abs": "https://arxiv.org/abs/2506.14511", "authors": ["Zhiwen Shao", "Yifan Cheng", "Feiran Li", "Yong Zhou", "Xuequan Lu", "Yuan Xie", "Lizhuang Ma"], "title": "MOL: Joint Estimation of Micro-Expression, Optical Flow, and Landmark via Transformer-Graph-Style Convolution", "comment": "This paper has been accepted by IEEE Transactions on Pattern Analysis\n  and Machine Intelligence", "summary": "Facial micro-expression recognition (MER) is a challenging problem, due to\ntransient and subtle micro-expression (ME) actions. Most existing methods\ndepend on hand-crafted features, key frames like onset, apex, and offset\nframes, or deep networks limited by small-scale and low-diversity datasets. In\nthis paper, we propose an end-to-end micro-action-aware deep learning framework\nwith advantages from transformer, graph convolution, and vanilla convolution.\nIn particular, we propose a novel F5C block composed of fully-connected\nconvolution and channel correspondence convolution to directly extract\nlocal-global features from a sequence of raw frames, without the prior\nknowledge of key frames. The transformer-style fully-connected convolution is\nproposed to extract local features while maintaining global receptive fields,\nand the graph-style channel correspondence convolution is introduced to model\nthe correlations among feature patterns. Moreover, MER, optical flow\nestimation, and facial landmark detection are jointly trained by sharing the\nlocal-global features. The two latter tasks contribute to capturing facial\nsubtle action information for MER, which can alleviate the impact of\ninsufficient training data. Extensive experiments demonstrate that our\nframework (i) outperforms the state-of-the-art MER methods on CASME II, SAMM,\nand SMIC benchmarks, (ii) works well for optical flow estimation and facial\nlandmark detection, and (iii) can capture facial subtle muscle actions in local\nregions associated with MEs. The code is available at\nhttps://github.com/CYF-cuber/MOL."}
{"id": "2506.14645", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.14645", "abs": "https://arxiv.org/abs/2506.14645", "authors": [". Pazzaglia", "V. Vendetti", "L. D. Comencini", "F. Deriu", "V. Modugno"], "title": "Passing the Turing Test in Political Discourse: Fine-Tuning LLMs to Mimic Polarized Social Media Comments", "comment": null, "summary": "The increasing sophistication of large language models (LLMs) has sparked\ngrowing concerns regarding their potential role in exacerbating ideological\npolarization through the automated generation of persuasive and biased content.\nThis study explores the extent to which fine-tuned LLMs can replicate and\namplify polarizing discourse within online environments. Using a curated\ndataset of politically charged discussions extracted from Reddit, we fine-tune\nan open-source LLM to produce context-aware and ideologically aligned\nresponses. The model's outputs are evaluated through linguistic analysis,\nsentiment scoring, and human annotation, with particular attention to\ncredibility and rhetorical alignment with the original discourse. The results\nindicate that, when trained on partisan data, LLMs are capable of producing\nhighly plausible and provocative comments, often indistinguishable from those\nwritten by humans. These findings raise significant ethical questions about the\nuse of AI in political discourse, disinformation, and manipulation campaigns.\nThe paper concludes with a discussion of the broader implications for AI\ngovernance, platform regulation, and the development of detection tools to\nmitigate adversarial fine-tuning risks."}
{"id": "2506.14512", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14512", "abs": "https://arxiv.org/abs/2506.14512", "authors": ["Zijian Song", "Xiaoxin Lin", "Qiuming Huang", "Guangrun Wang", "Liang Lin"], "title": "SIRI-Bench: Challenging VLMs' Spatial Intelligence through Complex Reasoning Tasks", "comment": "16 pages, 9 figures", "summary": "Large Language Models (LLMs) are experiencing rapid advancements in complex\nreasoning, exhibiting remarkable generalization in mathematics and programming.\nIn contrast, while spatial intelligence is fundamental for Vision-Language\nModels (VLMs) in real-world interaction, the systematic evaluation of their\ncomplex reasoning ability within spatial contexts remains underexplored. To\nbridge this gap, we introduce SIRI-Bench, a benchmark designed to evaluate\nVLMs' spatial intelligence through video-based reasoning tasks. SIRI-Bench\ncomprises nearly 1K video-question-answer triplets, where each problem is\nembedded in a realistic 3D scene and captured by video. By carefully designing\nquestions and corresponding 3D scenes, our benchmark ensures that solving the\nquestions requires both spatial comprehension for extracting information and\nhigh-level reasoning for deriving solutions, making it a challenging benchmark\nfor evaluating VLMs. To facilitate large-scale data synthesis, we develop an\nAutomatic Scene Creation Engine. This engine, leveraging multiple specialized\nLLM agents, can generate realistic 3D scenes from abstract math problems,\nensuring faithfulness to the original descriptions. Experimental results reveal\nthat state-of-the-art VLMs struggle significantly on SIRI-Bench, underscoring\nthe challenge of spatial reasoning. We hope that our study will bring\nresearchers' attention to spatially grounded reasoning and advance VLMs in\nvisual problem-solving."}
{"id": "2506.14646", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14646", "abs": "https://arxiv.org/abs/2506.14646", "authors": ["Hengyuan Zhang", "Xinrong Chen", "Yingmin Qiu", "Xiao Liang", "Ziyue Li", "Guanyu Wang", "Weiping Li", "Tong Mo", "Wenyue Li", "Hayden Kwok-Hay So", "Ngai Wong"], "title": "GuiLoMo: Allocating Expert Number and Rank for LoRA-MoE via Bilevel Optimization with GuidedSelection Vectors", "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) methods, particularly Low-Rank\nAdaptation (LoRA), offer an efficient way to adapt large language models with\nreduced computational costs. However, their performance is limited by the small\nnumber of trainable parameters. Recent work combines LoRA with the\nMixture-of-Experts (MoE), i.e., LoRA-MoE, to enhance capacity, but two\nlimitations remain in hindering the full exploitation of its potential: 1) the\ninfluence of downstream tasks when assigning expert numbers, and 2) the uniform\nrank assignment across all LoRA experts, which restricts representational\ndiversity. To mitigate these gaps, we propose GuiLoMo, a fine-grained\nlayer-wise expert numbers and ranks allocation strategy with GuidedSelection\nVectors (GSVs). GSVs are learned via a prior bilevel optimization process to\ncapture both model- and task-specific needs, and are then used to allocate\noptimal expert numbers and ranks. Experiments on three backbone models across\ndiverse benchmarks show that GuiLoMo consistently achieves superior or\ncomparable performance to all baselines. Further analysis offers key insights\ninto how expert numbers and ranks vary across layers and tasks, highlighting\nthe benefits of adaptive expert configuration. Our code is available at\nhttps://github.com/Liar406/Gui-LoMo.git."}
{"id": "2506.14525", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.14525", "abs": "https://arxiv.org/abs/2506.14525", "authors": ["Zhuoyue Tan", "Boyong He", "Yuxiang Ji", "Liaoni Wu"], "title": "VisLanding: Monocular 3D Perception for UAV Safe Landing via Depth-Normal Synergy", "comment": "Accepted by IROS2025", "summary": "This paper presents VisLanding, a monocular 3D perception-based framework for\nsafe UAV (Unmanned Aerial Vehicle) landing. Addressing the core challenge of\nautonomous UAV landing in complex and unknown environments, this study\ninnovatively leverages the depth-normal synergy prediction capabilities of the\nMetric3D V2 model to construct an end-to-end safe landing zones (SLZ)\nestimation framework. By introducing a safe zone segmentation branch, we\ntransform the landing zone estimation task into a binary semantic segmentation\nproblem. The model is fine-tuned and annotated using the WildUAV dataset from a\nUAV perspective, while a cross-domain evaluation dataset is constructed to\nvalidate the model's robustness. Experimental results demonstrate that\nVisLanding significantly enhances the accuracy of safe zone identification\nthrough a depth-normal joint optimization mechanism, while retaining the\nzero-shot generalization advantages of Metric3D V2. The proposed method\nexhibits superior generalization and robustness in cross-domain testing\ncompared to other approaches. Furthermore, it enables the estimation of landing\nzone area by integrating predicted depth and normal information, providing\ncritical decision-making support for practical applications."}
{"id": "2506.14681", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14681", "abs": "https://arxiv.org/abs/2506.14681", "authors": ["Yuto Harada", "Yusuke Yamauchi", "Yusuke Oda", "Yohei Oseki", "Yusuke Miyao", "Yu Takagi"], "title": "Massive Supervised Fine-tuning Experiments Reveal How Data, Layer, and Training Factors Shape LLM Alignment Quality", "comment": null, "summary": "Supervised fine-tuning (SFT) is a critical step in aligning large language\nmodels (LLMs) with human instructions and values, yet many aspects of SFT\nremain poorly understood. We trained a wide range of base models on a variety\nof datasets including code generation, mathematical reasoning, and\ngeneral-domain tasks, resulting in 1,000+ SFT models under controlled\nconditions. We then identified the dataset properties that matter most and\nexamined the layer-wise modifications introduced by SFT. Our findings reveal\nthat some training-task synergies persist across all models while others vary\nsubstantially, emphasizing the importance of model-specific strategies.\nMoreover, we demonstrate that perplexity consistently predicts SFT\neffectiveness--often surpassing superficial similarity between trained data and\nbenchmark--and that mid-layer weight changes correlate most strongly with\nperformance gains. We will release these 1,000+ SFT models and benchmark\nresults to accelerate further research."}
{"id": "2506.14541", "categories": ["cs.CV", "I.4.9"], "pdf": "https://arxiv.org/pdf/2506.14541", "abs": "https://arxiv.org/abs/2506.14541", "authors": ["Rongchang Lu", "Tianduo Luo", "Yunzhi Zhang", "Conghan Yue", "Pei Yang", "Guibao Liu", "Changyang Gu"], "title": "Exploring Diffusion with Test-Time Training on Efficient Image Restoration", "comment": "Submitted to The 8th Chinese Conference on Pattern Recognition and\n  Computer Vision (2025). Contact to nomodeset@qq.com. Source code will open in\n  4 months", "summary": "Image restoration faces challenges including ineffective feature fusion,\ncomputational bottlenecks and inefficient diffusion processes. To address\nthese, we propose DiffRWKVIR, a novel framework unifying Test-Time Training\n(TTT) with efficient diffusion. Our approach introduces three key innovations:\n(1) Omni-Scale 2D State Evolution extends RWKV's location-dependent\nparameterization to hierarchical multi-directional 2D scanning, enabling global\ncontextual awareness with linear complexity O(L); (2) Chunk-Optimized Flash\nProcessing accelerates intra-chunk parallelism by 3.2x via contiguous chunk\nprocessing (O(LCd) complexity), reducing sequential dependencies and\ncomputational overhead; (3) Prior-Guided Efficient Diffusion extracts a compact\nImage Prior Representation (IPR) in only 5-20 steps, proving 45% faster\ntraining/inference than DiffIR while solving computational inefficiency in\ndenoising. Evaluated across super-resolution and inpainting benchmarks (Set5,\nSet14, BSD100, Urban100, Places365), DiffRWKVIR outperforms SwinIR, HAT, and\nMambaIR/v2 in PSNR, SSIM, LPIPS, and efficiency metrics. Our method establishes\na new paradigm for adaptive, high-efficiency image restoration with optimized\nhardware utilization."}
{"id": "2506.14702", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.14702", "abs": "https://arxiv.org/abs/2506.14702", "authors": ["Daniel D'souza", "Julia Kreutzer", "Adrien Morisot", "Ahmet Üstün", "Sara Hooker"], "title": "Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time Markers", "comment": null, "summary": "One of the most profound challenges of modern machine learning is performing\nwell on the long-tail of rare and underrepresented features. Large\ngeneral-purpose models are trained for many tasks, but work best on\nhigh-frequency use cases. After training, it is hard to adapt a model to\nperform well on specific use cases underrepresented in the training corpus.\nRelying on prompt engineering or few-shot examples to maximize the output\nquality on a particular test case can be frustrating, as models can be highly\nsensitive to small changes, react in unpredicted ways or rely on a fixed system\nprompt for maintaining performance. In this work, we ask: \"Can we optimize our\ntraining protocols to both improve controllability and performance on\nunderrepresented use cases at inference time?\" We revisit the divide between\ntraining and inference techniques to improve long-tail performance while\nproviding users with a set of control levers the model is trained to be\nresponsive to. We create a detailed taxonomy of data characteristics and task\nprovenance to explicitly control generation attributes and implicitly condition\ngenerations at inference time. We fine-tune a base model to infer these markers\nautomatically, which makes them optional at inference time. This principled and\nflexible approach yields pronounced improvements in performance, especially on\nexamples from the long tail of the training distribution. While we observe an\naverage lift of 5.7% win rates in open-ended generation quality with our\nmarkers, we see over 9.1% gains in underrepresented domains. We also observe\nrelative lifts of up to 14.1% on underrepresented tasks like CodeRepair and\nabsolute improvements of 35.3% on length instruction following evaluations."}
{"id": "2506.14549", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14549", "abs": "https://arxiv.org/abs/2506.14549", "authors": ["Yong Liu", "Wenpeng Xiao", "Qianqian Wang", "Junlin Chen", "Shiyin Wang", "Yitong Wang", "Xinglong Wu", "Yansong Tang"], "title": "DreamLight: Towards Harmonious and Consistent Image Relighting", "comment": null, "summary": "We introduce a model named DreamLight for universal image relighting in this\nwork, which can seamlessly composite subjects into a new background while\nmaintaining aesthetic uniformity in terms of lighting and color tone. The\nbackground can be specified by natural images (image-based relighting) or\ngenerated from unlimited text prompts (text-based relighting). Existing studies\nprimarily focus on image-based relighting, while with scant exploration into\ntext-based scenarios. Some works employ intricate disentanglement pipeline\ndesigns relying on environment maps to provide relevant information, which\ngrapples with the expensive data cost required for intrinsic decomposition and\nlight source. Other methods take this task as an image translation problem and\nperform pixel-level transformation with autoencoder architecture. While these\nmethods have achieved decent harmonization effects, they struggle to generate\nrealistic and natural light interaction effects between the foreground and\nbackground. To alleviate these challenges, we reorganize the input data into a\nunified format and leverage the semantic prior provided by the pretrained\ndiffusion model to facilitate the generation of natural results. Moreover, we\npropose a Position-Guided Light Adapter (PGLA) that condenses light information\nfrom different directions in the background into designed light query\nembeddings, and modulates the foreground with direction-biased masked\nattention. In addition, we present a post-processing module named Spectral\nForeground Fixer (SFF) to adaptively reorganize different frequency components\nof subject and relighted background, which helps enhance the consistency of\nforeground appearance. Extensive comparisons and user study demonstrate that\nour DreamLight achieves remarkable relighting performance."}
{"id": "2506.14704", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14704", "abs": "https://arxiv.org/abs/2506.14704", "authors": ["Anton Changalidis", "Aki Härmä"], "title": "Capacity Matters: a Proof-of-Concept for Transformer Memorization on Real-World Data", "comment": "This work has been accepted for publication at the First Workshop on\n  Large Language Model Memorization (L2M2) at ACL 2025, Vienna, Austria", "summary": "This paper studies how the model architecture and data configurations\ninfluence the empirical memorization capacity of generative transformers. The\nmodels are trained using synthetic text datasets derived from the Systematized\nNomenclature of Medicine (SNOMED) knowledge graph: triplets, representing\nstatic connections, and sequences, simulating complex relation patterns. The\nresults show that embedding size is the primary determinant of learning speed\nand capacity, while additional layers provide limited benefits and may hinder\nperformance on simpler datasets. Activation functions play a crucial role, and\nSoftmax demonstrates greater stability and capacity. Furthermore, increasing\nthe complexity of the data set seems to improve the final memorization. These\ninsights improve our understanding of transformer memory mechanisms and provide\na framework for optimizing model design with structured real-world data."}
{"id": "2506.14560", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.14560", "abs": "https://arxiv.org/abs/2506.14560", "authors": ["David Butler", "Adrian Hilton", "Gustavo Carneiro"], "title": "Risk Estimation of Knee Osteoarthritis Progression via Predictive Multi-task Modelling from Efficient Diffusion Model using X-ray Images", "comment": null, "summary": "Medical imaging plays a crucial role in assessing knee osteoarthritis (OA)\nrisk by enabling early detection and disease monitoring. Recent machine\nlearning methods have improved risk estimation (i.e., predicting the likelihood\nof disease progression) and predictive modelling (i.e., the forecasting of\nfuture outcomes based on current data) using medical images, but clinical\nadoption remains limited due to their lack of interpretability. Existing\napproaches that generate future images for risk estimation are complex and\nimpractical. Additionally, previous methods fail to localize anatomical knee\nlandmarks, limiting interpretability. We address these gaps with a new\ninterpretable machine learning method to estimate the risk of knee OA\nprogression via multi-task predictive modelling that classifies future knee OA\nseverity and predicts anatomical knee landmarks from efficiently generated\nhigh-quality future images. Such image generation is achieved by leveraging a\ndiffusion model in a class-conditioned latent space to forecast disease\nprogression, offering a visual representation of how particular health\nconditions may evolve. Applied to the Osteoarthritis Initiative dataset, our\napproach improves the state-of-the-art (SOTA) by 2\\%, achieving an AUC of 0.71\nin predicting knee OA progression while offering ~9% faster inference time."}
{"id": "2506.14731", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14731", "abs": "https://arxiv.org/abs/2506.14731", "authors": ["Ring Team", "Bin Hu", "Cai Chen", "Deng Zhao", "Ding Liu", "Dingnan Jin", "Feng Zhu", "Hao Dai", "Hongzhi Luan", "Jia Guo", "Jiaming Liu", "Jiewei Wu", "Jun Mei", "Jun Zhou", "Junbo Zhao", "Junwu Xiong", "Kaihong Zhang", "Kuan Xu", "Lei Liang", "Liang Jiang", "Liangcheng Fu", "Longfei Zheng", "Qiang Gao", "Qing Cui", "Quan Wan", "Shaomian Zheng", "Shuaicheng Li", "Tongkai Yang", "Wang Ren", "Xiaodong Yan", "Xiaopei Wan", "Xiaoyun Feng", "Xin Zhao", "Xinxing Yang", "Xinyu Kong", "Xuemin Yang", "Yang Li", "Yingting Wu", "Yongkang Liu", "Zhankai Xu", "Zhenduo Zhang", "Zhenglei Zhou", "Zhenyu Huang", "Zhiqiang Zhang", "Zihao Wang", "Zujie Wen"], "title": "Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning for LLMs", "comment": "Technical Report", "summary": "We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model\noptimized via reinforcement learning (RL) to achieve efficient and robust\nreasoning capabilities. Built upon the publicly available Ling-lite model, a\n16.8 billion parameter model with 2.75 billion activated parameters, our\napproach matches the performance of state-of-the-art (SOTA) small-scale\nreasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench,\nGPQA-Diamond) while activating only one-third of the parameters required by\ncomparable models. To accomplish this, we introduce a joint training pipeline\nintegrating distillation with RL, revealing undocumented challenges in MoE RL\ntraining. First, we identify optimization instability during RL training, and\nwe propose Constrained Contextual Computation Policy Optimization(C3PO), a\nnovel approach that enhances training stability and improves computational\nthroughput via algorithm-system co-design methodology. Second, we empirically\ndemonstrate that selecting distillation checkpoints based on entropy loss for\nRL training, rather than validation metrics, yields superior\nperformance-efficiency trade-offs in subsequent RL training. Finally, we\ndevelop a two-stage training paradigm to harmonize multi-domain data\nintegration, addressing domain conflicts that arise in training with mixed\ndataset. We will release the model, dataset, and code."}
{"id": "2506.14583", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14583", "abs": "https://arxiv.org/abs/2506.14583", "authors": ["Krishna Sahukara", "Zineddine Bettouche", "Andreas Fischer"], "title": "Synthetic Data Augmentation for Table Detection: Re-evaluating TableNet's Performance with Automatically Generated Document Images", "comment": null, "summary": "Document pages captured by smartphones or scanners often contain tables, yet\nmanual extraction is slow and error-prone. We introduce an automated\nLaTeX-based pipeline that synthesizes realistic two-column pages with visually\ndiverse table layouts and aligned ground-truth masks. The generated corpus\naugments the real-world Marmot benchmark and enables a systematic resolution\nstudy of TableNet. Training TableNet on our synthetic data achieves a\npixel-wise XOR error of 4.04% on our synthetic test set with a 256x256 input\nresolution, and 4.33% with 1024x1024. The best performance on the Marmot\nbenchmark is 9.18% (at 256x256), while cutting manual annotation effort through\nautomation."}
{"id": "2506.14758", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14758", "abs": "https://arxiv.org/abs/2506.14758", "authors": ["Daixuan Cheng", "Shaohan Huang", "Xuekai Zhu", "Bo Dai", "Wayne Xin Zhao", "Zhenliang Zhang", "Furu Wei"], "title": "Reasoning with Exploration: An Entropy Perspective", "comment": null, "summary": "Balancing exploration and exploitation is a central goal in reinforcement\nlearning (RL). Despite recent advances in enhancing language model (LM)\nreasoning, most methods lean toward exploitation, and increasingly encounter\nperformance plateaus. In this work, we revisit entropy -- a signal of\nexploration in RL -- and examine its relationship to exploratory reasoning in\nLMs. Through empirical analysis, we uncover strong positive correlations\nbetween high-entropy regions and three types of exploratory reasoning actions:\n(1) pivotal tokens that determine or connect logical steps, (2) reflective\nactions such as self-verification and correction, and (3) rare behaviors\nunder-explored by the base LMs. Motivated by this, we introduce a minimal\nmodification to standard RL with only one line of code: augmenting the\nadvantage function with an entropy-based term. Unlike traditional\nmaximum-entropy methods which encourage exploration by promoting uncertainty,\nwe encourage exploration by promoting longer and deeper reasoning chains.\nNotably, our method achieves significant gains on the Pass@K metric -- an\nupper-bound estimator of LM reasoning capabilities -- even when evaluated with\nextremely large K values, pushing the boundaries of LM reasoning."}
{"id": "2506.14596", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14596", "abs": "https://arxiv.org/abs/2506.14596", "authors": ["Ming Xu", "Xu Zhang"], "title": "PoseGRAF: Geometric-Reinforced Adaptive Fusion for Monocular 3D Human Pose Estimation", "comment": null, "summary": "Existing monocular 3D pose estimation methods primarily rely on joint\npositional features, while overlooking intrinsic directional and angular\ncorrelations within the skeleton. As a result, they often produce implausible\nposes under joint occlusions or rapid motion changes. To address these\nchallenges, we propose the PoseGRAF framework. We first construct a dual graph\nconvolutional structure that separately processes joint and bone graphs,\neffectively capturing their local dependencies. A Cross-Attention module is\nthen introduced to model interdependencies between bone directions and joint\nfeatures. Building upon this, a dynamic fusion module is designed to adaptively\nintegrate both feature types by leveraging the relational dependencies between\njoints and bones. An improved Transformer encoder is further incorporated in a\nresidual manner to generate the final output. Experimental results on the\nHuman3.6M and MPI-INF-3DHP datasets show that our method exceeds\nstate-of-the-art approaches. Additional evaluations on in-the-wild videos\nfurther validate its generalizability. The code is publicly available at\nhttps://github.com/iCityLab/PoseGRAF."}
{"id": "2506.14761", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14761", "abs": "https://arxiv.org/abs/2506.14761", "authors": ["Mathurin Videau", "Badr Youbi Idrissi", "Alessandro Leite", "Marc Schoenauer", "Olivier Teytaud", "David Lopez-Paz"], "title": "From Bytes to Ideas: Language Modeling with Autoregressive U-Nets", "comment": null, "summary": "Tokenization imposes a fixed granularity on the input text, freezing how a\nlanguage model operates on data and how far in the future it predicts. Byte\nPair Encoding (BPE) and similar schemes split text once, build a static\nvocabulary, and leave the model stuck with that choice. We relax this rigidity\nby introducing an autoregressive U-Net that learns to embed its own tokens as\nit trains. The network reads raw bytes, pools them into words, then pairs of\nwords, then up to 4 words, giving it a multi-scale view of the sequence. At\ndeeper stages, the model must predict further into the future -- anticipating\nthe next few words rather than the next byte -- so deeper stages focus on\nbroader semantic patterns while earlier stages handle fine details. When\ncarefully tuning and controlling pretraining compute, shallow hierarchies tie\nstrong BPE baselines, and deeper hierarchies have a promising trend. Because\ntokenization now lives inside the model, the same system can handle\ncharacter-level tasks and carry knowledge across low-resource languages."}
{"id": "2506.14603", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.14603", "abs": "https://arxiv.org/abs/2506.14603", "authors": ["Amirmojtaba Sabour", "Sanja Fidler", "Karsten Kreis"], "title": "Align Your Flow: Scaling Continuous-Time Flow Map Distillation", "comment": "Project page:\n  https://research.nvidia.com/labs/toronto-ai/AlignYourFlow/", "summary": "Diffusion- and flow-based models have emerged as state-of-the-art generative\nmodeling approaches, but they require many sampling steps. Consistency models\ncan distill these models into efficient one-step generators; however, unlike\nflow- and diffusion-based methods, their performance inevitably degrades when\nincreasing the number of steps, which we show both analytically and\nempirically. Flow maps generalize these approaches by connecting any two noise\nlevels in a single step and remain effective across all step counts. In this\npaper, we introduce two new continuous-time objectives for training flow maps,\nalong with additional novel training techniques, generalizing existing\nconsistency and flow matching objectives. We further demonstrate that\nautoguidance can improve performance, using a low-quality model for guidance\nduring distillation, and an additional boost can be achieved by adversarial\nfinetuning, with minimal loss in sample diversity. We extensively validate our\nflow map models, called Align Your Flow, on challenging image generation\nbenchmarks and achieve state-of-the-art few-step generation performance on both\nImageNet 64x64 and 512x512, using small and efficient neural networks. Finally,\nwe show text-to-image flow map models that outperform all existing\nnon-adversarially trained few-step samplers in text-conditioned synthesis."}
{"id": "2506.14767", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.14767", "abs": "https://arxiv.org/abs/2506.14767", "authors": ["Li-Wei Chen", "Takuya Higuchi", "Zakaria Aldeneh", "Ahmed Hussen Abdelaziz", "Alexander Rudnicky"], "title": "A Variational Framework for Improving Naturalness in Generative Spoken Language Models", "comment": "International Conference on Machine Learning (ICML) 2025", "summary": "The success of large language models in text processing has inspired their\nadaptation to speech modeling. However, since speech is continuous and complex,\nit is often discretized for autoregressive modeling. Speech tokens derived from\nself-supervised models (known as semantic tokens) typically focus on the\nlinguistic aspects of speech but neglect prosodic information. As a result,\nmodels trained on these tokens can generate speech with reduced naturalness.\nExisting approaches try to fix this by adding pitch features to the semantic\ntokens. However, pitch alone cannot fully represent the range of paralinguistic\nattributes, and selecting the right features requires careful hand-engineering.\nTo overcome this, we propose an end-to-end variational approach that\nautomatically learns to encode these continuous speech attributes to enhance\nthe semantic tokens. Our approach eliminates the need for manual extraction and\nselection of paralinguistic features. Moreover, it produces preferred speech\ncontinuations according to human raters. Code, samples and models are available\nat https://github.com/b04901014/vae-gslm."}
{"id": "2506.14605", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.14605", "abs": "https://arxiv.org/abs/2506.14605", "authors": ["Giacomo Meanti", "Thomas Ryckeboer", "Michael Arbel", "Julien Mairal"], "title": "Unsupervised Imaging Inverse Problems with Diffusion Distribution Matching", "comment": "Code available at https://github.com/inria-thoth/ddm4ip", "summary": "This work addresses image restoration tasks through the lens of inverse\nproblems using unpaired datasets. In contrast to traditional approaches --\nwhich typically assume full knowledge of the forward model or access to paired\ndegraded and ground-truth images -- the proposed method operates under minimal\nassumptions and relies only on small, unpaired datasets. This makes it\nparticularly well-suited for real-world scenarios, where the forward model is\noften unknown or misspecified, and collecting paired data is costly or\ninfeasible. The method leverages conditional flow matching to model the\ndistribution of degraded observations, while simultaneously learning the\nforward model via a distribution-matching loss that arises naturally from the\nframework. Empirically, it outperforms both single-image blind and unsupervised\napproaches on deblurring and non-uniform point spread function (PSF)\ncalibration tasks. It also matches state-of-the-art performance on blind\nsuper-resolution. We also showcase the effectiveness of our method with a proof\nof concept for lens calibration: a real-world application traditionally\nrequiring time-consuming experiments and specialized equipment. In contrast,\nour approach achieves this with minimal data acquisition effort."}
{"id": "2506.14142", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14142", "abs": "https://arxiv.org/abs/2506.14142", "authors": ["Wenting Chen", "Yi Dong", "Zhaojun Ding", "Yucheng Shi", "Yifan Zhou", "Fang Zeng", "Yijun Luo", "Tianyu Lin", "Yihang Su", "Yichen Wu", "Kai Zhang", "Zhen Xiang", "Tianming Liu", "Ninghao Liu", "Lichao Sun", "Yixuan Yuan", "Xiang Li"], "title": "RadFabric: Agentic AI System with Reasoning Capability for Radiology", "comment": "4 figures, 2 tables", "summary": "Chest X ray (CXR) imaging remains a critical diagnostic tool for thoracic\nconditions, but current automated systems face limitations in pathology\ncoverage, diagnostic accuracy, and integration of visual and textual reasoning.\nTo address these gaps, we propose RadFabric, a multi agent, multimodal\nreasoning framework that unifies visual and textual analysis for comprehensive\nCXR interpretation. RadFabric is built on the Model Context Protocol (MCP),\nenabling modularity, interoperability, and scalability for seamless integration\nof new diagnostic agents. The system employs specialized CXR agents for\npathology detection, an Anatomical Interpretation Agent to map visual findings\nto precise anatomical structures, and a Reasoning Agent powered by large\nmultimodal reasoning models to synthesize visual, anatomical, and clinical data\ninto transparent and evidence based diagnoses. RadFabric achieves significant\nperformance improvements, with near-perfect detection of challenging\npathologies like fractures (1.000 accuracy) and superior overall diagnostic\naccuracy (0.799) compared to traditional systems (0.229 to 0.527). By\nintegrating cross modal feature alignment and preference-driven reasoning,\nRadFabric advances AI-driven radiology toward transparent, anatomically\nprecise, and clinically actionable CXR analysis."}
{"id": "2506.14629", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14629", "abs": "https://arxiv.org/abs/2506.14629", "authors": ["Md. Adnanul Islam", "Md. Faiyaz Abdullah Sayeedi", "Md. Asaduzzaman Shuvo", "Muhammad Ziaur Rahman", "Shahanur Rahman Bappy", "Raiyan Rahman", "Swakkhar Shatabda"], "title": "VisText-Mosquito: A Multimodal Dataset and Benchmark for AI-Based Mosquito Breeding Site Detection and Reasoning", "comment": null, "summary": "Mosquito-borne diseases pose a major global health risk, requiring early\ndetection and proactive control of breeding sites to prevent outbreaks. In this\npaper, we present VisText-Mosquito, a multimodal dataset that integrates visual\nand textual data to support automated detection, segmentation, and reasoning\nfor mosquito breeding site analysis. The dataset includes 1,828 annotated\nimages for object detection, 142 images for water surface segmentation, and\nnatural language reasoning texts linked to each image. The YOLOv9s model\nachieves the highest precision of 0.92926 and mAP@50 of 0.92891 for object\ndetection, while YOLOv11n-Seg reaches a segmentation precision of 0.91587 and\nmAP@50 of 0.79795. For reasoning generation, our fine-tuned BLIP model achieves\na final loss of 0.0028, with a BLEU score of 54.7, BERTScore of 0.91, and\nROUGE-L of 0.87. This dataset and model framework emphasize the theme\n\"Prevention is Better than Cure\", showcasing how AI-based detection can\nproactively address mosquito-borne disease risks. The dataset and\nimplementation code are publicly available at GitHub:\nhttps://github.com/adnanul-islam-jisun/VisText-Mosquito"}
{"id": "2506.14629", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14629", "abs": "https://arxiv.org/abs/2506.14629", "authors": ["Md. Adnanul Islam", "Md. Faiyaz Abdullah Sayeedi", "Md. Asaduzzaman Shuvo", "Muhammad Ziaur Rahman", "Shahanur Rahman Bappy", "Raiyan Rahman", "Swakkhar Shatabda"], "title": "VisText-Mosquito: A Multimodal Dataset and Benchmark for AI-Based Mosquito Breeding Site Detection and Reasoning", "comment": null, "summary": "Mosquito-borne diseases pose a major global health risk, requiring early\ndetection and proactive control of breeding sites to prevent outbreaks. In this\npaper, we present VisText-Mosquito, a multimodal dataset that integrates visual\nand textual data to support automated detection, segmentation, and reasoning\nfor mosquito breeding site analysis. The dataset includes 1,828 annotated\nimages for object detection, 142 images for water surface segmentation, and\nnatural language reasoning texts linked to each image. The YOLOv9s model\nachieves the highest precision of 0.92926 and mAP@50 of 0.92891 for object\ndetection, while YOLOv11n-Seg reaches a segmentation precision of 0.91587 and\nmAP@50 of 0.79795. For reasoning generation, our fine-tuned BLIP model achieves\na final loss of 0.0028, with a BLEU score of 54.7, BERTScore of 0.91, and\nROUGE-L of 0.87. This dataset and model framework emphasize the theme\n\"Prevention is Better than Cure\", showcasing how AI-based detection can\nproactively address mosquito-borne disease risks. The dataset and\nimplementation code are publicly available at GitHub:\nhttps://github.com/adnanul-islam-jisun/VisText-Mosquito"}
{"id": "2506.14642", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14642", "abs": "https://arxiv.org/abs/2506.14642", "authors": ["Yuke Xing", "Jiarui Wang", "Peizhi Niu", "Wenjie Huang", "Guangtao Zhai", "Yiling Xu"], "title": "3DGS-IEval-15K: A Large-scale Image Quality Evaluation Database for 3D Gaussian-Splatting", "comment": null, "summary": "3D Gaussian Splatting (3DGS) has emerged as a promising approach for novel\nview synthesis, offering real-time rendering with high visual fidelity.\nHowever, its substantial storage requirements present significant challenges\nfor practical applications. While recent state-of-the-art (SOTA) 3DGS methods\nincreasingly incorporate dedicated compression modules, there is a lack of a\ncomprehensive framework to evaluate their perceptual impact. Therefore we\npresent 3DGS-IEval-15K, the first large-scale image quality assessment (IQA)\ndataset specifically designed for compressed 3DGS representations. Our dataset\nencompasses 15,200 images rendered from 10 real-world scenes through 6\nrepresentative 3DGS algorithms at 20 strategically selected viewpoints, with\ndifferent compression levels leading to various distortion effects. Through\ncontrolled subjective experiments, we collect human perception data from 60\nviewers. We validate dataset quality through scene diversity and MOS\ndistribution analysis, and establish a comprehensive benchmark with 30\nrepresentative IQA metrics covering diverse types. As the largest-scale 3DGS\nquality assessment dataset to date, our work provides a foundation for\ndeveloping 3DGS specialized IQA metrics, and offers essential data for\ninvestigating view-dependent quality distribution patterns unique to 3DGS. The\ndatabase is publicly available at https://github.com/YukeXing/3DGS-IEval-15K."}
{"id": "2506.14766", "categories": ["cs.CV", "cs.CL", "68T45"], "pdf": "https://arxiv.org/pdf/2506.14766", "abs": "https://arxiv.org/abs/2506.14766", "authors": ["Yujun Wang", "Jinhe Bi", "Yunpu Ma", "Soeren Pirk"], "title": "ASCD: Attention-Steerable Contrastive Decoding for Reducing Hallucination in MLLM", "comment": "15 pages, 7 figures", "summary": "Multimodal Large Language Model (MLLM) often suffer from hallucinations. They\nover-rely on partial cues and generate incorrect responses. Recently, methods\nlike Visual Contrastive Decoding (VCD) and Instruction Contrastive Decoding\n(ICD) have been proposed to mitigate hallucinations by contrasting predictions\nfrom perturbed or negatively prefixed inputs against original outputs. In this\nwork, we uncover that methods like VCD and ICD fundamentally influence internal\nattention dynamics of the model. This observation suggests that their\neffectiveness may not stem merely from surface-level modifications to logits\nbut from deeper shifts in attention distribution. Inspired by this insight, we\npropose an attention-steerable contrastive decoding framework that directly\nintervenes in attention mechanisms of the model to offer a more principled\napproach to mitigating hallucinations. Our experiments across multiple MLLM\narchitectures and diverse decoding methods demonstrate that our approach\nsignificantly reduces hallucinations and improves the performance on benchmarks\nsuch as POPE, CHAIR, and MMHal-Bench, while simultaneously enhancing\nperformance on standard VQA benchmarks."}
{"id": "2506.14667", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14667", "abs": "https://arxiv.org/abs/2506.14667", "authors": ["Matt Poyser", "Toby P. Breckon"], "title": "DDS-NAS: Dynamic Data Selection within Neural Architecture Search via On-line Hard Example Mining applied to Image Classification", "comment": "27 single-column pages, 8 figures, to be published in Pattern\n  Recognition", "summary": "In order to address the scalability challenge within Neural Architecture\nSearch (NAS), we speed up NAS training via dynamic hard example mining within a\ncurriculum learning framework. By utilizing an autoencoder that enforces an\nimage similarity embedding in latent space, we construct an efficient kd-tree\nstructure to order images by furthest neighbour dissimilarity in a\nlow-dimensional embedding. From a given query image from our subsample dataset,\nwe can identify the most dissimilar image within the global dataset in\nlogarithmic time. Via curriculum learning, we then dynamically re-formulate an\nunbiased subsample dataset for NAS optimisation, upon which the current NAS\nsolution architecture performs poorly. We show that our DDS-NAS framework\nspeeds up gradient-based NAS strategies by up to 27x without loss in\nperformance. By maximising the contribution of each image sample during\ntraining, we reduce the duration of a NAS training cycle and the number of\niterations required for convergence."}
{"id": "2506.14674", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14674", "abs": "https://arxiv.org/abs/2506.14674", "authors": ["Ling Li", "Yao Zhou", "Yuxuan Liang", "Fugee Tsung", "Jiaheng Wei"], "title": "Recognition through Reasoning: Reinforcing Image Geo-localization with Large Vision-Language Models", "comment": null, "summary": "Previous methods for image geo-localization have typically treated the task\nas either classification or retrieval, often relying on black-box decisions\nthat lack interpretability. The rise of large vision-language models (LVLMs)\nhas enabled a rethinking of geo-localization as a reasoning-driven task\ngrounded in visual cues. However, two major challenges persist. On the data\nside, existing reasoning-focused datasets are primarily based on street-view\nimagery, offering limited scene diversity and constrained viewpoints. On the\nmodeling side, current approaches predominantly rely on supervised fine-tuning,\nwhich yields only marginal improvements in reasoning capabilities. To address\nthese challenges, we propose a novel pipeline that constructs a\nreasoning-oriented geo-localization dataset, MP16-Reason, using diverse social\nmedia images. We introduce GLOBE, Group-relative policy optimization for\nLocatability assessment and Optimized visual-clue reasoning, yielding\nBi-objective geo-Enhancement for the VLM in recognition and reasoning. GLOBE\nincorporates task-specific rewards that jointly enhance locatability\nassessment, visual clue reasoning, and geolocation accuracy. Both qualitative\nand quantitative results demonstrate that GLOBE outperforms state-of-the-art\nopen-source LVLMs on geo-localization tasks, particularly in diverse visual\nscenes, while also generating more insightful and interpretable reasoning\ntrajectories."}
{"id": "2506.14686", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14686", "abs": "https://arxiv.org/abs/2506.14686", "authors": ["Xi Chen", "Hengshuang Zhao"], "title": "FocalClick-XL: Towards Unified and High-quality Interactive Segmentation", "comment": null, "summary": "Interactive segmentation enables users to extract binary masks of target\nobjects through simple interactions such as clicks, scribbles, and boxes.\nHowever, existing methods often support only limited interaction forms and\nstruggle to capture fine details. In this paper, we revisit the classical\ncoarse-to-fine design of FocalClick and introduce significant extensions.\nInspired by its multi-stage strategy, we propose a novel pipeline,\nFocalClick-XL, to address these challenges simultaneously. Following the\nemerging trend of large-scale pretraining, we decompose interactive\nsegmentation into meta-tasks that capture different levels of information --\ncontext, object, and detail -- assigning a dedicated subnet to each level.This\ndecomposition allows each subnet to undergo scaled pretraining with independent\ndata and supervision, maximizing its effectiveness. To enhance flexibility, we\nshare context- and detail-level information across different interaction forms\nas common knowledge while introducing a prompting layer at the object level to\nencode specific interaction types. As a result, FocalClick-XL achieves\nstate-of-the-art performance on click-based benchmarks and demonstrates\nremarkable adaptability to diverse interaction formats, including boxes,\nscribbles, and coarse masks. Beyond binary mask generation, it is also capable\nof predicting alpha mattes with fine-grained details, making it a versatile and\npowerful tool for interactive segmentation."}
{"id": "2506.14696", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14696", "abs": "https://arxiv.org/abs/2506.14696", "authors": ["Dahang Wan", "Rongsheng Lu", "Yang Fang", "Xianli Lang", "Shuangbao Shu", "Jingjing Chen", "Siyuan Shen", "Ting Xu", "Zecong Ye"], "title": "YOLOv11-RGBT: Towards a Comprehensive Single-Stage Multispectral Object Detection Framework", "comment": "28 pages, 8 figures", "summary": "Multispectral object detection, which integrates information from multiple\nbands, can enhance detection accuracy and environmental adaptability, holding\ngreat application potential across various fields. Although existing methods\nhave made progress in cross-modal interaction, low-light conditions, and model\nlightweight, there are still challenges like the lack of a unified single-stage\nframework, difficulty in balancing performance and fusion strategy, and\nunreasonable modality weight allocation. To address these, based on the YOLOv11\nframework, we present YOLOv11-RGBT, a new comprehensive multimodal object\ndetection framework. We designed six multispectral fusion modes and\nsuccessfully applied them to models from YOLOv3 to YOLOv12 and RT-DETR. After\nreevaluating the importance of the two modalities, we proposed a P3 mid-fusion\nstrategy and multispectral controllable fine-tuning (MCF) strategy for\nmultispectral models. These improvements optimize feature fusion, reduce\nredundancy and mismatches, and boost overall model performance. Experiments\nshow our framework excels on three major open-source multispectral object\ndetection datasets, like LLVIP and FLIR. Particularly, the multispectral\ncontrollable fine-tuning strategy significantly enhanced model adaptability and\nrobustness. On the FLIR dataset, it consistently improved YOLOv11 models' mAP\nby 3.41%-5.65%, reaching a maximum of 47.61%, verifying the framework and\nstrategies' effectiveness. The code is available at:\nhttps://github.com/wandahangFY/YOLOv11-RGBT."}
{"id": "2506.14706", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14706", "abs": "https://arxiv.org/abs/2506.14706", "authors": ["Ni Ou", "Zhuo Chen", "Xinru Zhang", "Junzheng Wang"], "title": "Iterative Camera-LiDAR Extrinsic Optimization via Surrogate Diffusion", "comment": "7 pages, 4 figures, accepted by IROS 2025", "summary": "Cameras and LiDAR are essential sensors for autonomous vehicles. The fusion\nof camera and LiDAR data addresses the limitations of individual sensors but\nrelies on precise extrinsic calibration. Recently, numerous end-to-end\ncalibration methods have been proposed; however, most predict extrinsic\nparameters in a single step and lack iterative optimization capabilities. To\naddress the increasing demand for higher accuracy, we propose a versatile\niterative framework based on surrogate diffusion. This framework can enhance\nthe performance of any calibration method without requiring architectural\nmodifications. Specifically, the initial extrinsic parameters undergo iterative\nrefinement through a denoising process, in which the original calibration\nmethod serves as a surrogate denoiser to estimate the final extrinsics at each\nstep. For comparative analysis, we selected four state-of-the-art calibration\nmethods as surrogate denoisers and compared the results of our diffusion\nprocess with those of two other iterative approaches. Extensive experiments\ndemonstrate that when integrated with our diffusion model, all calibration\nmethods achieve higher accuracy, improved robustness, and greater stability\ncompared to other iterative techniques and their single-step counterparts."}
{"id": "2506.14709", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.14709", "abs": "https://arxiv.org/abs/2506.14709", "authors": ["Kunal Swami", "Debtanu Gupta", "Amrit Kumar Muduli", "Chirag Jaiswal", "Pankaj Kumar Bajpai"], "title": "DiFuse-Net: RGB and Dual-Pixel Depth Estimation using Window Bi-directional Parallax Attention and Cross-modal Transfer Learning", "comment": "Accepted in IROS 2025", "summary": "Depth estimation is crucial for intelligent systems, enabling applications\nfrom autonomous navigation to augmented reality. While traditional stereo and\nactive depth sensors have limitations in cost, power, and robustness,\ndual-pixel (DP) technology, ubiquitous in modern cameras, offers a compelling\nalternative. This paper introduces DiFuse-Net, a novel modality decoupled\nnetwork design for disentangled RGB and DP based depth estimation. DiFuse-Net\nfeatures a window bi-directional parallax attention mechanism (WBiPAM)\nspecifically designed to capture the subtle DP disparity cues unique to\nsmartphone cameras with small aperture. A separate encoder extracts contextual\ninformation from the RGB image, and these features are fused to enhance depth\nprediction. We also propose a Cross-modal Transfer Learning (CmTL) mechanism to\nutilize large-scale RGB-D datasets in the literature to cope with the\nlimitations of obtaining large-scale RGB-DP-D dataset. Our evaluation and\ncomparison of the proposed method demonstrates its superiority over the DP and\nstereo-based baseline methods. Additionally, we contribute a new, high-quality,\nreal-world RGB-DP-D training dataset, named Dual-Camera Dual-Pixel (DCDP)\ndataset, created using our novel symmetric stereo camera hardware setup, stereo\ncalibration and rectification protocol, and AI stereo disparity estimation\nmethod."}
{"id": "2506.14730", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14730", "abs": "https://arxiv.org/abs/2506.14730", "authors": ["Corey Scher", "Jamon Van Den Hoek"], "title": "Active InSAR monitoring of building damage in Gaza during the Israel-Hamas War", "comment": null, "summary": "Aerial bombardment of the Gaza Strip beginning October 7, 2023 is one of the\nmost intense bombing campaigns of the twenty-first century, driving widespread\nurban damage. Characterizing damage over a geographically dynamic and\nprotracted armed conflict requires active monitoring. Synthetic aperture radar\n(SAR) has precedence for mapping disaster-induced damage with bi-temporal\nmethods but applications to active monitoring during sustained crises are\nlimited. Using interferometric SAR data from Sentinel-1, we apply a long\ntemporal-arc coherent change detection (LT-CCD) approach to track weekly damage\ntrends over the first year of the 2023- Israel-Hamas War. We detect 92.5% of\ndamage labels in reference data from the United Nations with a negligible\n(1.2%) false positive rate. The temporal fidelity of our approach reveals\nrapidly increasing damage during the first three months of the war focused in\nnorthern Gaza, a notable pause in damage during a temporary ceasefire, and\nsurges of new damage as conflict hot-spots shift from north to south.\nThree-fifths (191,263) of all buildings are damaged or destroyed by the end of\nthe study. With massive need for timely data on damage in armed conflict zones,\nour low-cost and low-latency approach enables rapid uptake of damage\ninformation at humanitarian and journalistic organizations."}
{"id": "2506.14742", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14742", "abs": "https://arxiv.org/abs/2506.14742", "authors": ["Ziqiao Peng", "Wentao Hu", "Junyuan Ma", "Xiangyu Zhu", "Xiaomei Zhang", "Hao Zhao", "Hui Tian", "Jun He", "Hongyan Liu", "Zhaoxin Fan"], "title": "SyncTalk++: High-Fidelity and Efficient Synchronized Talking Heads Synthesis Using Gaussian Splatting", "comment": null, "summary": "Achieving high synchronization in the synthesis of realistic, speech-driven\ntalking head videos presents a significant challenge. A lifelike talking head\nrequires synchronized coordination of subject identity, lip movements, facial\nexpressions, and head poses. The absence of these synchronizations is a\nfundamental flaw, leading to unrealistic results. To address the critical issue\nof synchronization, identified as the ''devil'' in creating realistic talking\nheads, we introduce SyncTalk++, which features a Dynamic Portrait Renderer with\nGaussian Splatting to ensure consistent subject identity preservation and a\nFace-Sync Controller that aligns lip movements with speech while innovatively\nusing a 3D facial blendshape model to reconstruct accurate facial expressions.\nTo ensure natural head movements, we propose a Head-Sync Stabilizer, which\noptimizes head poses for greater stability. Additionally, SyncTalk++ enhances\nrobustness to out-of-distribution (OOD) audio by incorporating an Expression\nGenerator and a Torso Restorer, which generate speech-matched facial\nexpressions and seamless torso regions. Our approach maintains consistency and\ncontinuity in visual details across frames and significantly improves rendering\nspeed and quality, achieving up to 101 frames per second. Extensive experiments\nand user studies demonstrate that SyncTalk++ outperforms state-of-the-art\nmethods in synchronization and realism. We recommend watching the supplementary\nvideo: https://ziqiaopeng.github.io/synctalk++."}
{"id": "2506.14753", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.14753", "abs": "https://arxiv.org/abs/2506.14753", "authors": ["Qinchan", "Li", "Kenneth Chen", "Changyue", "Su", "Wittawat Jitkrittum", "Qi Sun", "Patsorn Sangkloy"], "title": "Cost-Aware Routing for Efficient Text-To-Image Generation", "comment": null, "summary": "Diffusion models are well known for their ability to generate a high-fidelity\nimage for an input prompt through an iterative denoising process.\nUnfortunately, the high fidelity also comes at a high computational cost due\nthe inherently sequential generative process. In this work, we seek to\noptimally balance quality and computational cost, and propose a framework to\nallow the amount of computation to vary for each prompt, depending on its\ncomplexity. Each prompt is automatically routed to the most appropriate\ntext-to-image generation function, which may correspond to a distinct number of\ndenoising steps of a diffusion model, or a disparate, independent text-to-image\nmodel. Unlike uniform cost reduction techniques (e.g., distillation, model\nquantization), our approach achieves the optimal trade-off by learning to\nreserve expensive choices (e.g., 100+ denoising steps) only for a few complex\nprompts, and employ more economical choices (e.g., small distilled model) for\nless sophisticated prompts. We empirically demonstrate on COCO and DiffusionDB\nthat by learning to route to nine already-trained text-to-image models, our\napproach is able to deliver an average quality that is higher than that\nachievable by any of these models alone."}
{"id": "2506.14765", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14765", "abs": "https://arxiv.org/abs/2506.14765", "authors": ["Nikolaos Dionelis", "Jente Bosmans", "Riccardo Musto", "Giancarlo Paoletti", "Simone Sarti", "Giacomo Cascarano", "Casper Fibaek", "Luke Camilleri", "Bertrand Le Saux", "Nicolas Longépé"], "title": "Scaling-Up the Pretraining of the Earth Observation Foundation Model PhilEO to the MajorTOM Dataset", "comment": "6 pages, 9 figures, 1 table, 29 references", "summary": "Today, Earth Observation (EO) satellites generate massive volumes of data,\nwith the Copernicus Sentinel-2 constellation alone producing approximately\n1.6TB per day. To fully exploit this information, it is essential to pretrain\nEO Foundation Models (FMs) on large unlabeled datasets, enabling efficient\nfine-tuning for several different downstream tasks with minimal labeled data.\nIn this work, we present the scaling-up of our recently proposed EO Foundation\nModel, PhilEO Geo-Aware U-Net, on the unlabeled 23TB dataset MajorTOM, which\ncovers the vast majority of the Earth's surface, as well as on the specialized\nsubset FastTOM 2TB that does not include oceans and ice. We develop and study\nvarious PhilEO model variants with different numbers of parameters and\narchitectures. Finally, we fine-tune the models on the PhilEO Bench for road\ndensity estimation, building density pixel-wise regression, and land cover\nsemantic segmentation, and we evaluate the performance. Our results demonstrate\nthat for all n-shots for road density regression, the PhilEO 44M MajorTOM 23TB\nmodel outperforms PhilEO Globe 0.5TB 44M. We also show that for most n-shots\nfor road density estimation and building density regression, PhilEO 200M\nFastTOM outperforms all the other models. The effectiveness of both dataset and\nmodel scaling is validated using the PhilEO Bench. We also study the impact of\narchitecture scaling, transitioning from U-Net Convolutional Neural Networks\n(CNN) to Vision Transformers (ViT)."}
{"id": "2506.14766", "categories": ["cs.CV", "cs.CL", "68T45"], "pdf": "https://arxiv.org/pdf/2506.14766", "abs": "https://arxiv.org/abs/2506.14766", "authors": ["Yujun Wang", "Jinhe Bi", "Yunpu Ma", "Soeren Pirk"], "title": "ASCD: Attention-Steerable Contrastive Decoding for Reducing Hallucination in MLLM", "comment": "15 pages, 7 figures", "summary": "Multimodal Large Language Model (MLLM) often suffer from hallucinations. They\nover-rely on partial cues and generate incorrect responses. Recently, methods\nlike Visual Contrastive Decoding (VCD) and Instruction Contrastive Decoding\n(ICD) have been proposed to mitigate hallucinations by contrasting predictions\nfrom perturbed or negatively prefixed inputs against original outputs. In this\nwork, we uncover that methods like VCD and ICD fundamentally influence internal\nattention dynamics of the model. This observation suggests that their\neffectiveness may not stem merely from surface-level modifications to logits\nbut from deeper shifts in attention distribution. Inspired by this insight, we\npropose an attention-steerable contrastive decoding framework that directly\nintervenes in attention mechanisms of the model to offer a more principled\napproach to mitigating hallucinations. Our experiments across multiple MLLM\narchitectures and diverse decoding methods demonstrate that our approach\nsignificantly reduces hallucinations and improves the performance on benchmarks\nsuch as POPE, CHAIR, and MMHal-Bench, while simultaneously enhancing\nperformance on standard VQA benchmarks."}
{"id": "2506.14769", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.14769", "abs": "https://arxiv.org/abs/2506.14769", "authors": ["Jiahua Ma", "Yiran Qin", "Yixiong Li", "Xuanqi Liao", "Yulan Guo", "Ruimao Zhang"], "title": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal Diffusion", "comment": null, "summary": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions."}
{"id": "2506.13888", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.13888", "abs": "https://arxiv.org/abs/2506.13888", "authors": ["Jipeng Zhang", "Kehao Miao", "Renjie Pi", "Zhaowei Wang", "Runtao Liu", "Rui Pan", "Tong Zhang"], "title": "VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative Training", "comment": null, "summary": "Reinforcement Fine-Tuning (RFT) with verifiable rewards has advanced large\nlanguage models but remains underexplored for Vision-Language (VL) models. The\nVision-Language Reward Model (VL-RM) is key to aligning VL models by providing\nstructured feedback, yet training effective VL-RMs faces two major challenges.\nFirst, the bootstrapping dilemma arises as high-quality training data depends\non already strong VL models, creating a cycle where self-generated supervision\nreinforces existing biases. Second, modality bias and negative example\namplification occur when VL models hallucinate incorrect visual attributes,\nleading to flawed preference data that further misguides training. To address\nthese issues, we propose an iterative training framework leveraging vision\nexperts, Chain-of-Thought (CoT) rationales, and Margin-based Rejection\nSampling. Our approach refines preference datasets, enhances structured\ncritiques, and iteratively improves reasoning. Experiments across VL-RM\nbenchmarks demonstrate superior performance in hallucination detection and\nmultimodal reasoning, advancing VL model alignment with reinforcement learning."}
{"id": "2506.13769", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.13769", "abs": "https://arxiv.org/abs/2506.13769", "authors": ["Filippo Leveni"], "title": "Non-planar Object Detection and Identification by Features Matching and Triangulation Growth", "comment": "Master's thesis at Politecnico di Milano", "summary": "Object detection and identification is surely a fundamental topic in the computer vision field; it plays a crucial role in many applications such as object tracking, industrial robots control, image retrieval, etc. We propose a feature-based approach for detecting and identifying distorted occurrences of a given template in a scene image by incremental grouping of feature matches between the image and the template. For this purpose, we consider the Delaunay triangulation of template features as an useful tool through which to be guided in this iterative approach. The triangulation is treated as a graph and, starting from a single triangle, neighboring nodes are considered and the corresponding features are identified; then matches related to them are evaluated to determine if they are worthy to be grouped. This evaluation is based on local consistency criteria derived from geometric and photometric properties of local features. Our solution allows the identification of the object in situations where geometric models (e.g. homography) does not hold, thus enable the detection of objects such that the template is non planar or when it is planar but appears distorted in the image. We show that our approach performs just as well or better than application of homography-based RANSAC in scenarios in which distortion is nearly absent, while when the deformation becomes relevant our method shows better description performance."}
{"id": "2506.13770", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.13770", "abs": "https://arxiv.org/abs/2506.13770", "authors": ["Shiwen Zhang", "Zhuowei Chen", "Lang Chen", "Yanze Wu"], "title": "CDST: Color Disentangled Style Transfer for Universal Style Reference Customization", "comment": "codes and models will be released if the paper is accepted", "summary": "We introduce Color Disentangled Style Transfer (CDST), a novel and efficient two-stream style transfer training paradigm which completely isolates color from style and forces the style stream to be color-blinded. With one same model, CDST unlocks universal style transfer capabilities in a tuning-free manner during inference. Especially, the characteristics-preserved style transfer with style and content references is solved in the tuning-free way for the first time. CDST significantly improves the style similarity by multi-feature image embeddings compression and preserves strong editing capability via our new CDST style definition inspired by Diffusion UNet disentanglement law. By conducting thorough qualitative and quantitative experiments and human evaluations, we demonstrate that CDST achieves state-of-the-art results on various style transfer tasks."}
{"id": "2506.13780", "categories": ["cs.CV", "cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.13780", "abs": "https://arxiv.org/abs/2506.13780", "authors": ["Sedat Porikli", "Vedat Porikli"], "title": "Hidden Bias in the Machine: Stereotypes in Text-to-Image Models", "comment": "Equal contribution by both authors, Published at CVPR 2025 Workshop on Experimental Model Auditing via Controllable Synthesis (EMACS) and Workshop on Demographic Diversity in Computer Vision (DemoDiv)", "summary": "Text-to-Image (T2I) models have transformed visual content creation, producing highly realistic images from natural language prompts. However, concerns persist around their potential to replicate and magnify existing societal biases. To investigate these issues, we curated a diverse set of prompts spanning thematic categories such as occupations, traits, actions, ideologies, emotions, family roles, place descriptions, spirituality, and life events. For each of the 160 unique topics, we crafted multiple prompt variations to reflect a wide range of meanings and perspectives. Using Stable Diffusion 1.5 (UNet-based) and Flux-1 (DiT-based) models with original checkpoints, we generated over 16,000 images under consistent settings. Additionally, we collected 8,000 comparison images from Google Image Search. All outputs were filtered to exclude abstract, distorted, or nonsensical results. Our analysis reveals significant disparities in the representation of gender, race, age, somatotype, and other human-centric factors across generated images. These disparities often mirror and reinforce harmful stereotypes embedded in societal narratives. We discuss the implications of these findings and emphasize the need for more inclusive datasets and development practices to foster fairness in generative visual systems."}
{"id": "2506.13846", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.13846", "abs": "https://arxiv.org/abs/2506.13846", "authors": ["Runtao Liu", "Jiahao Zhan", "Yingqing He", "Chen Wei", "Alan Yuille", "Qifeng Chen"], "title": "Fake it till You Make it: Reward Modeling as Discriminative Prediction", "comment": null, "summary": "An effective reward model plays a pivotal role in reinforcement learning for post-training enhancement of visual generative models. However, current approaches of reward modeling suffer from implementation complexity due to their reliance on extensive human-annotated preference data or meticulously engineered quality dimensions that are often incomplete and engineering-intensive. Inspired by adversarial training in generative adversarial networks (GANs), this paper proposes GAN-RM, an efficient reward modeling framework that eliminates manual preference annotation and explicit quality dimension engineering. Our method trains the reward model through discrimination between a small set of representative, unpaired target samples(denoted as Preference Proxy Data) and model-generated ordinary outputs, requiring only a few hundred target samples. Comprehensive experiments demonstrate our GAN-RM's effectiveness across multiple key applications including test-time scaling implemented as Best-of-N sample filtering, post-training approaches like Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO)."}
{"id": "2506.13796", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.13796", "abs": "https://arxiv.org/abs/2506.13796", "authors": ["Zhou Chen", "Xiao Wang", "Yuanhong Liao", "Ming Lin", "Yuqi Bai"], "title": "ClimateChat: Designing Data and Methods for Instruction Tuning LLMs to Answer Climate Change Queries", "comment": "ICLR 2025 camera ready, 13 pages, 4 figures, 4 tables", "summary": "As the issue of global climate change becomes increasingly severe, the demand for research in climate science continues to grow. Natural language processing technologies, represented by Large Language Models (LLMs), have been widely applied to climate change-specific research, providing essential information support for decision-makers and the public. Some studies have improved model performance on relevant tasks by constructing climate change-related instruction data and instruction-tuning LLMs. However, current research remains inadequate in efficiently producing large volumes of high-precision instruction data for climate change, which limits further development of climate change LLMs. This study introduces an automated method for constructing instruction data. The method generates instructions using facts and background knowledge from documents and enhances the diversity of the instruction data through web scraping and the collection of seed instructions. Using this method, we constructed a climate change instruction dataset, named ClimateChat-Corpus, which was used to fine-tune open-source LLMs, resulting in an LLM named ClimateChat. Evaluation results show that ClimateChat significantly improves performance on climate change question-and-answer tasks. Additionally, we evaluated the impact of different base models and instruction data on LLM performance and demonstrated its capability to adapt to a wide range of climate change scientific discovery tasks, emphasizing the importance of selecting an appropriate base model for instruction tuning. This research provides valuable references and empirical support for constructing climate change instruction data and training climate change-specific LLMs."}
{"id": "2506.13897", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.13897", "abs": "https://arxiv.org/abs/2506.13897", "authors": ["Thomas Kreutz", "Max Mühlhäuser", "Alejandro Sanchez Guinea"], "title": "DeSPITE: Exploring Contrastive Deep Skeleton-Pointcloud-IMU-Text Embeddings for Advanced Point Cloud Human Activity Understanding", "comment": "This work is currently under review at ICCV 2025", "summary": "Despite LiDAR (Light Detection and Ranging) being an effective privacy-preserving alternative to RGB cameras to perceive human activities, it remains largely underexplored in the context of multi-modal contrastive pre-training for human activity understanding (e.g., human activity recognition (HAR), retrieval, or person re-identification (RE-ID)). To close this gap, our work explores learning the correspondence between LiDAR point clouds, human skeleton poses, IMU data, and text in a joint embedding space. More specifically, we present DeSPITE, a Deep Skeleton-Pointcloud-IMU-Text Embedding model, which effectively learns a joint embedding space across these four modalities through noise contrastive estimation. At the heart of our empirical exploration, we have combined the existing LIPD and Babel datasets, which enabled us to synchronize data of all four modalities, allowing us to explore the learning of a new joint embedding space. Our experiments demonstrate novel human activity understanding tasks for point cloud sequences enabled through DeSPITE, including Skeleton<->Pointcloud<->IMU matching, retrieval, and temporal moment retrieval. Furthermore, we show that DeSPITE is an effective pre-training strategy for point cloud HAR through experiments in MSR-Action3D and HMPEAR."}
{"id": "2506.13886", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.13886", "abs": "https://arxiv.org/abs/2506.13886", "authors": ["Antara Raaghavi Bhattacharya", "Isabel Papadimitriou", "Kathryn Davidson", "David Alvarez-Melis"], "title": "Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles", "comment": null, "summary": "Across languages, numeral systems vary widely in how they construct and combine numbers. While humans consistently learn to navigate this diversity, large language models (LLMs) struggle with linguistic-mathematical puzzles involving cross-linguistic numeral systems, which humans can learn to solve successfully. We investigate why this task is difficult for LLMs through a series of experiments that untangle the linguistic and mathematical aspects of numbers in language. Our experiments establish that models cannot consistently solve such problems unless the mathematical operations in the problems are explicitly marked using known symbols ($+$, $\\times$, etc, as in \"twenty + three\"). In further ablation studies, we probe how individual parameters of numeral construction and combination affect performance. While humans use their linguistic understanding of numbers to make inferences about the implicit compositional structure of numerals, LLMs seem to lack this notion of implicit numeral structure. We conclude that the ability to flexibly infer compositional rules from implicit patterns in human-scale data remains an open challenge for current reasoning models."}
{"id": "2506.13902", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.13902", "abs": "https://arxiv.org/abs/2506.13902", "authors": ["Raymond Yu", "Paul Han", "Josh Myers-Dean", "Piper Wolters", "Favyen Bastani"], "title": "OPTIMUS: Observing Persistent Transformations in Multi-temporal Unlabeled Satellite-data", "comment": "WACV 2025", "summary": "In the face of pressing environmental issues in the 21st century, monitoring surface changes on Earth is more important than ever. Large-scale remote sensing, such as satellite imagery, is an important tool for this task. However, using supervised methods to detect changes is difficult because of the lack of satellite data annotated with change labels, especially for rare categories of change. Annotation proves challenging due to the sparse occurrence of changes in satellite images. Even within a vast collection of images, only a small fraction may exhibit persistent changes of interest. To address this challenge, we introduce OPTIMUS, a self-supervised learning method based on an intuitive principle: if a model can recover information about the relative order of images in the time series, then that implies that there are long-lasting changes in the images. OPTIMUS demonstrates this principle by using change point detection methods on model outputs in a time series. We demonstrate that OPTIMUS can directly detect interesting changes in satellite images, achieving an improvement in AUROC score from 56.3% to 87.6% at distinguishing changed time series from unchanged ones compared to baselines. Our code and dataset are available at https://huggingface.co/datasets/optimus-change/optimus-dataset/."}
{"id": "2506.13888", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.13888", "abs": "https://arxiv.org/abs/2506.13888", "authors": ["Jipeng Zhang", "Kehao Miao", "Renjie Pi", "Zhaowei Wang", "Runtao Liu", "Rui Pan", "Tong Zhang"], "title": "VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative Training", "comment": null, "summary": "Reinforcement Fine-Tuning (RFT) with verifiable rewards has advanced large language models but remains underexplored for Vision-Language (VL) models. The Vision-Language Reward Model (VL-RM) is key to aligning VL models by providing structured feedback, yet training effective VL-RMs faces two major challenges. First, the bootstrapping dilemma arises as high-quality training data depends on already strong VL models, creating a cycle where self-generated supervision reinforces existing biases. Second, modality bias and negative example amplification occur when VL models hallucinate incorrect visual attributes, leading to flawed preference data that further misguides training. To address these issues, we propose an iterative training framework leveraging vision experts, Chain-of-Thought (CoT) rationales, and Margin-based Rejection Sampling. Our approach refines preference datasets, enhances structured critiques, and iteratively improves reasoning. Experiments across VL-RM benchmarks demonstrate superior performance in hallucination detection and multimodal reasoning, advancing VL model alignment with reinforcement learning."}
{"id": "2506.13910", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.13910", "abs": "https://arxiv.org/abs/2506.13910", "authors": ["Aritra Dutta", "Pushpita Boral", "G Suseela"], "title": "Intelligent Image Sensing for Crime Analysis: A ML Approach towards Enhanced Violence Detection and Investigation", "comment": null, "summary": "The increasing global crime rate, coupled with substantial human and property losses, highlights the limitations of traditional surveillance methods in promptly detecting diverse and unexpected acts of violence. Addressing this pressing need for automatic violence detection, we leverage Machine Learning to detect and categorize violent events in video streams. This paper introduces a comprehensive framework for violence detection and classification, employing Supervised Learning for both binary and multi-class violence classification. The detection model relies on 3D Convolutional Neural Networks, while the classification model utilizes the separable convolutional 3D model for feature extraction and bidirectional LSTM for temporal processing. Training is conducted on a diverse customized datasets with frame-level annotations, incorporating videos from surveillance cameras, human recordings, hockey fight, sohas and wvd dataset across various platforms. Additionally, a camera module integrated with raspberry pi is used to capture live video feed, which is sent to the ML model for processing. Thus, demonstrating improved performance in terms of computational resource efficiency and accuracy."}
{"id": "2506.13894", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.13894", "abs": "https://arxiv.org/abs/2506.13894", "authors": ["Ryuki Matsuura", "Shikhar Bharadwaj", "Jiarui Liu", "Dhatchi Kunde Govindarajan"], "title": "EmoNews: A Spoken Dialogue System for Expressive News Conversations", "comment": null, "summary": "We develop a task-oriented spoken dialogue system (SDS) that regulates emotional speech based on contextual cues to enable more empathetic news conversations. Despite advancements in emotional text-to-speech (TTS) techniques, task-oriented emotional SDSs remain underexplored due to the compartmentalized nature of SDS and emotional TTS research, as well as the lack of standardized evaluation metrics for social goals. We address these challenges by developing an emotional SDS for news conversations that utilizes a large language model (LLM)-based sentiment analyzer to identify appropriate emotions and PromptTTS to synthesize context-appropriate emotional speech. We also propose subjective evaluation scale for emotional SDSs and judge the emotion regulation performance of the proposed and baseline systems. Experiments showed that our emotional SDS outperformed a baseline system in terms of the emotion regulation and engagement. These results suggest the critical role of speech emotion for more engaging conversations. All our source code is open-sourced at https://github.com/dhatchi711/espnet-emotional-news/tree/emo-sds/egs2/emo_news_sds/sds1"}
{"id": "2506.13925", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.13925", "abs": "https://arxiv.org/abs/2506.13925", "authors": ["Numair Nadeem", "Saeed Anwar", "Muhammad Hamza Asad", "Abdul Bais"], "title": "HierVL: Semi-Supervised Segmentation leveraging Hierarchical Vision-Language Synergy with Dynamic Text-Spatial Query Alignment", "comment": null, "summary": "Semi-supervised semantic segmentation remains challenging under severe label scarcity and domain variability. Vision-only methods often struggle to generalize, resulting in pixel misclassification between similar classes, poor generalization and boundary localization. Vision-Language Models offer robust, domain-invariant semantics but lack the spatial grounding required for dense prediction. We introduce HierVL, a unified framework that bridges this gap by integrating abstract text embeddings into a mask-transformer architecture tailored for semi-supervised segmentation. HierVL features three novel components: a Hierarchical Semantic Query Generator that filters and projects abstract class embeddings into multi-scale queries to suppress irrelevant classes and handle intra-class variability; a Cross-Modal Spatial Alignment Module that aligns semantic queries with pixel features for sharper boundaries under sparse supervision; and a Dual-Query Transformer Decoder that fuses semantic and instance-level queries to prevent instance collapse. We also introduce targeted regularization losses that maintain vision-language alignment throughout training to reinforce semantic grounding. HierVL establishes a new state-of-the-art by achieving a +4.4% mean improvement of the intersection over the union on COCO (with 232 labeled images), +3.1% on Pascal VOC (with 92 labels), +5.9% on ADE20 (with 158 labels) and +1.8% on Cityscapes (with 100 labels), demonstrating better performance under 1% supervision on four benchmark datasets. Our results show that language-guided segmentation closes the label efficiency gap and unlocks new levels of fine-grained, instance-aware generalization."}
{"id": "2506.13901", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.13901", "abs": "https://arxiv.org/abs/2506.13901", "authors": ["Abhilekh Borah", "Chhavi Sharma", "Danush Khanna", "Utkarsh Bhatt", "Gurpreet Singh", "Hasnat Md Abdullah", "Raghav Kaushik Ravi", "Vinija Jain", "Jyoti Patel", "Shubham Singh", "Vasu Sharma", "Arpita Vats", "Rahul Raja", "Aman Chadha", "Amitava Das"], "title": "Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise Pooled Representations", "comment": null, "summary": "Alignment is no longer a luxury, it is a necessity. As large language models (LLMs) enter high-stakes domains like education, healthcare, governance, and law, their behavior must reliably reflect human-aligned values and safety constraints. Yet current evaluations rely heavily on behavioral proxies such as refusal rates, G-Eval scores, and toxicity classifiers, all of which have critical blind spots. Aligned models are often vulnerable to jailbreaking, stochasticity of generation, and alignment faking.\n  To address this issue, we introduce the Alignment Quality Index (AQI). This novel geometric and prompt-invariant metric empirically assesses LLM alignment by analyzing the separation of safe and unsafe activations in latent space. By combining measures such as the Davies-Bouldin Score (DBS), Dunn Index (DI), Xie-Beni Index (XBI), and Calinski-Harabasz Index (CHI) across various formulations, AQI captures clustering quality to detect hidden misalignments and jailbreak risks, even when outputs appear compliant. AQI also serves as an early warning signal for alignment faking, offering a robust, decoding invariant tool for behavior agnostic safety auditing.\n  Additionally, we propose the LITMUS dataset to facilitate robust evaluation under these challenging conditions. Empirical tests on LITMUS across different models trained under DPO, GRPO, and RLHF conditions demonstrate AQI's correlation with external judges and ability to reveal vulnerabilities missed by refusal metrics. We make our implementation publicly available to foster future research in this area."}
{"id": "2506.13993", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.13993", "abs": "https://arxiv.org/abs/2506.13993", "authors": ["Michelangelo Conserva", "Alex Wilson", "Charlotte Stanton", "Vishal Batchu", "Varun Gulshan"], "title": "Mapping Farmed Landscapes from Remote Sensing", "comment": null, "summary": "Effective management of agricultural landscapes is critical for meeting global biodiversity targets, but efforts are hampered by the absence of detailed, large-scale ecological maps. To address this, we introduce Farmscapes, the first large-scale (covering most of England), high-resolution (25cm) map of rural landscape features, including ecologically vital elements like hedgerows, woodlands, and stone walls. This map was generated using a deep learning segmentation model trained on a novel, dataset of 942 manually annotated tiles derived from aerial imagery. Our model accurately identifies key habitats, achieving high f1-scores for woodland (96\\%) and farmed land (95\\%), and demonstrates strong capability in segmenting linear features, with an F1-score of 72\\% for hedgerows. By releasing the England-wide map on Google Earth Engine, we provide a powerful, open-access tool for ecologists and policymakers. This work enables data-driven planning for habitat restoration, supports the monitoring of initiatives like the EU Biodiversity Strategy, and lays the foundation for advanced analysis of landscape connectivity."}
{"id": "2506.13956", "categories": ["cs.CL", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.13956", "abs": "https://arxiv.org/abs/2506.13956", "authors": ["Shang-Chi Tsai", "Seiya Kawano", "Angel Garcia Contreras", "Koichiro Yoshino", "Yun-Nung Chen"], "title": "ASMR: Augmenting Life Scenario using Large Generative Models for Robotic Action Reflection", "comment": "IWSDS 2024 Best Paper Award", "summary": "When designing robots to assist in everyday human activities, it is crucial to enhance user requests with visual cues from their surroundings for improved intent understanding. This process is defined as a multimodal classification task. However, gathering a large-scale dataset encompassing both visual and linguistic elements for model training is challenging and time-consuming. To address this issue, our paper introduces a novel framework focusing on data augmentation in robotic assistance scenarios, encompassing both dialogues and related environmental imagery. This approach involves leveraging a sophisticated large language model to simulate potential conversations and environmental contexts, followed by the use of a stable diffusion model to create images depicting these environments. The additionally generated data serves to refine the latest multimodal models, enabling them to more accurately determine appropriate actions in response to user interactions with the limited target data. Our experimental results, based on a dataset collected from real-world scenarios, demonstrate that our methodology significantly enhances the robot's action selection capabilities, achieving the state-of-the-art performance."}
{"id": "2506.14008", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14008", "abs": "https://arxiv.org/abs/2506.14008", "authors": ["Daniel Montoya", "Aymen Bouguerra", "Alexandra Gomez-Villa", "Fabio Arnez"], "title": "FindMeIfYouCan: Bringing Open Set metrics to $\\textit{near} $, $ \\textit{far} $ and $\\textit{farther}$ Out-of-Distribution Object Detection", "comment": "Preprint", "summary": "State-of-the-art Object Detection (OD) methods predominantly operate under a closed-world assumption, where test-time categories match those encountered during training. However, detecting and localizing unknown objects is crucial for safety-critical applications in domains such as autonomous driving and medical imaging. Recently, Out-Of-Distribution (OOD) detection has emerged as a vital research direction for OD, focusing on identifying incorrect predictions typically associated with unknown objects. This paper shows that the current evaluation protocol for OOD-OD violates the assumption of non-overlapping objects with respect to the In-Distribution (ID) datasets, and obscures crucial situations such as ignoring unknown objects, potentially leading to overconfidence in deployment scenarios where truly novel objects might be encountered. To address these limitations, we manually curate, and enrich the existing benchmark by exploiting semantic similarity to create new evaluation splits categorized as $\\textit{near}$, $\\textit{far}$, and $\\textit{farther}$ from ID distributions. Additionally, we incorporate established metrics from the Open Set community, providing deeper insights into how effectively methods detect unknowns, when they ignore them, and when they mistakenly classify OOD objects as ID. Our comprehensive evaluation demonstrates that semantically and visually close OOD objects are easier to localize than far ones, but are also more easily confounded with ID objects. $\\textit{Far}$ and $\\textit{farther}$ objects are harder to localize but less prone to be taken for an ID object."}
{"id": "2506.13965", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.13965", "abs": "https://arxiv.org/abs/2506.13965", "authors": ["Aleksander Smywiński-Pohl", "Tomer Libal", "Adam Kaczmarczyk", "Magdalena Król"], "title": "Are manual annotations necessary for statutory interpretations retrieval?", "comment": null, "summary": "One of the elements of legal research is looking for cases where judges have extended the meaning of a legal concept by providing interpretations of what a concept means or does not mean. This allow legal professionals to use such interpretations as precedents as well as laymen to better understand the legal concept. The state-of-the-art approach for retrieving the most relevant interpretations for these concepts currently depends on the ranking of sentences and the training of language models over annotated examples. That manual annotation process can be quite expensive and need to be repeated for each such concept, which prompted recent research in trying to automate this process. In this paper, we highlight the results of various experiments conducted to determine the volume, scope and even the need for manual annotation. First of all, we check what is the optimal number of annotations per a legal concept. Second, we check if we can draw the sentences for annotation randomly or there is a gain in the performance of the model, when only the best candidates are annotated. As the last question we check what is the outcome of automating the annotation process with the help of an LLM."}
{"id": "2506.14015", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14015", "abs": "https://arxiv.org/abs/2506.14015", "authors": ["Nick Yiwen Huang", "Akin Caliskan", "Berkay Kicanaoglu", "James Tompkin", "Hyeongwoo Kim"], "title": "Disentangling 3D from Large Vision-Language Models for Controlled Portrait Generation", "comment": null, "summary": "We consider the problem of disentangling 3D from large vision-language models, which we show on generative 3D portraits. This allows free-form text control of appearance attributes like age, hair style, and glasses, and 3D geometry control of face expression and camera pose. In this setting, we assume we use a pre-trained large vision-language model (LVLM; CLIP) to generate from a smaller 2D dataset with no additional paired labels and with a pre-defined 3D morphable model (FLAME). First, we disentangle using canonicalization to a 2D reference frame from a deformable neural 3D triplane representation. But another form of entanglement arises from the significant noise in the LVLM's embedding space that describes irrelevant features. This damages output quality and diversity, but we overcome this with a Jacobian regularization that can be computed efficiently with a stochastic approximator. Compared to existing methods, our approach produces portraits with added text and 3D control, where portraits remain consistent when either control is changed. Broadly, this approach lets creators control 3D generators on their own 2D face data without needing resources to label large data or train large models."}
{"id": "2506.13978", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.13978", "abs": "https://arxiv.org/abs/2506.13978", "authors": ["Xiuwen Wu", "Hao Wang", "Zhiang Yan", "Xiaohan Tang", "Pengfei Xu", "Wai-Ting Siok", "Ping Li", "Jia-Hong Gao", "Bingjiang Lyu", "Lang Qin"], "title": "AI shares emotion with humans across languages and cultures", "comment": null, "summary": "Effective and safe human-machine collaboration requires the regulated and meaningful exchange of emotions between humans and artificial intelligence (AI). Current AI systems based on large language models (LLMs) can provide feedback that makes people feel heard. Yet it remains unclear whether LLMs represent emotion in language as humans do, or whether and how the emotional tone of their output can be controlled. We assess human-AI emotional alignment across linguistic-cultural groups and model-families, using interpretable LLM features translated from concept-sets for over twenty nuanced emotion categories (including six basic emotions). Our analyses reveal that LLM-derived emotion spaces are structurally congruent with human perception, underpinned by the fundamental affective dimensions of valence and arousal. Furthermore, these emotion-related features also accurately predict large-scale behavioural data on word ratings along these two core dimensions, reflecting both universal and language-specific patterns. Finally, by leveraging steering vectors derived solely from human-centric emotion concepts, we show that model expressions can be stably and naturally modulated across distinct emotion categories, which provides causal evidence that human emotion concepts can be used to systematically induce LLMs to produce corresponding affective states when conveying content. These findings suggest AI not only shares emotional representations with humans but its affective outputs can be precisely guided using psychologically grounded emotion concepts."}
{"id": "2506.14035", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14035", "abs": "https://arxiv.org/abs/2506.14035", "authors": ["Chelsi Jain", "Yiran Wu", "Yifan Zeng", "Jiale Liu", "S hengyu Dai", "Zhenwen Shao", "Qingyun Wu", "Huazheng Wang"], "title": "SimpleDoc: Multi-Modal Document Understanding with Dual-Cue Page Retrieval and Iterative Refinement", "comment": null, "summary": "Document Visual Question Answering (DocVQA) is a practical yet challenging task, which is to ask questions based on documents while referring to multiple pages and different modalities of information, e.g, images and tables. To handle multi-modality, recent methods follow a similar Retrieval Augmented Generation (RAG) pipeline, but utilize Visual Language Models (VLMs) based embedding model to embed and retrieve relevant pages as images, and generate answers with VLMs that can accept an image as input. In this paper, we introduce SimpleDoc, a lightweight yet powerful retrieval - augmented framework for DocVQA. It boosts evidence page gathering by first retrieving candidates through embedding similarity and then filtering and re-ranking these candidates based on page summaries. A single VLM-based reasoner agent repeatedly invokes this dual-cue retriever, iteratively pulling fresh pages into a working memory until the question is confidently answered. SimpleDoc outperforms previous baselines by 3.2% on average on 4 DocVQA datasets with much fewer pages retrieved. Our code is available at https://github.com/ag2ai/SimpleDoc."}
{"id": "2506.14012", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14012", "abs": "https://arxiv.org/abs/2506.14012", "authors": ["Amr Mohamed", "Yang Zhang", "Michalis Vazirgiannis", "Guokan Shang"], "title": "Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text", "comment": null, "summary": "Code-switching (CSW) is the act of alternating between two or more languages within a single discourse. This phenomenon is widespread in multilingual communities, and increasingly prevalent in online content, where users naturally mix languages in everyday communication. As a result, Large Language Models (LLMs), now central to content processing and generation, are frequently exposed to code-switched inputs. Given their widespread use, it is crucial to understand how LLMs process and reason about such mixed-language text. This paper presents a systematic evaluation of LLM comprehension under code-switching by generating CSW variants of established reasoning and comprehension benchmarks. While degradation is evident when foreign tokens disrupt English text$\\unicode{x2013}$even under linguistic constraints$\\unicode{x2013}$embedding English into other languages often improves comprehension. Though prompting yields mixed results, fine-tuning offers a more stable path to degradation mitigation."}
{"id": "2506.14096", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14096", "abs": "https://arxiv.org/abs/2506.14096", "authors": ["Sanjeda Akter", "Ibne Farabi Shihab", "Anuj Sharma"], "title": "Image Segmentation with Large Language Models: A Survey with Perspectives for Intelligent Transportation Systems", "comment": null, "summary": "The integration of Large Language Models (LLMs) with computer vision is profoundly transforming perception tasks like image segmentation. For intelligent transportation systems (ITS), where accurate scene understanding is critical for safety and efficiency, this new paradigm offers unprecedented capabilities. This survey systematically reviews the emerging field of LLM-augmented image segmentation, focusing on its applications, challenges, and future directions within ITS. We provide a taxonomy of current approaches based on their prompting mechanisms and core architectures, and we highlight how these innovations can enhance road scene understanding for autonomous driving, traffic monitoring, and infrastructure maintenance. Finally, we identify key challenges, including real-time performance and safety-critical reliability, and outline a perspective centered on explainable, human-centric AI as a prerequisite for the successful deployment of this technology in next-generation transportation systems."}
{"id": "2506.14028", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14028", "abs": "https://arxiv.org/abs/2506.14028", "authors": ["Xueqing Peng", "Lingfei Qian", "Yan Wang", "Ruoyu Xiang", "Yueru He", "Yang Ren", "Mingyang Jiang", "Jeff Zhao", "Huan He", "Yi Han", "Yun Feng", "Yuechen Jiang", "Yupeng Cao", "Haohang Li", "Yangyang Yu", "Xiaoyu Wang", "Penglei Gao", "Shengyuan Lin", "Keyi Wang", "Shanshan Yang", "Yilun Zhao", "Zhiwei Liu", "Peng Lu", "Jerry Huang", "Suyuchen Wang", "Triantafillos Papadopoulos", "Polydoros Giannouris", "Efstathia Soufleri", "Nuo Chen", "Guojun Xiong", "Zhiyang Deng", "Yijia Zhao", "Mingquan Lin", "Meikang Qiu", "Kaleb E Smith", "Arman Cohan", "Xiao-Yang Liu", "Jimin Huang", "Alejandro Lopez-Lira", "Xi Chen", "Junichi Tsujii", "Jian-Yun Nie", "Sophia Ananiadou", "Qianqian Xie"], "title": "MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark for Financial LLM Evaluation", "comment": null, "summary": "Recent advances in large language models (LLMs) have accelerated progress in financial NLP and applications, yet existing benchmarks remain limited to monolingual and unimodal settings, often over-relying on simple tasks and failing to reflect the complexity of real-world financial communication. We introduce MultiFinBen, the first multilingual and multimodal benchmark tailored to the global financial domain, evaluating LLMs across modalities (text, vision, audio) and linguistic settings (monolingual, bilingual, multilingual) on domain-specific tasks. We introduce two novel tasks, including PolyFiQA-Easy and PolyFiQA-Expert, the first multilingual financial benchmarks requiring models to perform complex reasoning over mixed-language inputs; and EnglishOCR and SpanishOCR, the first OCR-embedded financial QA tasks challenging models to extract and reason over information from visual-text financial documents. Moreover, we propose a dynamic, difficulty-aware selection mechanism and curate a compact, balanced benchmark rather than simple aggregation existing datasets. Extensive evaluation of 22 state-of-the-art models reveals that even the strongest models, despite their general multimodal and multilingual capabilities, struggle dramatically when faced with complex cross-lingual and multimodal tasks in financial domain. MultiFinBen is publicly released to foster transparent, reproducible, and inclusive progress in financial studies and applications."}
{"id": "2506.14121", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14121", "abs": "https://arxiv.org/abs/2506.14121", "authors": ["Siyu Xu", "Wenjie Li", "Guangwei Gao", "Jian Yang", "Guo-Jun Qi", "Chia-Wen Lin"], "title": "FADPNet: Frequency-Aware Dual-Path Network for Face Super-Resolution", "comment": "12 pages, 11 figures, 6 tales", "summary": "Face super-resolution (FSR) under limited computational costs remains an open problem. Existing approaches typically treat all facial pixels equally, resulting in suboptimal allocation of computational resources and degraded FSR performance. CNN is relatively sensitive to high-frequency facial features, such as component contours and facial outlines. Meanwhile, Mamba excels at capturing low-frequency features like facial color and fine-grained texture, and does so with lower complexity than Transformers. Motivated by these observations, we propose FADPNet, a Frequency-Aware Dual-Path Network that decomposes facial features into low- and high-frequency components and processes them via dedicated branches. For low-frequency regions, we introduce a Mamba-based Low-Frequency Enhancement Block (LFEB), which combines state-space attention with squeeze-and-excitation operations to extract low-frequency global interactions and emphasize informative channels. For high-frequency regions, we design a CNN-based Deep Position-Aware Attention (DPA) module to enhance spatially-dependent structural details, complemented by a lightweight High-Frequency Refinement (HFR) module that further refines frequency-specific representations. Through the above designs, our method achieves an excellent balance between FSR quality and model efficiency, outperforming existing approaches."}
{"id": "2506.14040", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.14040", "abs": "https://arxiv.org/abs/2506.14040", "authors": ["Md Nazmus Sakib"], "title": "An Interdisciplinary Review of Commonsense Reasoning and Intent Detection", "comment": null, "summary": "This review explores recent advances in commonsense reasoning and intent detection, two key challenges in natural language understanding. We analyze 28 papers from ACL, EMNLP, and CHI (2020-2025), organizing them by methodology and application. Commonsense reasoning is reviewed across zero-shot learning, cultural adaptation, structured evaluation, and interactive contexts. Intent detection is examined through open-set models, generative formulations, clustering, and human-centered systems. By bridging insights from NLP and HCI, we highlight emerging trends toward more adaptive, multilingual, and context-aware models, and identify key gaps in grounding, generalization, and benchmark design."}
{"id": "2506.14130", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.14130", "abs": "https://arxiv.org/abs/2506.14130", "authors": ["Chunyu Cao", "Jintao Cheng", "Zeyu Chen", "Linfan Zhan", "Rui Fan", "Zhijian He", "Xiaoyu Tang"], "title": "KDMOS:Knowledge Distillation for Motion Segmentation", "comment": null, "summary": "Motion Object Segmentation (MOS) is crucial for autonomous driving, as it enhances localization, path planning, map construction, scene flow estimation, and future state prediction. While existing methods achieve strong performance, balancing accuracy and real-time inference remains a challenge. To address this, we propose a logits-based knowledge distillation framework for MOS, aiming to improve accuracy while maintaining real-time efficiency. Specifically, we adopt a Bird's Eye View (BEV) projection-based model as the student and a non-projection model as the teacher. To handle the severe imbalance between moving and non-moving classes, we decouple them and apply tailored distillation strategies, allowing the teacher model to better learn key motion-related features. This approach significantly reduces false positives and false negatives. Additionally, we introduce dynamic upsampling, optimize the network architecture, and achieve a 7.69% reduction in parameter count, mitigating overfitting. Our method achieves a notable IoU of 78.8% on the hidden test set of the SemanticKITTI-MOS dataset and delivers competitive results on the Apollo dataset. The KDMOS implementation is available at https://github.com/SCNU-RISLAB/KDMOS."}
{"id": "2506.14046", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14046", "abs": "https://arxiv.org/abs/2506.14046", "authors": ["David Kogan", "Max Schumacher", "Sam Nguyen", "Masanori Suzuki", "Melissa Smith", "Chloe Sophia Bellows", "Jared Bernstein"], "title": "Ace-CEFR -- A Dataset for Automated Evaluation of the Linguistic Difficulty of Conversational Texts for LLM Applications", "comment": null, "summary": "There is an unmet need to evaluate the language difficulty of short, conversational passages of text, particularly for training and filtering Large Language Models (LLMs). We introduce Ace-CEFR, a dataset of English conversational text passages expert-annotated with their corresponding level of text difficulty. We experiment with several models on Ace-CEFR, including Transformer-based models and LLMs. We show that models trained on Ace-CEFR can measure text difficulty more accurately than human experts and have latency appropriate to production environments. Finally, we release the Ace-CEFR dataset to the public for research and development."}
{"id": "2506.14136", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14136", "abs": "https://arxiv.org/abs/2506.14136", "authors": ["Nafiz Sadman", "Farhana Zulkernine", "Benjamin Kwan"], "title": "Interpreting Biomedical VLMs on High-Imbalance Out-of-Distributions: An Insight into BiomedCLIP on Radiology", "comment": "GitHub: https://github.com/Nafiz95/BioVLM_Eval_CXR", "summary": "In this paper, we construct two research objectives: i) explore the learned embedding space of BiomedCLIP, an open-source large vision language model, to analyse meaningful class separations, and ii) quantify the limitations of BiomedCLIP when applied to a highly imbalanced, out-of-distribution multi-label medical dataset. We experiment on IU-xray dataset, which exhibits the aforementioned criteria, and evaluate BiomedCLIP in classifying images (radiographs) in three contexts: zero-shot inference, full finetuning, and linear probing. The results show that the model under zero-shot settings over-predicts all labels, leading to poor precision and inter-class separability. Full fine-tuning improves classification of distinct diseases, while linear probing detects overlapping features. We demonstrate visual understanding of the model using Grad-CAM heatmaps and compare with 15 annotations by a radiologist. We highlight the need for careful adaptations of the models to foster reliability and applicability in a real-world setting. The code for the experiments in this work is available and maintained on GitHub."}
{"id": "2506.14064", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14064", "abs": "https://arxiv.org/abs/2506.14064", "authors": ["Iona Carslaw", "Sivan Milton", "Nicolas Navarre", "Ciyang Qing", "Wataru Uegaki"], "title": "Automatic Extraction of Clausal Embedding Based on Large-Scale English Text Data", "comment": "Accepted in the Society for Computation in Linguistics", "summary": "For linguists, embedded clauses have been of special interest because of their intricate distribution of syntactic and semantic features. Yet, current research relies on schematically created language examples to investigate these constructions, missing out on statistical information and naturally-occurring examples that can be gained from large language corpora. Thus, we present a methodological approach for detecting and annotating naturally-occurring examples of English embedded clauses in large-scale text data using constituency parsing and a set of parsing heuristics. Our tool has been evaluated on our dataset Golden Embedded Clause Set (GECS), which includes hand-annotated examples of naturally-occurring English embedded clause sentences. Finally, we present a large-scale dataset of naturally-occurring English embedded clauses which we have extracted from the open-source corpus Dolma using our extraction tool."}
{"id": "2506.14142", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14142", "abs": "https://arxiv.org/abs/2506.14142", "authors": ["Wenting Chen", "Yi Dong", "Zhaojun Ding", "Yucheng Shi", "Yifan Zhou", "Fang Zeng", "Yijun Luo", "Tianyu Lin", "Yihang Su", "Yichen Wu", "Kai Zhang", "Zhen Xiang", "Tianming Liu", "Ninghao Liu", "Lichao Sun", "Yixuan Yuan", "Xiang Li"], "title": "RadFabric: Agentic AI System with Reasoning Capability for Radiology", "comment": "4 figures, 2 tables", "summary": "Chest X ray (CXR) imaging remains a critical diagnostic tool for thoracic conditions, but current automated systems face limitations in pathology coverage, diagnostic accuracy, and integration of visual and textual reasoning. To address these gaps, we propose RadFabric, a multi agent, multimodal reasoning framework that unifies visual and textual analysis for comprehensive CXR interpretation. RadFabric is built on the Model Context Protocol (MCP), enabling modularity, interoperability, and scalability for seamless integration of new diagnostic agents. The system employs specialized CXR agents for pathology detection, an Anatomical Interpretation Agent to map visual findings to precise anatomical structures, and a Reasoning Agent powered by large multimodal reasoning models to synthesize visual, anatomical, and clinical data into transparent and evidence based diagnoses. RadFabric achieves significant performance improvements, with near-perfect detection of challenging pathologies like fractures (1.000 accuracy) and superior overall diagnostic accuracy (0.799) compared to traditional systems (0.229 to 0.527). By integrating cross modal feature alignment and preference-driven reasoning, RadFabric advances AI-driven radiology toward transparent, anatomically precise, and clinically actionable CXR analysis."}
{"id": "2506.14101", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14101", "abs": "https://arxiv.org/abs/2506.14101", "authors": ["Paul Landes", "Sitara Rao", "Aaron Jeremy Chaise", "Barbara Di Eugenio"], "title": "Abstract Meaning Representation for Hospital Discharge Summarization", "comment": null, "summary": "The Achilles heel of Large Language Models (LLMs) is hallucination, which has drastic consequences for the clinical domain. This is particularly important with regards to automatically generating discharge summaries (a lengthy medical document that summarizes a hospital in-patient visit). Automatically generating these summaries would free physicians to care for patients and reduce documentation burden. The goal of this work is to discover new methods that combine language-based graphs and deep learning models to address provenance of content and trustworthiness in automatic summarization. Our method shows impressive reliability results on the publicly available Medical Information Mart for Intensive III (MIMIC-III) corpus and clinical notes written by physicians at Anonymous Hospital. rovide our method, generated discharge ary output examples, source code and trained models."}
{"id": "2506.14144", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14144", "abs": "https://arxiv.org/abs/2506.14144", "authors": ["Juho Bai", "Inwook Shim"], "title": "SceneAware: Scene-Constrained Pedestrian Trajectory Prediction with LLM-Guided Walkability", "comment": null, "summary": "Accurate prediction of pedestrian trajectories is essential for applications in robotics and surveillance systems. While existing approaches primarily focus on social interactions between pedestrians, they often overlook the rich environmental context that significantly shapes human movement patterns. In this paper, we propose SceneAware, a novel framework that explicitly incorporates scene understanding to enhance trajectory prediction accuracy. Our method leverages a Vision Transformer~(ViT) scene encoder to process environmental context from static scene images, while Multi-modal Large Language Models~(MLLMs) generate binary walkability masks that distinguish between accessible and restricted areas during training. We combine a Transformer-based trajectory encoder with the ViT-based scene encoder, capturing both temporal dynamics and spatial constraints. The framework integrates collision penalty mechanisms that discourage predicted trajectories from violating physical boundaries, ensuring physically plausible predictions. SceneAware is implemented in both deterministic and stochastic variants. Comprehensive experiments on the ETH/UCY benchmark datasets show that our approach outperforms state-of-the-art methods, with more than 50\\% improvement over previous models. Our analysis based on different trajectory categories shows that the model performs consistently well across various types of pedestrian movement. This highlights the importance of using explicit scene information and shows that our scene-aware approach is both effective and reliable in generating accurate and physically plausible predictions. Code is available at: https://github.com/juho127/SceneAware."}
{"id": "2506.14111", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.14111", "abs": "https://arxiv.org/abs/2506.14111", "authors": ["Essential AI", ":", "Andrew Hojel", "Michael Pust", "Tim Romanski", "Yash Vanjani", "Ritvik Kapila", "Mohit Parmar", "Adarsh Chaluvaraju", "Alok Tripathy", "Anil Thomas", "Ashish Tanwer", "Darsh J Shah", "Ishaan Shah", "Karl Stratos", "Khoi Nguyen", "Kurt Smith", "Michael Callahan", "Peter Rushton", "Philip Monk", "Platon Mazarakis", "Saad Jamal", "Saurabh Srivastava", "Somanshu Singla", "Ashish Vaswani"], "title": "Essential-Web v1.0: 24T tokens of organized web data", "comment": null, "summary": "Data plays the most prominent role in how language models acquire skills and knowledge. The lack of massive, well-organized pre-training datasets results in costly and inaccessible data pipelines. We present Essential-Web v1.0, a 24-trillion-token dataset in which every document is annotated with a twelve-category taxonomy covering topic, format, content complexity, and quality. Taxonomy labels are produced by EAI-Distill-0.5b, a fine-tuned 0.5b-parameter model that achieves an annotator agreement within 3% of Qwen2.5-32B-Instruct. With nothing more than SQL-style filters, we obtain competitive web-curated datasets in math (-8.0% relative to SOTA), web code (+14.3%), STEM (+24.5%) and medical (+8.6%). Essential-Web v1.0 is available on HuggingFace: https://huggingface.co/datasets/EssentialAI/essential-web-v1.0"}
{"id": "2506.14168", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14168", "abs": "https://arxiv.org/abs/2506.14168", "authors": ["Hu Yu", "Biao Gong", "Hangjie Yuan", "DanDan Zheng", "Weilong Chai", "Jingdong Chen", "Kecheng Zheng", "Feng Zhao"], "title": "VideoMAR: Autoregressive Video Generatio with Continuous Tokens", "comment": "Submitted to NeurIPS 2025", "summary": "Masked-based autoregressive models have demonstrated promising image generation capability in continuous space. However, their potential for video generation remains under-explored. In this paper, we propose \\textbf{VideoMAR}, a concise and efficient decoder-only autoregressive image-to-video model with continuous tokens, composing temporal frame-by-frame and spatial masked generation. We first identify temporal causality and spatial bi-directionality as the first principle of video AR models, and propose the next-frame diffusion loss for the integration of mask and video generation. Besides, the huge cost and difficulty of long sequence autoregressive modeling is a basic but crucial issue. To this end, we propose the temporal short-to-long curriculum learning and spatial progressive resolution training, and employ progressive temperature strategy at inference time to mitigate the accumulation error. Furthermore, VideoMAR replicates several unique capacities of language models to video generation. It inherently bears high efficiency due to simultaneous temporal-wise KV cache and spatial-wise parallel generation, and presents the capacity of spatial and temporal extrapolation via 3D rotary embeddings. On the VBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos I2V) while requiring significantly fewer parameters ($9.3\\%$), training data ($0.5\\%$), and GPU resources ($0.2\\%$)."}
{"id": "2506.14123", "categories": ["cs.CL", "cs.FL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.14123", "abs": "https://arxiv.org/abs/2506.14123", "authors": ["Jonathan Hayase", "Alisa Liu", "Noah A. Smith", "Sewoong Oh"], "title": "Sampling from Your Language Model One Byte at a Time", "comment": "23 pages, 8 figures", "summary": "Tokenization is used almost universally by modern language models, enabling efficient text representation using multi-byte or multi-character tokens. However, prior work has shown that tokenization can introduce distortion into the model's generations. For example, users are often advised not to end their prompts with a space because it prevents the model from including the space as part of the next token. This Prompt Boundary Problem (PBP) also arises in languages such as Chinese and in code generation, where tokens often do not line up with syntactic boundaries. Additionally mismatching tokenizers often hinder model composition and interoperability. For example, it is not possible to directly ensemble models with different tokenizers due to their mismatching vocabularies. To address these issues, we present an inference-time method to convert any autoregressive LM with a BPE tokenizer into a character-level or byte-level LM, without changing its generative distribution at the text level. Our method efficient solves the PBP and is also able to unify the vocabularies of language models with different tokenizers, allowing one to ensemble LMs with different tokenizers at inference time as well as transfer the post-training from one model to another using proxy-tuning. We demonstrate in experiments that the ensemble and proxy-tuned models outperform their constituents on downstream evals."}
{"id": "2506.14170", "categories": ["cs.CV", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2506.14170", "abs": "https://arxiv.org/abs/2506.14170", "authors": ["Shulong Zhang", "Mingyuan Yao", "Jiayin Zhao", "Xiao Liu", "Haihua Wang"], "title": "A multi-stage augmented multimodal interaction network for fish feeding intensity quantification", "comment": null, "summary": "In recirculating aquaculture systems, accurate and effective assessment of fish feeding intensity is crucial for reducing feed costs and calculating optimal feeding times. However, current studies have limitations in modality selection, feature extraction and fusion, and co-inference for decision making, which restrict further improvement in the accuracy, applicability and reliability of multimodal fusion models. To address this problem, this study proposes a Multi-stage Augmented Multimodal Interaction Network (MAINet) for quantifying fish feeding intensity. Firstly, a general feature extraction framework is proposed to efficiently extract feature information from input image, audio and water wave datas. Second, an Auxiliary-modality Reinforcement Primary-modality Mechanism (ARPM) is designed for inter-modal interaction and generate enhanced features, which consists of a Channel Attention Fusion Network (CAFN) and a Dual-mode Attention Fusion Network (DAFN). Finally, an Evidence Reasoning (ER) rule is introduced to fuse the output results of each modality and make decisions, thereby completing the quantification of fish feeding intensity. The experimental results show that the constructed MAINet reaches 96.76%, 96.78%, 96.79% and 96.79% in accuracy, precision, recall and F1-Score respectively, and its performance is significantly higher than the comparison models. Compared with models that adopt single-modality, dual-modality fusion and different decision-making fusion methods, it also has obvious advantages. Meanwhile, the ablation experiments further verified the key role of the proposed improvement strategy in improving the robustness and feature utilization efficiency of model, which can effectively improve the accuracy of the quantitative results of fish feeding intensity."}
{"id": "2506.14157", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14157", "abs": "https://arxiv.org/abs/2506.14157", "authors": ["Chengyu Huang", "Tanya Goyal"], "title": "DCRM: A Heuristic to Measure Response Pair Quality in Preference Optimization", "comment": null, "summary": "Recent research has attempted to associate preference optimization (PO) performance with the underlying preference datasets. In this work, our observation is that the differences between the preferred response $y^+$ and dispreferred response $y^-$ influence what LLMs can learn, which may not match the desirable differences to learn. Therefore, we use distance and reward margin to quantify these differences, and combine them to get Distance Calibrated Reward Margin (DCRM), a metric that measures the quality of a response pair for PO. Intuitively, DCRM encourages minimal noisy differences and maximal desired differences. With this, we study 3 types of commonly used preference datasets, classified along two axes: the source of the responses and the preference labeling function. We establish a general correlation between higher DCRM of the training set and better learning outcome. Inspired by this, we propose a best-of-$N^2$ pairing method that selects response pairs with the highest DCRM. Empirically, in various settings, our method produces training datasets that can further improve models' performance on AlpacaEval, MT-Bench, and Arena-Hard over the existing training sets."}
{"id": "2506.14176", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14176", "abs": "https://arxiv.org/abs/2506.14176", "authors": ["Renao Yan"], "title": "One-Shot Neural Architecture Search with Network Similarity Directed Initialization for Pathological Image Classification", "comment": null, "summary": "Deep learning-based pathological image analysis presents unique challenges due to the practical constraints of network design. Most existing methods apply computer vision models directly to medical tasks, neglecting the distinct characteristics of pathological images. This mismatch often leads to computational inefficiencies, particularly in edge-computing scenarios. To address this, we propose a novel Network Similarity Directed Initialization (NSDI) strategy to improve the stability of neural architecture search (NAS). Furthermore, we introduce domain adaptation into one-shot NAS to better handle variations in staining and semantic scale across pathology datasets. Experiments on the BRACS dataset demonstrate that our method outperforms existing approaches, delivering both superior classification performance and clinically relevant feature localization."}
{"id": "2506.14158", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14158", "abs": "https://arxiv.org/abs/2506.14158", "authors": ["Tao He", "Guang Huang", "Yu Yang", "Tianshi Xu", "Sicheng Zhao", "Guiguang Ding", "Pengyang Wang", "Feng Tian"], "title": "S$^4$C: Speculative Sampling with Syntactic and Semantic Coherence for Efficient Inference of Large Language Models", "comment": null, "summary": "Large language models (LLMs) exhibit remarkable reasoning capabilities across diverse downstream tasks. However, their autoregressive nature leads to substantial inference latency, posing challenges for real-time applications. Speculative sampling mitigates this issue by introducing a drafting phase followed by a parallel validation phase, enabling faster token generation and verification. Existing approaches, however, overlook the inherent coherence in text generation, limiting their efficiency. To address this gap, we propose a Speculative Sampling with Syntactic and Semantic Coherence (S$^4$C) framework, which extends speculative sampling by leveraging multi-head drafting for rapid token generation and a continuous verification tree for efficient candidate validation and feature reuse. Experimental results demonstrate that S$^4$C surpasses baseline methods across mainstream tasks, offering enhanced efficiency, parallelism, and the ability to generate more valid tokens with fewer computational resources. On Spec-bench benchmarks, S$^4$C achieves an acceleration ratio of 2.26x-2.60x, outperforming state-of-the-art methods."}
{"id": "2506.14181", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14181", "abs": "https://arxiv.org/abs/2506.14181", "authors": ["Yufei Li", "Jirui Wu", "Long Tian", "Liming Wang", "Xiaonan Liu", "Zijun Liu", "Xiyang Liu"], "title": "Meta-SurDiff: Classification Diffusion Model Optimized by Meta Learning is Reliable for Online Surgical Phase Recognition", "comment": "15 pages, 5 figures", "summary": "Online surgical phase recognition has drawn great attention most recently due to its potential downstream applications closely related to human life and health. Despite deep models have made significant advances in capturing the discriminative long-term dependency of surgical videos to achieve improved recognition, they rarely account for exploring and modeling the uncertainty in surgical videos, which should be crucial for reliable online surgical phase recognition. We categorize the sources of uncertainty into two types, frame ambiguity in videos and unbalanced distribution among surgical phases, which are inevitable in surgical videos. To address this pivot issue, we introduce a meta-learning-optimized classification diffusion model (Meta-SurDiff), to take full advantage of the deep generative model and meta-learning in achieving precise frame-level distribution estimation for reliable online surgical phase recognition. For coarse recognition caused by ambiguous video frames, we employ a classification diffusion model to assess the confidence of recognition results at a finer-grained frame-level instance. For coarse recognition caused by unbalanced phase distribution, we use a meta-learning based objective to learn the diffusion model, thus enhancing the robustness of classification boundaries for different surgical phases.We establish effectiveness of Meta-SurDiff in online surgical phase recognition through extensive experiments on five widely used datasets using more than four practical metrics. The datasets include Cholec80, AutoLaparo, M2Cai16, OphNet, and NurViD, where OphNet comes from ophthalmic surgeries, NurViD is the daily care dataset, while the others come from laparoscopic surgeries. We will release the code upon acceptance."}
{"id": "2506.14161", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14161", "abs": "https://arxiv.org/abs/2506.14161", "authors": ["Yanlin Li", "Hao Liu", "Huimin Liu", "Yinwei Wei", "Yupeng Hu"], "title": "MIST: Towards Multi-dimensional Implicit Bias and Stereotype Evaluation of LLMs via Theory of Mind", "comment": null, "summary": "Theory of Mind (ToM) in Large Language Models (LLMs) refers to their capacity for reasoning about mental states, yet failures in this capacity often manifest as systematic implicit bias. Evaluating this bias is challenging, as conventional direct-query methods are susceptible to social desirability effects and fail to capture its subtle, multi-dimensional nature. To this end, we propose an evaluation framework that leverages the Stereotype Content Model (SCM) to reconceptualize bias as a multi-dimensional failure in ToM across Competence, Sociability, and Morality. The framework introduces two indirect tasks: the Word Association Bias Test (WABT) to assess implicit lexical associations and the Affective Attribution Test (AAT) to measure covert affective leanings, both designed to probe latent stereotypes without triggering model avoidance. Extensive experiments on 8 State-of-the-Art LLMs demonstrate our framework's capacity to reveal complex bias structures, including pervasive sociability bias, multi-dimensional divergence, and asymmetric stereotype amplification, thereby providing a more robust methodology for identifying the structural nature of implicit bias."}
{"id": "2506.14189", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14189", "abs": "https://arxiv.org/abs/2506.14189", "authors": ["Kunyuan Deng", "Yi Wang", "Lap-Pui Chau"], "title": "Egocentric Human-Object Interaction Detection: A New Benchmark and Method", "comment": null, "summary": "Understanding the interaction between humans and objects has gained much attention in recent years. Existing human-object interaction (HOI) detection methods mainly focus on the third-person perspectives, overlooking a more intuitive way from the egocentric view of HOI, namely Ego-HOI. This paper introduces an Ego-HOIBench, a new dataset to promote the benchmarking and development of Ego-HOI detection. Our Ego-HOIBench comprises more than 27K egocentric images with high-quality hand-verb-object triplet annotations across 123 fine-grained interaction categories and locations, covering a rich diversity of scenarios, object types, and hand configurations in daily activities. In addition, we explore and adapt third-person HOI detection methods to Ego-HOIBench and illustrate the challenges of hand-occluded objects and the complexity of single- and two-hand interactions. To build a new baseline, we propose a Hand Geometry and Interactivity Refinement (HGIR) scheme, which leverages hand pose and geometric information as valuable cues for interpreting interactions. Specifically, the HGIR scheme explicitly extracts global hand geometric features from the estimated hand pose proposals and refines the interaction-specific features using pose-interaction attention. This scheme enables the model to obtain a robust and powerful interaction representation, significantly improving the Ego-HOI detection capability. Our approach is lightweight and effective, and it can be easily applied to HOI baselines in a plug-and-play manner to achieve state-of-the-art results on Ego-HOIBench. Our project is available at: https://dengkunyuan.github.io/EgoHOIBench/"}
{"id": "2506.14175", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14175", "abs": "https://arxiv.org/abs/2506.14175", "authors": ["Chenglong Wang", "Yang Gan", "Yifu Huo", "Yongyu Mu", "Qiaozhi He", "Murun Yang", "Bei Li", "Tong Xiao", "Chunliang Zhang", "Tongran Liu", "Jingbo Zhu"], "title": "GRAM: A Generative Foundation Reward Model for Reward Generalization", "comment": "Accepted by ICML 2025", "summary": "In aligning large language models (LLMs), reward models have played an important role, but are standardly trained as discriminative models and rely only on labeled human preference data. In this paper, we explore methods that train reward models using both unlabeled and labeled data. Building on the generative models in LLMs, we develop a generative reward model that is first trained via large-scale unsupervised learning and then fine-tuned via supervised learning. We also show that by using label smoothing, we are in fact optimizing a regularized pairwise ranking loss. This result, in turn, provides a new view of training reward models, which links generative models and discriminative models under the same class of training objectives. The outcome of these techniques is a foundation reward model, which can be applied to a wide range of tasks with little or no further fine-tuning effort. Extensive experiments show that this model generalizes well across several tasks, including response ranking, reinforcement learning from human feedback, and task adaptation with fine-tuning, achieving significant performance improvements over several strong baseline models."}
{"id": "2506.14229", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14229", "abs": "https://arxiv.org/abs/2506.14229", "authors": ["Changbai Li", "Haodong Zhu", "Hanlin Chen", "Juan Zhang", "Tongfei Chen", "Shuo Yang", "Shuwei Shao", "Wenhao Dong", "Baochang Zhang"], "title": "HRGS: Hierarchical Gaussian Splatting for Memory-Efficient High-Resolution 3D Reconstruction", "comment": null, "summary": "3D Gaussian Splatting (3DGS) has made significant strides in real-time 3D scene reconstruction, but faces memory scalability issues in high-resolution scenarios. To address this, we propose Hierarchical Gaussian Splatting (HRGS), a memory-efficient framework with hierarchical block-level optimization. First, we generate a global, coarse Gaussian representation from low-resolution data. Then, we partition the scene into multiple blocks, refining each block with high-resolution data. The partitioning involves two steps: Gaussian partitioning, where irregular scenes are normalized into a bounded cubic space with a uniform grid for task distribution, and training data partitioning, where only relevant observations are retained for each block. By guiding block refinement with the coarse Gaussian prior, we ensure seamless Gaussian fusion across adjacent blocks. To reduce computational demands, we introduce Importance-Driven Gaussian Pruning (IDGP), which computes importance scores for each Gaussian and removes those with minimal contribution, speeding up convergence and reducing memory usage. Additionally, we incorporate normal priors from a pretrained model to enhance surface reconstruction quality. Our method enables high-quality, high-resolution 3D scene reconstruction even under memory constraints. Extensive experiments on three benchmarks show that HRGS achieves state-of-the-art performance in high-resolution novel view synthesis (NVS) and surface reconstruction tasks."}
{"id": "2506.14177", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.14177", "abs": "https://arxiv.org/abs/2506.14177", "authors": ["Tuan Nguyen", "Huy-Dat Tran"], "title": "Can we train ASR systems on Code-switch without real code-switch data? Case study for Singapore's languages", "comment": "Accepted by Interspeech 2025", "summary": "Code-switching (CS), common in multilingual settings, presents challenges for ASR due to scarce and costly transcribed data caused by linguistic complexity. This study investigates building CS-ASR using synthetic CS data. We propose a phrase-level mixing method to generate synthetic CS data that mimics natural patterns. Utilizing monolingual augmented with synthetic phrase-mixed CS data to fine-tune large pretrained ASR models (Whisper, MMS, SeamlessM4T). This paper focuses on three under-resourced Southeast Asian language pairs: Malay-English (BM-EN), Mandarin-Malay (ZH-BM), and Tamil-English (TA-EN), establishing a new comprehensive benchmark for CS-ASR to evaluate the performance of leading ASR models. Experimental results show that the proposed training strategy enhances ASR performance on monolingual and CS tests, with BM-EN showing highest gains, then TA-EN and ZH-BM. This finding offers a cost-effective approach for CS-ASR development, benefiting research and industry."}
{"id": "2506.14238", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14238", "abs": "https://arxiv.org/abs/2506.14238", "authors": ["Yinuo Zheng", "Lipeng Gu", "Honghua Chen", "Liangliang Nan", "Mingqiang Wei"], "title": "Unified Representation Space for 3D Visual Grounding", "comment": null, "summary": "3D visual grounding (3DVG) is a critical task in scene understanding that aims to identify objects in 3D scenes based on text descriptions. However, existing methods rely on separately pre-trained vision and text encoders, resulting in a significant gap between the two modalities in terms of spatial geometry and semantic categories. This discrepancy often causes errors in object positioning and classification. The paper proposes UniSpace-3D, which innovatively introduces a unified representation space for 3DVG, effectively bridging the gap between visual and textual features. Specifically, UniSpace-3D incorporates three innovative designs: i) a unified representation encoder that leverages the pre-trained CLIP model to map visual and textual features into a unified representation space, effectively bridging the gap between the two modalities; ii) a multi-modal contrastive learning module that further reduces the modality gap; iii) a language-guided query selection module that utilizes the positional and semantic information to identify object candidate points aligned with textual descriptions. Extensive experiments demonstrate that UniSpace-3D outperforms baseline models by at least 2.24% on the ScanRefer and Nr3D/Sr3D datasets. The code will be made available upon acceptance of the paper."}
{"id": "2506.14190", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.14190", "abs": "https://arxiv.org/abs/2506.14190", "authors": ["Tuan Nguyen", "Huy-Dat Tran"], "title": "AsyncSwitch: Asynchronous Text-Speech Adaptation for Code-Switched ASR", "comment": "This work has been submitted to the IEEE for possible publication. This paper is a preprint version submitted to the 2025 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU 2025)", "summary": "Developing code-switched ASR systems is challenging due to language ambiguity and limited exposure to multilingual, code-switched data, while collecting such speech is costly. Prior work generates synthetic audio from text, but these methods are computationally intensive and hard to scale. We introduce AsyncSwitch, a novel asynchronous adaptation framework that leverages large-scale, text-rich web data to pre-expose ASR models to diverse code-switched domains before fine-tuning on paired speech-text corpora. Our three-stage process (1) trains decoder self-attention and feedforward layers on code-switched text, (2) aligns decoder and encoder via cross-attention using limited speech-text data, and (3) fully fine-tunes the entire model. Experiments with Whisper on Malay-English code-switching demonstrate a 9.02% relative WER reduction, while improving monolingual performance in Singlish, Malay, and other English variants."}
{"id": "2506.14243", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14243", "abs": "https://arxiv.org/abs/2506.14243", "authors": ["Xiaohui Jiang", "Haijiang Zhu", "Chadei Li", "Fulin Tang", "Ning An"], "title": "Cross-Modal Geometric Hierarchy Fusion: An Implicit-Submap Driven Framework for Resilient 3D Place Recognition", "comment": null, "summary": "LiDAR-based place recognition serves as a crucial enabler for long-term autonomy in robotics and autonomous driving systems. Yet, prevailing methodologies relying on handcrafted feature extraction face dual challenges: (1) Inconsistent point cloud density, induced by ego-motion dynamics and environmental disturbances during repeated traversals, leads to descriptor instability, and (2) Representation fragility stems from reliance on single-level geometric abstractions that lack discriminative power in structurally complex scenarios. To address these limitations, we propose a novel framework that redefines 3D place recognition through density-agnostic geometric reasoning. Specifically, we introduce an implicit 3D representation based on elastic points, which is immune to the interference of original scene point cloud density and achieves the characteristic of uniform distribution. Subsequently, we derive the occupancy grid and normal vector information of the scene from this implicit representation. Finally, with the aid of these two types of information, we obtain descriptors that fuse geometric information from both bird's-eye view (capturing macro-level spatial layouts) and 3D segment (encoding micro-scale surface geometries) perspectives. We conducted extensive experiments on numerous datasets (KITTI, KITTI-360, MulRan, NCLT) across diverse environments. The experimental results demonstrate that our method achieves state-of-the-art performance. Moreover, our approach strikes an optimal balance between accuracy, runtime, and memory optimization for historical maps, showcasing excellent Resilient and scalability. Our code will be open-sourced in the future."}
{"id": "2506.14199", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14199", "abs": "https://arxiv.org/abs/2506.14199", "authors": ["Junghwan Kim", "Kieun Park", "Sohee Park", "Hyunggug Kim", "Bongwon Suh"], "title": "MAS-LitEval : Multi-Agent System for Literary Translation Quality Assessment", "comment": "4 Pages, 2 tables, EMNLP submitted", "summary": "Literary translation requires preserving cultural nuances and stylistic elements, which traditional metrics like BLEU and METEOR fail to assess due to their focus on lexical overlap. This oversight neglects the narrative consistency and stylistic fidelity that are crucial for literary works. To address this, we propose MAS-LitEval, a multi-agent system using Large Language Models (LLMs) to evaluate translations based on terminology, narrative, and style. We tested MAS-LitEval on translations of The Little Prince and A Connecticut Yankee in King Arthur's Court, generated by various LLMs, and compared it to traditional metrics. \\textbf{MAS-LitEval} outperformed these metrics, with top models scoring up to 0.890 in capturing literary nuances. This work introduces a scalable, nuanced framework for Translation Quality Assessment (TQA), offering a practical tool for translators and researchers."}
{"id": "2506.14255", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14255", "abs": "https://arxiv.org/abs/2506.14255", "authors": ["Johannes Flotzinger", "Fabian Deuser", "Achref Jaziri", "Heiko Neumann", "Norbert Oswald", "Visvanathan Ramesh", "Thomas Braml"], "title": "synth-dacl: Does Synthetic Defect Data Enhance Segmentation Accuracy and Robustness for Real-World Bridge Inspections?", "comment": null, "summary": "Adequate bridge inspection is increasingly challenging in many countries due to growing ailing stocks, compounded with a lack of staff and financial resources. Automating the key task of visual bridge inspection, classification of defects and building components on pixel level, improves efficiency, increases accuracy and enhances safety in the inspection process and resulting building assessment. Models overtaking this task must cope with an assortment of real-world conditions. They must be robust to variations in image quality, as well as background texture, as defects often appear on surfaces of diverse texture and degree of weathering. dacl10k is the largest and most diverse dataset for real-world concrete bridge inspections. However, the dataset exhibits class imbalance, which leads to notably poor model performance particularly when segmenting fine-grained classes such as cracks and cavities. This work introduces \"synth-dacl\", a compilation of three novel dataset extensions based on synthetic concrete textures. These extensions are designed to balance class distribution in dacl10k and enhance model performance, especially for crack and cavity segmentation. When incorporating the synth-dacl extensions, we observe substantial improvements in model robustness across 15 perturbed test sets. Notably, on the perturbed test set, a model trained on dacl10k combined with all synthetic extensions achieves a 2% increase in mean IoU, F1 score, Recall, and Precision compared to the same model trained solely on dacl10k."}
{"id": "2506.14200", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.14200", "abs": "https://arxiv.org/abs/2506.14200", "authors": ["Brihi Joshi", "Keyu He", "Sahana Ramnath", "Sadra Sabouri", "Kaitlyn Zhou", "Souti Chattopadhyay", "Swabha Swayamdipta", "Xiang Ren"], "title": "ELI-Why: Evaluating the Pedagogical Utility of Language Model Explanations", "comment": "Findings of ACL 2025", "summary": "Language models today are widely used in education, yet their ability to tailor responses for learners with varied informational needs and knowledge backgrounds remains under-explored. To this end, we introduce ELI-Why, a benchmark of 13.4K \"Why\" questions to evaluate the pedagogical capabilities of language models. We then conduct two extensive human studies to assess the utility of language model-generated explanatory answers (explanations) on our benchmark, tailored to three distinct educational grades: elementary, high-school and graduate school. In our first study, human raters assume the role of an \"educator\" to assess model explanations' fit to different educational grades. We find that GPT-4-generated explanations match their intended educational background only 50% of the time, compared to 79% for lay human-curated explanations. In our second study, human raters assume the role of a learner to assess if an explanation fits their own informational needs. Across all educational backgrounds, users deemed GPT-4-generated explanations 20% less suited on average to their informational needs, when compared to explanations curated by lay people. Additionally, automated evaluation metrics reveal that explanations generated across different language model families for different informational needs remain indistinguishable in their grade-level, limiting their pedagogical effectiveness."}
{"id": "2506.14256", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14256", "abs": "https://arxiv.org/abs/2506.14256", "authors": ["Deepak Ghimire", "Joonwhoan Lee"], "title": "Comparison of Two Methods for Stationary Incident Detection Based on Background Image", "comment": "8 pages, 6 figures", "summary": "In general, background subtraction-based methods are used to detect moving objects in visual tracking applications. In this paper, we employed a background subtraction-based scheme to detect the temporarily stationary objects. We proposed two schemes for stationary object detection, and we compare those in terms of detection performance and computational complexity. In the first approach, we used a single background, and in the second approach, we used dual backgrounds, generated with different learning rates, in order to detect temporarily stopped objects. Finally, we used normalized cross correlation (NCC) based image comparison to monitor and track the detected stationary object in a video scene. The proposed method is robust with partial occlusion, short-time fully occlusion, and illumination changes, and it can operate in real time."}
{"id": "2506.14203", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14203", "abs": "https://arxiv.org/abs/2506.14203", "authors": ["Jongho Kim", "Romain Storaï", "Seung-won Hwang"], "title": "Intended Target Identification for Anomia Patients with Gradient-based Selective Augmentation", "comment": "EMNLP 2024 Findings (long)", "summary": "In this study, we investigate the potential of language models (LMs) in aiding patients experiencing anomia, a difficulty identifying the names of items. Identifying the intended target item from patient's circumlocution involves the two challenges of term failure and error: (1) The terms relevant to identifying the item remain unseen. (2) What makes the challenge unique is inherent perturbed terms by semantic paraphasia, which are not exactly related to the target item, hindering the identification process. To address each, we propose robustifying the model from semantically paraphasic errors and enhancing the model with unseen terms with gradient-based selective augmentation. Specifically, the gradient value controls augmented data quality amid semantic errors, while the gradient variance guides the inclusion of unseen but relevant terms. Due to limited domain-specific datasets, we evaluate the model on the Tip-of-the-Tongue dataset as an intermediary task and then apply our findings to real patient data from AphasiaBank. Our results demonstrate strong performance against baselines, aiding anomia patients by addressing the outlined challenges."}
{"id": "2506.14265", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14265", "abs": "https://arxiv.org/abs/2506.14265", "authors": ["Siran Dai", "Qianqian Xu", "Peisong Wen", "Yang Liu", "Qingming Huang"], "title": "Exploring Non-contrastive Self-supervised Representation Learning for Image-based Profiling", "comment": "CVPR 2025 Computer Vision for Drug Discovery", "summary": "Image-based cell profiling aims to create informative representations of cell images. This technique is critical in drug discovery and has greatly advanced with recent improvements in computer vision. Inspired by recent developments in non-contrastive Self-Supervised Learning (SSL), this paper provides an initial exploration into training a generalizable feature extractor for cell images using such methods. However, there are two major challenges: 1) There is a large difference between the distributions of cell images and natural images, causing the view-generation process in existing SSL methods to fail; and 2) Unlike typical scenarios where each representation is based on a single image, cell profiling often involves multiple input images, making it difficult to effectively combine all available information. To overcome these challenges, we propose SSLProfiler, a non-contrastive SSL framework specifically designed for cell profiling. We introduce specialized data augmentation and representation post-processing methods tailored to cell images, which effectively address the issues mentioned above and result in a robust feature extractor. With these improvements, SSLProfiler won the Cell Line Transferability challenge at CVPR 2025."}
{"id": "2506.14205", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14205", "abs": "https://arxiv.org/abs/2506.14205", "authors": ["Jingxu Xie", "Dylan Xu", "Xuandong Zhao", "Dawn Song"], "title": "AgentSynth: Scalable Task Generation for Generalist Computer-Use Agents", "comment": null, "summary": "We introduce AgentSynth, a scalable and cost-efficient pipeline for automatically synthesizing high-quality tasks and trajectory datasets for generalist computer-use agents. Leveraging information asymmetry, AgentSynth constructs subtasks that are simple during generation but significantly more challenging when composed into long-horizon tasks, enabling the creation of over 6,000 diverse and realistic tasks. Our pipeline begins with an LLM-based task proposer guided by a persona, followed by an execution agent that completes the task and logs the trajectory. This process is repeated iteratively to form a sequence of subtasks, which are then summarized by a separate agent into a composite task of controllable difficulty. A key strength of AgentSynth is its ability to precisely modulate task complexity by varying the number of subtasks. Empirical evaluations show that state-of-the-art LLM agents suffer a steep performance drop, from 18% success at difficulty level 1 to just 4% at level 6, highlighting the benchmark's difficulty and discriminative power. Moreover, our pipeline achieves a low average cost of \\$0.60 per trajectory, orders of magnitude cheaper than human annotations. Our code and data are publicly available at https://github.com/sunblaze-ucb/AgentSynth"}
{"id": "2506.14271", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14271", "abs": "https://arxiv.org/abs/2506.14271", "authors": ["Weiming Zhang", "Dingwen Xiao", "Aobotao Dai", "Yexin Liu", "Tianbo Pan", "Shiqi Wen", "Lei Chen", "Lin Wang"], "title": "Leader360V: The Large-scale, Real-world 360 Video Dataset for Multi-task Learning in Diverse Environment", "comment": "23 pages, 16 figures", "summary": "360 video captures the complete surrounding scenes with the ultra-large field of view of 360X180. This makes 360 scene understanding tasks, eg, segmentation and tracking, crucial for appications, such as autonomous driving, robotics. With the recent emergence of foundation models, the community is, however, impeded by the lack of large-scale, labelled real-world datasets. This is caused by the inherent spherical properties, eg, severe distortion in polar regions, and content discontinuities, rendering the annotation costly yet complex. This paper introduces Leader360V, the first large-scale, labeled real-world 360 video datasets for instance segmentation and tracking. Our datasets enjoy high scene diversity, ranging from indoor and urban settings to natural and dynamic outdoor scenes. To automate annotation, we design an automatic labeling pipeline, which subtly coordinates pre-trained 2D segmentors and large language models to facilitate the labeling. The pipeline operates in three novel stages. Specifically, in the Initial Annotation Phase, we introduce a Semantic- and Distortion-aware Refinement module, which combines object mask proposals from multiple 2D segmentors with LLM-verified semantic labels. These are then converted into mask prompts to guide SAM2 in generating distortion-aware masks for subsequent frames. In the Auto-Refine Annotation Phase, missing or incomplete regions are corrected either by applying the SDR again or resolving the discontinuities near the horizontal borders. The Manual Revision Phase finally incorporates LLMs and human annotators to further refine and validate the annotations. Extensive user studies and evaluations demonstrate the effectiveness of our labeling pipeline. Meanwhile, experiments confirm that Leader360V significantly enhances model performance for 360 video segmentation and tracking, paving the way for more scalable 360 scene understanding."}
{"id": "2506.14206", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14206", "abs": "https://arxiv.org/abs/2506.14206", "authors": ["Jia-Chen Zhang", "Zheng Zhou", "Yu-Jie Xiong", "Chun-Ming Xia", "Fei Dai"], "title": "CausalDiffTab: Mixed-Type Causal-Aware Diffusion for Tabular Data Generation", "comment": null, "summary": "Training data has been proven to be one of the most critical components in training generative AI. However, obtaining high-quality data remains challenging, with data privacy issues presenting a significant hurdle. To address the need for high-quality data. Synthesize data has emerged as a mainstream solution, demonstrating impressive performance in areas such as images, audio, and video. Generating mixed-type data, especially high-quality tabular data, still faces significant challenges. These primarily include its inherent heterogeneous data types, complex inter-variable relationships, and intricate column-wise distributions. In this paper, we introduce CausalDiffTab, a diffusion model-based generative model specifically designed to handle mixed tabular data containing both numerical and categorical features, while being more flexible in capturing complex interactions among variables. We further propose a hybrid adaptive causal regularization method based on the principle of Hierarchical Prior Fusion. This approach adaptively controls the weight of causal regularization, enhancing the model's performance without compromising its generative capabilities. Comprehensive experiments conducted on seven datasets demonstrate that CausalDiffTab outperforms baseline methods across all metrics. Our code is publicly available at: https://github.com/Godz-z/CausalDiffTab."}
{"id": "2506.14322", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.14322", "abs": "https://arxiv.org/abs/2506.14322", "authors": ["Avigail Cohen Rimon", "Mirela Ben-Chen", "Or Litany"], "title": "FRIDU: Functional Map Refinement with Guided Image Diffusion", "comment": "Accepted to SGP 2025 (Symposium on Geometry Processing)", "summary": "We propose a novel approach for refining a given correspondence map between two shapes. A correspondence map represented as a functional map, namely a change of basis matrix, can be additionally treated as a 2D image. With this perspective, we train an image diffusion model directly in the space of functional maps, enabling it to generate accurate maps conditioned on an inaccurate initial map. The training is done purely in the functional space, and thus is highly efficient. At inference time, we use the pointwise map corresponding to the current functional map as guidance during the diffusion process. The guidance can additionally encourage different functional map objectives, such as orthogonality and commutativity with the Laplace-Beltrami operator. We show that our approach is competitive with state-of-the-art methods of map refinement and that guided diffusion models provide a promising pathway to functional map processing."}
{"id": "2506.14211", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14211", "abs": "https://arxiv.org/abs/2506.14211", "authors": ["Sina Abdidizaji", "Md Kowsher", "Niloofar Yousefi", "Ivan Garibay"], "title": "Explainable Detection of Implicit Influential Patterns in Conversations via Data Augmentation", "comment": "Accepted at the HCI International conference 2025", "summary": "In the era of digitalization, as individuals increasingly rely on digital platforms for communication and news consumption, various actors employ linguistic strategies to influence public perception. While models have become proficient at detecting explicit patterns, which typically appear in texts as single remarks referred to as utterances, such as social media posts, malicious actors have shifted toward utilizing implicit influential verbal patterns embedded within conversations. These verbal patterns aim to mentally penetrate the victim's mind in order to influence them, enabling the actor to obtain the desired information through implicit means. This paper presents an improved approach for detecting such implicit influential patterns. Furthermore, the proposed model is capable of identifying the specific locations of these influential elements within a conversation. To achieve this, the existing dataset was augmented using the reasoning capabilities of state-of-the-art language models. Our designed framework resulted in a 6% improvement in the detection of implicit influential patterns in conversations. Moreover, this approach improved the multi-label classification tasks related to both the techniques used for influence and the vulnerability of victims by 33% and 43%, respectively."}
{"id": "2506.14350", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.14350", "abs": "https://arxiv.org/abs/2506.14350", "authors": ["Zoubida Ameur", "Frédéric Lefebvre", "Philippe De Lagrange", "Miloš Radosavljević"], "title": "FGA-NN: Film Grain Analysis Neural Network", "comment": null, "summary": "Film grain, once a by-product of analog film, is now present in most cinematographic content for aesthetic reasons. However, when such content is compressed at medium to low bitrates, film grain is lost due to its random nature. To preserve artistic intent while compressing efficiently, film grain is analyzed and modeled before encoding and synthesized after decoding. This paper introduces FGA-NN, the first learning-based film grain analysis method to estimate conventional film grain parameters compatible with conventional synthesis. Quantitative and qualitative results demonstrate FGA-NN's superior balance between analysis accuracy and synthesis complexity, along with its robustness and applicability."}
{"id": "2506.14213", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14213", "abs": "https://arxiv.org/abs/2506.14213", "authors": ["Jongho Kim", "Dohyeon Lee", "Minsoo Kim", "Seung-won Hwang"], "title": "Chaining Event Spans for Temporal Relation Grounding", "comment": "In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1689-1700", "summary": "Accurately understanding temporal relations between events is a critical building block of diverse tasks, such as temporal reading comprehension (TRC) and relation extraction (TRE). For example in TRC, we need to understand the temporal semantic differences between the following two questions that are lexically near-identical: \"What finished right before the decision?\" or \"What finished right after the decision?\". To discern the two questions, existing solutions have relied on answer overlaps as a proxy label to contrast similar and dissimilar questions. However, we claim that answer overlap can lead to unreliable results, due to spurious overlaps of two dissimilar questions with coincidentally identical answers. To address the issue, we propose a novel approach that elicits proper reasoning behaviors through a module for predicting time spans of events. We introduce the Timeline Reasoning Network (TRN) operating in a two-step inductive reasoning process: In the first step model initially answers each question with semantic and syntactic information. The next step chains multiple questions on the same event to predict a timeline, which is then used to ground the answers. Results on the TORQUE and TB-dense, TRC and TRE tasks respectively, demonstrate that TRN outperforms previous methods by effectively resolving the spurious overlaps using the predicted timeline."}
{"id": "2506.14356", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14356", "abs": "https://arxiv.org/abs/2506.14356", "authors": ["Xiaoqi Wang", "Yi Wang", "Lap-Pui Chau"], "title": "EVA02-AT: Egocentric Video-Language Understanding with Spatial-Temporal Rotary Positional Embeddings and Symmetric Optimization", "comment": null, "summary": "Egocentric video-language understanding demands both high efficiency and accurate spatial-temporal modeling. Existing approaches face three key challenges: 1) Excessive pre-training cost arising from multi-stage pre-training pipelines, 2) Ineffective spatial-temporal encoding due to manually split 3D rotary positional embeddings that hinder feature interactions, and 3) Imprecise learning objectives in soft-label multi-instance retrieval, which neglect negative pair correlations. In this paper, we introduce EVA02-AT, a suite of EVA02-based video-language foundation models tailored to egocentric video understanding tasks. EVA02-AT first efficiently transfers an image-based CLIP model into a unified video encoder via a single-stage pretraining. Second, instead of applying rotary positional embeddings to isolated dimensions, we introduce spatial-temporal rotary positional embeddings along with joint attention, which can effectively encode both spatial and temporal information on the entire hidden dimension. This joint encoding of spatial-temporal features enables the model to learn cross-axis relationships, which are crucial for accurately modeling motion and interaction in videos. Third, focusing on multi-instance video-language retrieval tasks, we introduce the Symmetric Multi-Similarity (SMS) loss and a novel training framework that advances all soft labels for both positive and negative pairs, providing a more precise learning objective. Extensive experiments on Ego4D, EPIC-Kitchens-100, and Charades-Ego under zero-shot and fine-tuning settings demonstrate that EVA02-AT achieves state-of-the-art performance across diverse egocentric video-language tasks with fewer parameters. Models with our SMS loss also show significant performance gains on multi-instance retrieval benchmarks. Our code and models are publicly available at https://github.com/xqwang14/EVA02-AT ."}
{"id": "2506.14234", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14234", "abs": "https://arxiv.org/abs/2506.14234", "authors": ["Md Tanzib Hosain", "Salman Rahman", "Md Kishor Morol", "Md Rizwan Parvez"], "title": "Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just Like an Olympiad Team", "comment": null, "summary": "Despite impressive progress on complex reasoning, current large language models (LLMs) typically operate in isolation - treating each problem as an independent attempt, without accumulating or integrating experiential knowledge. In contrast, expert problem solvers - such as Olympiad or programming contest teams - leverage a rich tapestry of experiences: absorbing mentorship from coaches, developing intuition from past problems, leveraging knowledge of tool usage and library functionality, adapting strategies based on the expertise and experiences of peers, continuously refining their reasoning through trial and error, and learning from other related problems even during competition. We introduce Xolver, a training-free multi-agent reasoning framework that equips a black-box LLM with a persistent, evolving memory of holistic experience. Xolver integrates diverse experience modalities, including external and self-retrieval, tool use, collaborative interactions, agent-driven evaluation, and iterative refinement. By learning from relevant strategies, code fragments, and abstract reasoning patterns at inference time, Xolver avoids generating solutions from scratch - marking a transition from isolated inference toward experience-aware language agents. Built on both open-weight and proprietary models, Xolver consistently outperforms specialized reasoning agents. Even with lightweight backbones (e.g., QWQ-32B), it often surpasses advanced models including Qwen3-235B, Gemini 2.5 Pro, o3, and o4-mini-high. With o3-mini-high, it achieves new best results on GSM8K (98.1%), AIME'24 (94.4%), AIME'25 (93.7%), Math-500 (99.8%), and LiveCodeBench-V5 (91.6%) - highlighting holistic experience learning as a key step toward generalist agents capable of expert-level reasoning. Code and data are available at https://kagnlp.github.io/xolver.github.io/."}
{"id": "2506.14362", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14362", "abs": "https://arxiv.org/abs/2506.14362", "authors": ["Daniele Rege Cambrin", "Eleonora Poeta", "Eliana Pastor", "Isaac Corley", "Tania Cerquitelli", "Elena Baralis", "Paolo Garza"], "title": "HydroChronos: Forecasting Decades of Surface Water Change", "comment": null, "summary": "Forecasting surface water dynamics is crucial for water resource management and climate change adaptation. However, the field lacks comprehensive datasets and standardized benchmarks. In this paper, we introduce HydroChronos, a large-scale, multi-modal spatiotemporal dataset for surface water dynamics forecasting designed to address this gap. We couple the dataset with three forecasting tasks. The dataset includes over three decades of aligned Landsat 5 and Sentinel-2 imagery, climate data, and Digital Elevation Models for diverse lakes and rivers across Europe, North America, and South America. We also propose AquaClimaTempo UNet, a novel spatiotemporal architecture with a dedicated climate data branch, as a strong benchmark baseline. Our model significantly outperforms a Persistence baseline for forecasting future water dynamics by +14% and +11% F1 across change detection and direction of change classification tasks, and by +0.1 MAE on the magnitude of change regression. Finally, we conduct an Explainable AI analysis to identify the key climate variables and input channels that influence surface water change, providing insights to inform and guide future modeling efforts."}
{"id": "2506.14235", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14235", "abs": "https://arxiv.org/abs/2506.14235", "authors": ["Yimin Deng", "Yuxia Wu", "Yejing Wang", "Guoshuai Zhao", "Li Zhu", "Qidong Liu", "Derong Xu", "Zichuan Fu", "Xian Wu", "Yefeng Zheng", "Xiangyu Zhao", "Xueming Qian"], "title": "A Multi-Expert Structural-Semantic Hybrid Framework for Unveiling Historical Patterns in Temporal Knowledge Graphs", "comment": "ACL25 findings", "summary": "Temporal knowledge graph reasoning aims to predict future events with knowledge of existing facts and plays a key role in various downstream tasks. Previous methods focused on either graph structure learning or semantic reasoning, failing to integrate dual reasoning perspectives to handle different prediction scenarios. Moreover, they lack the capability to capture the inherent differences between historical and non-historical events, which limits their generalization across different temporal contexts. To this end, we propose a Multi-Expert Structural-Semantic Hybrid (MESH) framework that employs three kinds of expert modules to integrate both structural and semantic information, guiding the reasoning process for different events. Extensive experiments on three datasets demonstrate the effectiveness of our approach."}
{"id": "2506.14367", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14367", "abs": "https://arxiv.org/abs/2506.14367", "authors": ["Sumshun Nahar Eity", "Mahin Montasir Afif", "Tanisha Fairooz", "Md. Mortuza Ahmmed", "Md Saef Ullah Miah"], "title": "DGG-XNet: A Hybrid Deep Learning Framework for Multi-Class Brain Disease Classification with Explainable AI", "comment": null, "summary": "Accurate diagnosis of brain disorders such as Alzheimer's disease and brain tumors remains a critical challenge in medical imaging. Conventional methods based on manual MRI analysis are often inefficient and error-prone. To address this, we propose DGG-XNet, a hybrid deep learning model integrating VGG16 and DenseNet121 to enhance feature extraction and classification. DenseNet121 promotes feature reuse and efficient gradient flow through dense connectivity, while VGG16 contributes strong hierarchical spatial representations. Their fusion enables robust multiclass classification of neurological conditions. Grad-CAM is applied to visualize salient regions, enhancing model transparency. Trained on a combined dataset from BraTS 2021 and Kaggle, DGG-XNet achieved a test accuracy of 91.33\\%, with precision, recall, and F1-score all exceeding 91\\%. These results highlight DGG-XNet's potential as an effective and interpretable tool for computer-aided diagnosis (CAD) of neurodegenerative and oncological brain disorders."}
{"id": "2506.14248", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14248", "abs": "https://arxiv.org/abs/2506.14248", "authors": ["Chenghao Li", "Liu Liu", "Baosheng Yu", "Jiayan Qiu", "Yibing Zhan"], "title": "Re-Initialization Token Learning for Tool-Augmented Large Language Models", "comment": null, "summary": "Large language models have demonstrated exceptional performance, yet struggle with complex tasks such as numerical reasoning, plan generation. Integrating external tools, such as calculators and databases, into large language models (LLMs) is crucial for enhancing problem-solving capabilities. Current methods assign a unique token to each tool, enabling LLMs to call tools through token prediction-similar to word generation. However, this approach fails to account for the relationship between tool and word tokens, limiting adaptability within pre-trained LLMs. To address this issue, we propose a novel token learning method that aligns tool tokens with the existing word embedding space from the perspective of initialization, thereby enhancing model performance. We begin by constructing prior token embeddings for each tool based on the tool's name or description, which are used to initialize and regularize the learnable tool token embeddings. This ensures the learned embeddings are well-aligned with the word token space, improving tool call accuracy. We evaluate the method on tasks such as numerical reasoning, knowledge-based question answering, and embodied plan generation using GSM8K-XL, FuncQA, KAMEL, and VirtualHome datasets. The results demonstrate clear improvements over recent baselines, including CoT, REACT, ICL, and ToolkenGPT, indicating that our approach effectively augments LLMs with tools through relevant tokens across diverse domains."}
{"id": "2506.14373", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14373", "abs": "https://arxiv.org/abs/2506.14373", "authors": ["Junyeob Baek", "Hosung Lee", "Christopher Hoang", "Mengye Ren", "Sungjin Ahn"], "title": "Discrete JEPA: Learning Discrete Token Representations without Reconstruction", "comment": null, "summary": "The cornerstone of cognitive intelligence lies in extracting hidden patterns from observations and leveraging these principles to systematically predict future outcomes. However, current image tokenization methods demonstrate significant limitations in tasks requiring symbolic abstraction and logical reasoning capabilities essential for systematic inference. To address this challenge, we propose Discrete-JEPA, extending the latent predictive coding framework with semantic tokenization and novel complementary objectives to create robust tokenization for symbolic reasoning tasks. Discrete-JEPA dramatically outperforms baselines on visual symbolic prediction tasks, while striking visual evidence reveals the spontaneous emergence of deliberate systematic patterns within the learned semantic token space. Though an initial model, our approach promises a significant impact for advancing Symbolic world modeling and planning capabilities in artificial intelligence systems."}
{"id": "2506.14285", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14285", "abs": "https://arxiv.org/abs/2506.14285", "authors": ["Seongbo Jang", "Minjin Jeon", "Jaehoon Lee", "Seonghyeon Lee", "Dongha Lee", "Hwanjo Yu"], "title": "From What to Respond to When to Respond: Timely Response Generation for Open-domain Dialogue Agents", "comment": "Work in progress", "summary": "While research on dialogue response generation has primarily focused on generating coherent responses conditioning on textual context, the critical question of when to respond grounded on the temporal context remains underexplored. To bridge this gap, we propose a novel task called timely dialogue response generation and introduce the TimelyChat benchmark, which evaluates the capabilities of language models to predict appropriate time intervals and generate time-conditioned responses. Additionally, we construct a large-scale training dataset by leveraging unlabeled event knowledge from a temporal commonsense knowledge graph and employing a large language model (LLM) to synthesize 55K event-driven dialogues. We then train Timer, a dialogue agent designed to proactively predict time intervals and generate timely responses that align with those intervals. Experimental results show that Timer outperforms prompting-based LLMs and other fine-tuned baselines in both turn-level and dialogue-level evaluations. We publicly release our data, model, and code."}
{"id": "2506.14382", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14382", "abs": "https://arxiv.org/abs/2506.14382", "authors": ["Ning Zhou", "Shanxiong Chen", "Mingting Zhou", "Haigang Sui", "Lieyun Hu", "Han Li", "Li Hua", "Qiming Zhou"], "title": "DepthSeg: Depth prompting in remote sensing semantic segmentation", "comment": null, "summary": "Remote sensing semantic segmentation is crucial for extracting detailed land surface information, enabling applications such as environmental monitoring, land use planning, and resource assessment. In recent years, advancements in artificial intelligence have spurred the development of automatic remote sensing semantic segmentation methods. However, the existing semantic segmentation methods focus on distinguishing spectral characteristics of different objects while ignoring the differences in the elevation of the different targets. This results in land cover misclassification in complex scenarios involving shadow occlusion and spectral confusion. In this paper, we introduce a depth prompting two-dimensional (2D) remote sensing semantic segmentation framework (DepthSeg). It automatically models depth/height information from 2D remote sensing images and integrates it into the semantic segmentation framework to mitigate the effects of spectral confusion and shadow occlusion. During the feature extraction phase of DepthSeg, we introduce a lightweight adapter to enable cost-effective fine-tuning of the large-parameter vision transformer encoder pre-trained by natural images. In the depth prompting phase, we propose a depth prompter to model depth/height features explicitly. In the semantic prediction phase, we introduce a semantic classification decoder that couples the depth prompts with high-dimensional land-cover features, enabling accurate extraction of land-cover types. Experiments on the LiuZhou dataset validate the advantages of the DepthSeg framework in land cover mapping tasks. Detailed ablation studies further highlight the significance of the depth prompts in remote sensing semantic segmentation."}
{"id": "2506.14302", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14302", "abs": "https://arxiv.org/abs/2506.14302", "authors": ["Xueyang Feng", "Jingsen Zhang", "Jiakai Tang", "Wei Li", "Guohao Cai", "Xu Chen", "Quanyu Dai", "Yue Zhu", "Zhenhua Dong"], "title": "Expectation Confirmation Preference Optimization for Multi-Turn Conversational Recommendation Agent", "comment": "Accepted to Findings of ACL 2025", "summary": "Recent advancements in Large Language Models (LLMs) have significantly propelled the development of Conversational Recommendation Agents (CRAs). However, these agents often generate short-sighted responses that fail to sustain user guidance and meet expectations. Although preference optimization has proven effective in aligning LLMs with user expectations, it remains costly and performs poorly in multi-turn dialogue. To address this challenge, we introduce a novel multi-turn preference optimization (MTPO) paradigm ECPO, which leverages Expectation Confirmation Theory to explicitly model the evolution of user satisfaction throughout multi-turn dialogues, uncovering the underlying causes of dissatisfaction. These causes can be utilized to support targeted optimization of unsatisfactory responses, thereby achieving turn-level preference optimization. ECPO ingeniously eliminates the significant sampling overhead of existing MTPO methods while ensuring the optimization process drives meaningful improvements. To support ECPO, we introduce an LLM-based user simulator, AILO, to simulate user feedback and perform expectation confirmation during conversational recommendations. Experimental results show that ECPO significantly enhances CRA's interaction capabilities, delivering notable improvements in both efficiency and effectiveness over existing MTPO methods."}
{"id": "2506.14384", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14384", "abs": "https://arxiv.org/abs/2506.14384", "authors": ["Huan Kang", "Hui Li", "Xiao-Jun Wu", "Tianyang Xu", "Rui Wang", "Chunyang Cheng", "Josef Kittler"], "title": "GrFormer: A Novel Transformer on Grassmann Manifold for Infrared and Visible Image Fusion", "comment": "16 pages, 11 figures", "summary": "In the field of image fusion, promising progress has been made by modeling data from different modalities as linear subspaces.\n  However, in practice, the source images are often located in a non-Euclidean space, where the Euclidean methods usually cannot\n  encapsulate the intrinsic topological structure. Typically, the inner product performed in the Euclidean space calculates the algebraic\n  similarity rather than the semantic similarity, which results in undesired attention output and a decrease in fusion performance.\n  While the balance of low-level details and high-level semantics should be considered in infrared and visible image fusion task. To\n  address this issue, in this paper, we propose a novel attention mechanism based on Grassmann manifold for infrared and visible\n  image fusion (GrFormer). Specifically, our method constructs a low-rank subspace mapping through projection constraints on the\n  Grassmann manifold, compressing attention features into subspaces of varying rank levels. This forces the features to decouple into\n  high-frequency details (local low-rank) and low-frequency semantics (global low-rank), thereby achieving multi-scale semantic\n  fusion. Additionally, to effectively integrate the significant information, we develop a cross-modal fusion strategy (CMS) based on\n  a covariance mask to maximise the complementary properties between different modalities and to suppress the features with high\n  correlation, which are deemed redundant. The experimental results demonstrate that our network outperforms SOTA methods both\n  qualitatively and quantitatively on multiple image fusion benchmarks. The codes are available at https://github.com/Shaoyun2023."}
{"id": "2506.14335", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14335", "abs": "https://arxiv.org/abs/2506.14335", "authors": ["Silvia Casola", "Yang Janet Liu", "Siyao Peng", "Oliver Kraus", "Albert Gatt", "Barbara Plank"], "title": "Evaluation Should Not Ignore Variation: On the Impact of Reference Set Choice on Summarization Metrics", "comment": "17 pages, 13 figures", "summary": "Human language production exhibits remarkable richness and variation, reflecting diverse communication styles and intents. However, this variation is often overlooked in summarization evaluation. While having multiple reference summaries is known to improve correlation with human judgments, the impact of using different reference sets on reference-based metrics has not been systematically investigated. This work examines the sensitivity of widely used reference-based metrics in relation to the choice of reference sets, analyzing three diverse multi-reference summarization datasets: SummEval, GUMSum, and DUC2004. We demonstrate that many popular metrics exhibit significant instability. This instability is particularly concerning for n-gram-based metrics like ROUGE, where model rankings vary depending on the reference sets, undermining the reliability of model comparisons. We also collect human judgments on LLM outputs for genre-diverse data and examine their correlation with metrics to supplement existing findings beyond newswire summaries, finding weak-to-no correlation. Taken together, we recommend incorporating reference set variation into summarization evaluation to enhance consistency alongside correlation with human judgments, especially when evaluating LLMs."}
{"id": "2506.14399", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14399", "abs": "https://arxiv.org/abs/2506.14399", "authors": ["Tian Xia", "Fabio De Sousa Ribeiro", "Rajat R Rasal", "Avinash Kori", "Raghav Mehta", "Ben Glocker"], "title": "Decoupled Classifier-Free Guidance for Counterfactual Diffusion Models", "comment": null, "summary": "Counterfactual image generation aims to simulate realistic visual outcomes under specific causal interventions. Diffusion models have recently emerged as a powerful tool for this task, combining DDIM inversion with conditional generation via classifier-free guidance (CFG). However, standard CFG applies a single global weight across all conditioning variables, which can lead to poor identity preservation and spurious attribute changes - a phenomenon known as attribute amplification. To address this, we propose Decoupled Classifier-Free Guidance (DCFG), a flexible and model-agnostic framework that introduces group-wise conditioning control. DCFG builds on an attribute-split embedding strategy that disentangles semantic inputs, enabling selective guidance on user-defined attribute groups. For counterfactual generation, we partition attributes into intervened and invariant sets based on a causal graph and apply distinct guidance to each. Experiments on CelebA-HQ, MIMIC-CXR, and EMBED show that DCFG improves intervention fidelity, mitigates unintended changes, and enhances reversibility, enabling more faithful and interpretable counterfactual image generation."}
{"id": "2506.14345", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.14345", "abs": "https://arxiv.org/abs/2506.14345", "authors": ["Bruno Martins", "Piotr Szymański", "Piotr Gramacki"], "title": "A Vision for Geo-Temporal Deep Research Systems: Towards Comprehensive, Transparent, and Reproducible Geo-Temporal Information Synthesis", "comment": null, "summary": "The emergence of Large Language Models (LLMs) has transformed information access, with current LLMs also powering deep research systems that can generate comprehensive report-style answers, through planned iterative search, retrieval, and reasoning. Still, current deep research systems lack the geo-temporal capabilities that are essential for answering context-rich questions involving geographic and/or temporal constraints, frequently occurring in domains like public health, environmental science, or socio-economic analysis. This paper reports our vision towards next generation systems, identifying important technical, infrastructural, and evaluative challenges in integrating geo-temporal reasoning into deep research pipelines. We argue for augmenting retrieval and synthesis processes with the ability to handle geo-temporal constraints, supported by open and reproducible infrastructures and rigorous evaluation protocols. Our vision outlines a path towards more advanced and geo-temporally aware deep research systems, of potential impact to the future of AI-driven information access."}
{"id": "2506.14404", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14404", "abs": "https://arxiv.org/abs/2506.14404", "authors": ["Nikos Spyrou", "Athanasios Vlontzos", "Paraskevas Pegios", "Thomas Melistas", "Nefeli Gkouti", "Yannis Panagakis", "Giorgos Papanastasiou", "Sotirios A. Tsaftaris"], "title": "Causally Steered Diffusion for Automated Video Counterfactual Generation", "comment": null, "summary": "Adapting text-to-image (T2I) latent diffusion models for video editing has shown strong visual fidelity and controllability, but challenges remain in maintaining causal relationships in video content. Edits affecting causally dependent attributes risk generating unrealistic or misleading outcomes if these relationships are ignored. In this work, we propose a causally faithful framework for counterfactual video generation, guided by a vision-language model (VLM). Our method is agnostic to the underlying video editing system and does not require access to its internal mechanisms or finetuning. Instead, we guide the generation by optimizing text prompts based on an assumed causal graph, addressing the challenge of latent space control in LDMs. We evaluate our approach using standard video quality metrics and counterfactual-specific criteria, such as causal effectiveness and minimality. Our results demonstrate that causally faithful video counterfactuals can be effectively generated within the learned distribution of LDMs through prompt-based causal steering. With its compatibility with any black-box video editing system, our method holds significant potential for generating realistic \"what-if\" video scenarios in diverse areas such as healthcare and digital media."}
{"id": "2506.14370", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14370", "abs": "https://arxiv.org/abs/2506.14370", "authors": ["Amrit Poudel", "Yifan Ding", "Jurgen Pfeffer", "Tim Weninger"], "title": "Digital Gatekeepers: Google's Role in Curating Hashtags and Subreddits", "comment": "Accepted to ACL 2025 Main", "summary": "Search engines play a crucial role as digital gatekeepers, shaping the visibility of Web and social media content through algorithmic curation. This study investigates how search engines like Google selectively promotes or suppresses certain hashtags and subreddits, impacting the information users encounter. By comparing search engine results with nonsampled data from Reddit and Twitter/X, we reveal systematic biases in content visibility. Google's algorithms tend to suppress subreddits and hashtags related to sexually explicit material, conspiracy theories, advertisements, and cryptocurrencies, while promoting content associated with higher engagement. These findings suggest that Google's gatekeeping practices influence public discourse by curating the social media narratives available to users."}
{"id": "2506.14418", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14418", "abs": "https://arxiv.org/abs/2506.14418", "authors": ["Jiayi Chen", "Yanbiao Ma", "Andi Zhang", "Weidong Tang", "Wei Dai", "Bowei Liu"], "title": "Compositional Attribute Imbalance in Vision Datasets", "comment": null, "summary": "Visual attribute imbalance is a common yet underexplored issue in image classification, significantly impacting model performance and generalization. In this work, we first define the first-level and second-level attributes of images and then introduce a CLIP-based framework to construct a visual attribute dictionary, enabling automatic evaluation of image attributes. By systematically analyzing both single-attribute imbalance and compositional attribute imbalance, we reveal how the rarity of attributes affects model performance. To tackle these challenges, we propose adjusting the sampling probability of samples based on the rarity of their compositional attributes. This strategy is further integrated with various data augmentation techniques (such as CutMix, Fmix, and SaliencyMix) to enhance the model's ability to represent rare attributes. Extensive experiments on benchmark datasets demonstrate that our method effectively mitigates attribute imbalance, thereby improving the robustness and fairness of deep neural networks. Our research highlights the importance of modeling visual attribute distributions and provides a scalable solution for long-tail image classification tasks."}
{"id": "2506.14371", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.14371", "abs": "https://arxiv.org/abs/2506.14371", "authors": ["Lucile Favero", "Daniel Frases", "Juan Antonio Pérez-Ortiz", "Tanja Käser", "Nuria Oliver"], "title": "ELLIS Alicante at CQs-Gen 2025: Winning the critical thinking questions shared task: LLM-based question generation and selection", "comment": "Proceedings of the 12th Workshop on Argument Mining", "summary": "The widespread adoption of chat interfaces based on Large Language Models (LLMs) raises concerns about promoting superficial learning and undermining the development of critical thinking skills. Instead of relying on LLMs purely for retrieving factual information, this work explores their potential to foster deeper reasoning by generating critical questions that challenge unsupported or vague claims in debate interventions. This study is part of a shared task of the 12th Workshop on Argument Mining, co-located with ACL 2025, focused on automatic critical question generation. We propose a two-step framework involving two small-scale open source language models: a Questioner that generates multiple candidate questions and a Judge that selects the most relevant ones. Our system ranked first in the shared task competition, demonstrating the potential of the proposed LLM-based approach to encourage critical engagement with argumentative texts."}
{"id": "2506.14428", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14428", "abs": "https://arxiv.org/abs/2506.14428", "authors": ["Ruihao Xi", "Xuekuan Wang", "Yongcheng Li", "Shuhua Li", "Zichen Wang", "Yiwei Wang", "Feng Wei", "Cairong Zhao"], "title": "Toward Rich Video Human-Motion2D Generation", "comment": null, "summary": "Generating realistic and controllable human motions, particularly those involving rich multi-character interactions, remains a significant challenge due to data scarcity and the complexities of modeling inter-personal dynamics. To address these limitations, we first introduce a new large-scale rich video human motion 2D dataset (Motion2D-Video-150K) comprising 150,000 video sequences. Motion2D-Video-150K features a balanced distribution of diverse single-character and, crucially, double-character interactive actions, each paired with detailed textual descriptions. Building upon this dataset, we propose a novel diffusion-based rich video human motion2D generation (RVHM2D) model. RVHM2D incorporates an enhanced textual conditioning mechanism utilizing either dual text encoders (CLIP-L/B) or T5-XXL with both global and local features. We devise a two-stage training strategy: the model is first trained with a standard diffusion objective, and then fine-tuned using reinforcement learning with an FID-based reward to further enhance motion realism and text alignment. Extensive experiments demonstrate that RVHM2D achieves leading performance on the Motion2D-Video-150K benchmark in generating both single and interactive double-character scenarios."}
{"id": "2506.14397", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14397", "abs": "https://arxiv.org/abs/2506.14397", "authors": ["Yeonkyoung So", "Gyuseong Lee", "Sungmok Jung", "Joonhak Lee", "JiA Kang", "Sangho Kim", "Jaejin Lee"], "title": "Thunder-NUBench: A Benchmark for LLMs' Sentence-Level Negation Understanding", "comment": null, "summary": "Negation is a fundamental linguistic phenomenon that poses persistent challenges for Large Language Models (LLMs), particularly in tasks requiring deep semantic understanding. Existing benchmarks often treat negation as a side case within broader tasks like natural language inference, resulting in a lack of benchmarks that exclusively target negation understanding. In this work, we introduce \\textbf{Thunder-NUBench}, a novel benchmark explicitly designed to assess sentence-level negation understanding in LLMs. Thunder-NUBench goes beyond surface-level cue detection by contrasting standard negation with structurally diverse alternatives such as local negation, contradiction, and paraphrase. The benchmark consists of manually curated sentence-negation pairs and a multiple-choice dataset that enables in-depth evaluation of models' negation understanding."}
{"id": "2506.14435", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.14435", "abs": "https://arxiv.org/abs/2506.14435", "authors": ["Hongyu Wang", "Jiayu Xu", "Ruiping Wang", "Yan Feng", "Yitao Zhai", "Peng Pei", "Xunliang Cai", "Xilin Chen"], "title": "MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal Models", "comment": "Work in progress", "summary": "Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size to boost performance while maintaining fixed active parameters. However, previous works primarily utilized full-precision experts during sparse up-cycling. Despite they show superior performance on end tasks, the large amount of experts introduces higher memory footprint, which poses significant challenges for the deployment on edge devices. In this work, we propose MoTE, a scalable and memory-efficient approach to train Mixture-of-Ternary-Experts models from dense checkpoint. Instead of training fewer high-precision experts, we propose to train more low-precision experts during up-cycling. Specifically, we use the pre-trained FFN as a shared expert and train ternary routed experts with parameters in {-1, 0, 1}. Extensive experiments show that our approach has promising scaling trend along model size. MoTE achieves comparable performance to full-precision baseline MoE-LLaVA while offering lower memory footprint. Furthermore, our approach is compatible with post-training quantization methods and the advantage further amplifies when memory-constraint goes lower. Given the same amount of expert memory footprint of 3.4GB and combined with post-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3% average accuracy on end tasks, demonstrating its effectiveness and potential for memory-constrained devices."}
{"id": "2506.14407", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14407", "abs": "https://arxiv.org/abs/2506.14407", "authors": ["Zeinab Sadat Taghavi", "Ali Modarressi", "Yunpu Ma", "Hinrich Schütze"], "title": "ImpliRet: Benchmarking the Implicit Fact Retrieval Challenge", "comment": null, "summary": "Retrieval systems are central to many NLP pipelines, but often rely on surface-level cues such as keyword overlap and lexical semantic similarity. To evaluate retrieval beyond these shallow signals, recent benchmarks introduce reasoning-heavy queries; however, they primarily shift the burden to query-side processing techniques -- like prompting or multi-hop retrieval -- that can help resolve complexity. In contrast, we present ImpliRet, a benchmark that shifts the reasoning challenge to document-side processing: The queries are simple, but relevance depends on facts stated implicitly in documents through temporal (e.g., resolving \"two days ago\"), arithmetic, and world knowledge relationships. We evaluate a range of sparse and dense retrievers, all of which struggle in this setting: the best nDCG@10 is only 15.07%. We also test whether long-context models can overcome this limitation. But even with a short context of only ten documents, including the positive document, GPT-4.1 scores only 35.06%, showing that document-side reasoning remains a challenge. Our codes are available at github.com/ZeinabTaghavi/IMPLIRET.Contribution."}
{"id": "2506.14440", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.14440", "abs": "https://arxiv.org/abs/2506.14440", "authors": ["David E. Hernandez", "Jose Chang", "Torbjörn E. M. Nordling"], "title": "Model compression using knowledge distillation with integrated gradients", "comment": "49 pages, 12 figures", "summary": "Model compression is critical for deploying deep learning models on resource-constrained devices. We introduce a novel method enhancing knowledge distillation with integrated gradients (IG) as a data augmentation strategy. Our approach overlays IG maps onto input images during training, providing student models with deeper insights into teacher models' decision-making processes. Extensive evaluation on CIFAR-10 demonstrates that our IG-augmented knowledge distillation achieves 92.6% testing accuracy with a 4.1x compression factor-a significant 1.1 percentage point improvement ($p<0.001$) over non-distilled models (91.5%). This compression reduces inference time from 140 ms to 13 ms. Our method precomputes IG maps before training, transforming substantial runtime costs into a one-time preprocessing step. Our comprehensive experiments include: (1) comparisons with attention transfer, revealing complementary benefits when combined with our approach; (2) Monte Carlo simulations confirming statistical robustness; (3) systematic evaluation of compression factor versus accuracy trade-offs across a wide range (2.2x-1122x); and (4) validation on an ImageNet subset aligned with CIFAR-10 classes, demonstrating generalisability beyond the initial dataset. These extensive ablation studies confirm that IG-based knowledge distillation consistently outperforms conventional approaches across varied architectures and compression ratios. Our results establish this framework as a viable compression technique for real-world deployment on edge devices while maintaining competitive accuracy."}
{"id": "2506.14429", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14429", "abs": "https://arxiv.org/abs/2506.14429", "authors": ["Xiaoran Liu", "Zhigeng Liu", "Zengfeng Huang", "Qipeng Guo", "Ziwei He", "Xipeng Qiu"], "title": "LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs", "comment": "16 pages, 12 figures, work in progress", "summary": "Large Language Diffusion Models, or diffusion LLMs, have emerged as a significant focus in NLP research, with substantial effort directed toward understanding their scalability and downstream task performance. However, their long-context capabilities remain unexplored, lacking systematic analysis or methods for context extension. In this work, we present the first systematic investigation comparing the long-context performance of diffusion LLMs and traditional auto-regressive LLMs. We first identify a unique characteristic of diffusion LLMs, unlike auto-regressive LLMs, they maintain remarkably \\textbf{\\textit{stable perplexity}} during direct context extrapolation. Furthermore, where auto-regressive models fail outright during the Needle-In-A-Haystack task with context exceeding their pretrained length, we discover diffusion LLMs exhibit a distinct \\textbf{\\textit{local perception}} phenomenon, enabling successful retrieval from recent context segments. We explain both phenomena through the lens of Rotary Position Embedding (RoPE) scaling theory. Building on these observations, we propose LongLLaDA, a training-free method that integrates LLaDA with the NTK-based RoPE extrapolation. Our results validate that established extrapolation scaling laws remain effective for extending the context windows of diffusion LLMs. Furthermore, we identify long-context tasks where diffusion LLMs outperform auto-regressive LLMs and others where they fall short. Consequently, this study establishes the first context extrapolation method for diffusion LLMs while providing essential theoretical insights and empirical benchmarks critical for advancing future research on long-context diffusion LLMs."}
{"id": "2506.14451", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14451", "abs": "https://arxiv.org/abs/2506.14451", "authors": ["Aditya Shourya", "Michel Dumontier", "Chang Sun"], "title": "Adapting Lightweight Vision Language Models for Radiological Visual Question Answering", "comment": null, "summary": "Recent advancements in vision-language systems have improved the accuracy of Radiological Visual Question Answering (VQA) Models. However, some challenges remain across each stage of model development: limited expert-labeled images hinders data procurement at scale; the intricate and nuanced patterns of radiological images make modeling inherently difficult; and the lack of evaluation evaluation efforts makes it difficult to identify cases where the model might be ill-conditioned. In this study, we fine-tune a lightweight 3B parameter vision-language model for Radiological VQA, demonstrating that small models, when appropriately tuned with curated data, can achieve robust performance across both open- and closed-ended questions. We propose a cost-effective training pipeline from synthetic question-answer pair generation to multi-stage fine-tuning on specialised radiological domain-targeted datasets (e.g., ROCO v2.0, MedPix v2.0). Our results show that despite operating at a fraction of the scale of state-of-the-art models such as LLaVA-Med, our model achieves promising performance given its small parameter size and the limited scale of training data. We introduce a lightweight saliency-based diagnostic tool that enables domain experts to inspect VQA model performance and identify ill-conditioned failure modes through saliency analysis."}
{"id": "2506.14448", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14448", "abs": "https://arxiv.org/abs/2506.14448", "authors": ["Jiayin Wang", "Zhiquang Guo", "Weizhi Ma", "Min Zhang"], "title": "How Far Can LLMs Improve from Experience? Measuring Test-Time Learning Ability in LLMs with Human Comparison", "comment": null, "summary": "As evaluation designs of large language models may shape our trajectory toward artificial general intelligence, comprehensive and forward-looking assessment is essential. Existing benchmarks primarily assess static knowledge, while intelligence also entails the ability to rapidly learn from experience. To this end, we advocate for the evaluation of Test-time Learning, the capacity to improve performance in experience-based, reasoning-intensive tasks during test time. In this work, we propose semantic games as effective testbeds for evaluating test-time learning, due to their resistance to saturation and inherent demand for strategic reasoning. We introduce an objective evaluation framework that compares model performance under both limited and cumulative experience settings, and contains four forms of experience representation. To provide a comparative baseline, we recruit eight human participants to complete the same task. Results show that LLMs exhibit measurable test-time learning capabilities; however, their improvements are less stable under cumulative experience and progress more slowly than those observed in humans. These findings underscore the potential of LLMs as general-purpose learning machines, while also revealing a substantial intellectual gap between models and humans, irrespective of how well LLMs perform on static benchmarks."}
{"id": "2506.14471", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14471", "abs": "https://arxiv.org/abs/2506.14471", "authors": ["Yikang Zhou", "Tao Zhang", "Dizhe Zhang", "Shunping Ji", "Xiangtai Li", "Lu Qi"], "title": "Dense360: Dense Understanding from Omnidirectional Panoramas", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) require comprehensive visual inputs to achieve dense understanding of the physical world. While existing MLLMs demonstrate impressive world understanding capabilities through limited field-of-view (FOV) visual inputs (e.g., 70 degree), we take the first step toward dense understanding from omnidirectional panoramas. We first introduce an omnidirectional panoramas dataset featuring a comprehensive suite of reliability-scored annotations. Specifically, our dataset contains 160K panoramas with 5M dense entity-level captions, 1M unique referring expressions, and 100K entity-grounded panoramic scene descriptions. Compared to multi-view alternatives, panoramas can provide more complete, compact, and continuous scene representations through equirectangular projections (ERP). However, the use of ERP introduces two key challenges for MLLMs: i) spatial continuity along the circle of latitude, and ii) latitude-dependent variation in information density. We address these challenges through ERP-RoPE, a position encoding scheme specifically designed for panoramic ERP. In addition, we introduce Dense360-Bench, the first benchmark for evaluating MLLMs on omnidirectional captioning and grounding, establishing a comprehensive framework for advancing dense visual-language understanding in panoramic settings."}
{"id": "2506.14474", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.14474", "abs": "https://arxiv.org/abs/2506.14474", "authors": ["Eyal German", "Sagiv Antebi", "Edan Habler", "Asaf Shabtai", "Yuval Elovici"], "title": "LexiMark: Robust Watermarking via Lexical Substitutions to Enhance Membership Verification of an LLM's Textual Training Data", "comment": null, "summary": "Large language models (LLMs) can be trained or fine-tuned on data obtained without the owner's consent. Verifying whether a specific LLM was trained on particular data instances or an entire dataset is extremely challenging. Dataset watermarking addresses this by embedding identifiable modifications in training data to detect unauthorized use. However, existing methods often lack stealth, making them relatively easy to detect and remove. In light of these limitations, we propose LexiMark, a novel watermarking technique designed for text and documents, which embeds synonym substitutions for carefully selected high-entropy words. Our method aims to enhance an LLM's memorization capabilities on the watermarked text without altering the semantic integrity of the text. As a result, the watermark is difficult to detect, blending seamlessly into the text with no visible markers, and is resistant to removal due to its subtle, contextually appropriate substitutions that evade automated and manual detection. We evaluated our method using baseline datasets from recent studies and seven open-source models: LLaMA-1 7B, LLaMA-3 8B, Mistral 7B, Pythia 6.9B, as well as three smaller variants from the Pythia family (160M, 410M, and 1B). Our evaluation spans multiple training settings, including continued pretraining and fine-tuning scenarios. The results demonstrate significant improvements in AUROC scores compared to existing methods, underscoring our method's effectiveness in reliably verifying whether unauthorized watermarked data was used in LLM training."}
{"id": "2506.14473", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.14473", "abs": "https://arxiv.org/abs/2506.14473", "authors": ["Zhijing Wan", "Zhixiang Wang", "Zheng Wang", "Xin Xu", "Shin'ichi Satoh"], "title": "Foundation Model Insights and a Multi-Model Approach for Superior Fine-Grained One-shot Subset Selection", "comment": "18 pages, 10 figures, accepted by ICML 2025", "summary": "One-shot subset selection serves as an effective tool to reduce deep learning training costs by identifying an informative data subset based on the information extracted by an information extractor (IE). Traditional IEs, typically pre-trained on the target dataset, are inherently dataset-dependent. Foundation models (FMs) offer a promising alternative, potentially mitigating this limitation. This work investigates two key questions: (1) Can FM-based subset selection outperform traditional IE-based methods across diverse datasets? (2) Do all FMs perform equally well as IEs for subset selection? Extensive experiments uncovered surprising insights: FMs consistently outperform traditional IEs on fine-grained datasets, whereas their advantage diminishes on coarse-grained datasets with noisy labels. Motivated by these finding, we propose RAM-APL (RAnking Mean-Accuracy of Pseudo-class Labels), a method tailored for fine-grained image datasets. RAM-APL leverages multiple FMs to enhance subset selection by exploiting their complementary strengths. Our approach achieves state-of-the-art performance on fine-grained datasets, including Oxford-IIIT Pet, Food-101, and Caltech-UCSD Birds-200-2011."}
{"id": "2506.14493", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.14493", "abs": "https://arxiv.org/abs/2506.14493", "authors": ["Jiyuan Fu", "Kaixun Jiang", "Lingyi Hong", "Jinglun Li", "Haijing Guo", "Dingkang Yang", "Zhaoyu Chen", "Wenqiang Zhang"], "title": "LingoLoop Attack: Trapping MLLMs via Linguistic Context and State Entrapment into Endless Loops", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have shown great promise but require substantial computational resources during inference. Attackers can exploit this by inducing excessive output, leading to resource exhaustion and service degradation. Prior energy-latency attacks aim to increase generation time by broadly shifting the output token distribution away from the EOS token, but they neglect the influence of token-level Part-of-Speech (POS) characteristics on EOS and sentence-level structural patterns on output counts, limiting their efficacy. To address this, we propose LingoLoop, an attack designed to induce MLLMs to generate excessively verbose and repetitive sequences. First, we find that the POS tag of a token strongly affects the likelihood of generating an EOS token. Based on this insight, we propose a POS-Aware Delay Mechanism to postpone EOS token generation by adjusting attention weights guided by POS information. Second, we identify that constraining output diversity to induce repetitive loops is effective for sustained generation. We introduce a Generative Path Pruning Mechanism that limits the magnitude of hidden states, encouraging the model to produce persistent loops. Extensive experiments demonstrate LingoLoop can increase generated tokens by up to 30 times and energy consumption by a comparable factor on models like Qwen2.5-VL-3B, consistently driving MLLMs towards their maximum generation limits. These findings expose significant MLLMs' vulnerabilities, posing challenges for their reliable deployment. The code will be released publicly following the paper's acceptance."}
{"id": "2506.14495", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14495", "abs": "https://arxiv.org/abs/2506.14495", "authors": ["Yu Qi", "Lipeng Gu", "Honghua Chen", "Liangliang Nan", "Mingqiang Wei"], "title": "I Speak and You Find: Robust 3D Visual Grounding with Noisy and Ambiguous Speech Inputs", "comment": null, "summary": "Existing 3D visual grounding methods rely on precise text prompts to locate objects within 3D scenes. Speech, as a natural and intuitive modality, offers a promising alternative. Real-world speech inputs, however, often suffer from transcription errors due to accents, background noise, and varying speech rates, limiting the applicability of existing 3DVG methods. To address these challenges, we propose \\textbf{SpeechRefer}, a novel 3DVG framework designed to enhance performance in the presence of noisy and ambiguous speech-to-text transcriptions. SpeechRefer integrates seamlessly with xisting 3DVG models and introduces two key innovations. First, the Speech Complementary Module captures acoustic similarities between phonetically related words and highlights subtle distinctions, generating complementary proposal scores from the speech signal. This reduces dependence on potentially erroneous transcriptions. Second, the Contrastive Complementary Module employs contrastive learning to align erroneous text features with corresponding speech features, ensuring robust performance even when transcription errors dominate. Extensive experiments on the SpeechRefer and peechNr3D datasets demonstrate that SpeechRefer improves the performance of existing 3DVG methods by a large margin, which highlights SpeechRefer's potential to bridge the gap between noisy speech inputs and reliable 3DVG, enabling more intuitive and practical multimodal systems."}
{"id": "2506.14532", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14532", "abs": "https://arxiv.org/abs/2506.14532", "authors": ["Can Zheng", "Jiguang He", "Chung G. Kang", "Guofa Cai", "Zitong Yu", "Merouane Debbah"], "title": "M2BeamLLM: Multimodal Sensing-empowered mmWave Beam Prediction with Large Language Models", "comment": "13 pages, 20 figures", "summary": "This paper introduces a novel neural network framework called M2BeamLLM for beam prediction in millimeter-wave (mmWave) massive multi-input multi-output (mMIMO) communication systems. M2BeamLLM integrates multi-modal sensor data, including images, radar, LiDAR, and GPS, leveraging the powerful reasoning capabilities of large language models (LLMs) such as GPT-2 for beam prediction. By combining sensing data encoding, multimodal alignment and fusion, and supervised fine-tuning (SFT), M2BeamLLM achieves significantly higher beam prediction accuracy and robustness, demonstrably outperforming traditional deep learning (DL) models in both standard and few-shot scenarios. Furthermore, its prediction performance consistently improves with increased diversity in sensing modalities. Our study provides an efficient and intelligent beam prediction solution for vehicle-to-infrastructure (V2I) mmWave communication systems."}
{"id": "2506.14511", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14511", "abs": "https://arxiv.org/abs/2506.14511", "authors": ["Zhiwen Shao", "Yifan Cheng", "Feiran Li", "Yong Zhou", "Xuequan Lu", "Yuan Xie", "Lizhuang Ma"], "title": "MOL: Joint Estimation of Micro-Expression, Optical Flow, and Landmark via Transformer-Graph-Style Convolution", "comment": "This paper has been accepted by IEEE Transactions on Pattern Analysis and Machine Intelligence", "summary": "Facial micro-expression recognition (MER) is a challenging problem, due to transient and subtle micro-expression (ME) actions. Most existing methods depend on hand-crafted features, key frames like onset, apex, and offset frames, or deep networks limited by small-scale and low-diversity datasets. In this paper, we propose an end-to-end micro-action-aware deep learning framework with advantages from transformer, graph convolution, and vanilla convolution. In particular, we propose a novel F5C block composed of fully-connected convolution and channel correspondence convolution to directly extract local-global features from a sequence of raw frames, without the prior knowledge of key frames. The transformer-style fully-connected convolution is proposed to extract local features while maintaining global receptive fields, and the graph-style channel correspondence convolution is introduced to model the correlations among feature patterns. Moreover, MER, optical flow estimation, and facial landmark detection are jointly trained by sharing the local-global features. The two latter tasks contribute to capturing facial subtle action information for MER, which can alleviate the impact of insufficient training data. Extensive experiments demonstrate that our framework (i) outperforms the state-of-the-art MER methods on CASME II, SAMM, and SMIC benchmarks, (ii) works well for optical flow estimation and facial landmark detection, and (iii) can capture facial subtle muscle actions in local regions associated with MEs. The code is available at https://github.com/CYF-cuber/MOL."}
{"id": "2506.14562", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.14562", "abs": "https://arxiv.org/abs/2506.14562", "authors": ["Di He", "Ajay Jaiswal", "Songjun Tu", "Li Shen", "Ganzhao Yuan", "Shiwei Liu", "Lu Yin"], "title": "AlphaDecay:Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs", "comment": null, "summary": "Weight decay is a standard regularization technique for training large language models (LLMs). While it is common to assign a uniform decay rate to every layer, this approach overlooks the structural diversity of LLMs and the varying spectral properties across modules. In this paper, we introduce AlphaDecay, a simple yet effective method that adaptively assigns different weight decay strengths to each module of an LLM. Our approach is guided by Heavy-Tailed Self-Regularization (HT-SR) theory, which analyzes the empirical spectral density (ESD) of weight correlation matrices to quantify \"heavy-tailedness.\" Modules exhibiting more pronounced heavy-tailed ESDs, reflecting stronger feature learning, are assigned weaker decay, while modules with lighter-tailed spectra receive stronger decay. Our method leverages tailored weight decay assignments to balance the module-wise differences in spectral properties, leading to improved performance. Extensive pre-training tasks with various model sizes from 60M to 1B demonstrate that AlphaDecay achieves better perplexity and generalization than conventional uniform decay and other adaptive decay baselines."}
{"id": "2506.14512", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14512", "abs": "https://arxiv.org/abs/2506.14512", "authors": ["Zijian Song", "Xiaoxin Lin", "Qiuming Huang", "Guangrun Wang", "Liang Lin"], "title": "SIRI-Bench: Challenging VLMs' Spatial Intelligence through Complex Reasoning Tasks", "comment": "16 pages, 9 figures", "summary": "Large Language Models (LLMs) are experiencing rapid advancements in complex reasoning, exhibiting remarkable generalization in mathematics and programming. In contrast, while spatial intelligence is fundamental for Vision-Language Models (VLMs) in real-world interaction, the systematic evaluation of their complex reasoning ability within spatial contexts remains underexplored. To bridge this gap, we introduce SIRI-Bench, a benchmark designed to evaluate VLMs' spatial intelligence through video-based reasoning tasks. SIRI-Bench comprises nearly 1K video-question-answer triplets, where each problem is embedded in a realistic 3D scene and captured by video. By carefully designing questions and corresponding 3D scenes, our benchmark ensures that solving the questions requires both spatial comprehension for extracting information and high-level reasoning for deriving solutions, making it a challenging benchmark for evaluating VLMs. To facilitate large-scale data synthesis, we develop an Automatic Scene Creation Engine. This engine, leveraging multiple specialized LLM agents, can generate realistic 3D scenes from abstract math problems, ensuring faithfulness to the original descriptions. Experimental results reveal that state-of-the-art VLMs struggle significantly on SIRI-Bench, underscoring the challenge of spatial reasoning. We hope that our study will bring researchers' attention to spatially grounded reasoning and advance VLMs in visual problem-solving."}
{"id": "2506.14580", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14580", "abs": "https://arxiv.org/abs/2506.14580", "authors": ["David Wan", "Eran Hirsch", "Elias Stengel-Eskin", "Ido Dagan", "Mohit Bansal"], "title": "GenerationPrograms: Fine-grained Attribution with Executable Programs", "comment": "27 Pages. Code: https://github.com/meetdavidwan/generationprograms", "summary": "Recent large language models (LLMs) achieve impressive performance in source-conditioned text generation but often fail to correctly provide fine-grained attributions for their outputs, undermining verifiability and trust. Moreover, existing attribution methods do not explain how and why models leverage the provided source documents to generate their final responses, limiting interpretability. To overcome these challenges, we introduce a modular generation framework, GenerationPrograms, inspired by recent advancements in executable \"code agent\" architectures. Unlike conventional generation methods that simultaneously generate outputs and attributions or rely on post-hoc attribution, GenerationPrograms decomposes the process into two distinct stages: first, creating an executable program plan composed of modular text operations (such as paraphrasing, compression, and fusion) explicitly tailored to the query, and second, executing these operations following the program's specified instructions to produce the final response. Empirical evaluations demonstrate that GenerationPrograms significantly improves attribution quality at both the document level and sentence level across two long-form question-answering tasks and a multi-document summarization task. We further demonstrate that GenerationPrograms can effectively function as a post-hoc attribution method, outperforming traditional techniques in recovering accurate attributions. In addition, the interpretable programs generated by GenerationPrograms enable localized refinement through modular-level improvements that further enhance overall attribution quality."}
{"id": "2506.14525", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.14525", "abs": "https://arxiv.org/abs/2506.14525", "authors": ["Zhuoyue Tan", "Boyong He", "Yuxiang Ji", "Liaoni Wu"], "title": "VisLanding: Monocular 3D Perception for UAV Safe Landing via Depth-Normal Synergy", "comment": "Accepted by IROS2025", "summary": "This paper presents VisLanding, a monocular 3D perception-based framework for safe UAV (Unmanned Aerial Vehicle) landing. Addressing the core challenge of autonomous UAV landing in complex and unknown environments, this study innovatively leverages the depth-normal synergy prediction capabilities of the Metric3D V2 model to construct an end-to-end safe landing zones (SLZ) estimation framework. By introducing a safe zone segmentation branch, we transform the landing zone estimation task into a binary semantic segmentation problem. The model is fine-tuned and annotated using the WildUAV dataset from a UAV perspective, while a cross-domain evaluation dataset is constructed to validate the model's robustness. Experimental results demonstrate that VisLanding significantly enhances the accuracy of safe zone identification through a depth-normal joint optimization mechanism, while retaining the zero-shot generalization advantages of Metric3D V2. The proposed method exhibits superior generalization and robustness in cross-domain testing compared to other approaches. Furthermore, it enables the estimation of landing zone area by integrating predicted depth and normal information, providing critical decision-making support for practical applications."}
{"id": "2506.14606", "categories": ["cs.CL", "cs.AR", "cs.LG", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.14606", "abs": "https://arxiv.org/abs/2506.14606", "authors": ["Ahmed Heakl", "Sarim Hashmi", "Chaimaa Abi", "Celine Lee", "Abdulrahman Mahmoud"], "title": "Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC Transpilation with Testing Guarantees", "comment": "Project page: https://ahmedheakl.github.io/Guaranteed-Guess/", "summary": "The hardware ecosystem is rapidly evolving, with increasing interest in translating low-level programs across different instruction set architectures (ISAs) in a quick, flexible, and correct way to enhance the portability and longevity of existing code. A particularly challenging class of this transpilation problem is translating between complex- (CISC) and reduced- (RISC) hardware architectures, due to fundamental differences in instruction complexity, memory models, and execution paradigms. In this work, we introduce GG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the translation power of pre-trained large language models (LLMs) with the rigor of established software testing constructs. Our method generates candidate translations using an LLM from one ISA to another, and embeds such translations within a software-testing framework to build quantifiable confidence in the translation. We evaluate our GG approach over two diverse datasets, enforce high code coverage (>98%) across unit tests, and achieve functional/semantic correctness of 99% on HumanEval programs and 49% on BringupBench programs, respectively. Further, we compare our approach to the state-of-the-art Rosetta 2 framework on Apple Silicon, showcasing 1.73x faster runtime performance, 1.47x better energy efficiency, and 2.41x better memory usage for our transpiled code, demonstrating the effectiveness of GG for real-world CISC-to-RISC translation tasks. We will open-source our codes, data, models, and benchmarks to establish a common foundation for ISA-level code translation research."}
{"id": "2506.14541", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14541", "abs": "https://arxiv.org/abs/2506.14541", "authors": ["Rongchang Lu", "Tianduo Luo", "Yunzhi Zhang", "Conghan Yue", "Pei Yang", "Guibao Liu", "Changyang Gu"], "title": "Exploring Diffusion with Test-Time Training on Efficient Image Restoration", "comment": "Submitted to The 8th Chinese Conference on Pattern Recognition and Computer Vision (2025). Contact to nomodeset@qq.com. Source code will open in 4 months", "summary": "Image restoration faces challenges including ineffective feature fusion, computational bottlenecks and inefficient diffusion processes. To address these, we propose DiffRWKVIR, a novel framework unifying Test-Time Training (TTT) with efficient diffusion. Our approach introduces three key innovations: (1) Omni-Scale 2D State Evolution extends RWKV's location-dependent parameterization to hierarchical multi-directional 2D scanning, enabling global contextual awareness with linear complexity O(L); (2) Chunk-Optimized Flash Processing accelerates intra-chunk parallelism by 3.2x via contiguous chunk processing (O(LCd) complexity), reducing sequential dependencies and computational overhead; (3) Prior-Guided Efficient Diffusion extracts a compact Image Prior Representation (IPR) in only 5-20 steps, proving 45% faster training/inference than DiffIR while solving computational inefficiency in denoising. Evaluated across super-resolution and inpainting benchmarks (Set5, Set14, BSD100, Urban100, Places365), DiffRWKVIR outperforms SwinIR, HAT, and MambaIR/v2 in PSNR, SSIM, LPIPS, and efficiency metrics. Our method establishes a new paradigm for adaptive, high-efficiency image restoration with optimized hardware utilization."}
{"id": "2506.14613", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14613", "abs": "https://arxiv.org/abs/2506.14613", "authors": ["Junghyun Min", "Xiulin Yang", "Shira Wein"], "title": "When Does Meaning Backfire? Investigating the Role of AMRs in NLI", "comment": "9 pages, 2 figures", "summary": "Natural Language Inference (NLI) relies heavily on adequately parsing the semantic content of the premise and hypothesis. In this work, we investigate whether adding semantic information in the form of an Abstract Meaning Representation (AMR) helps pretrained language models better generalize in NLI. Our experiments integrating AMR into NLI in both fine-tuning and prompting settings show that the presence of AMR in fine-tuning hinders model generalization while prompting with AMR leads to slight gains in \\texttt{GPT-4o}. However, an ablation study reveals that the improvement comes from amplifying surface-level differences rather than aiding semantic reasoning. This amplification can mislead models to predict non-entailment even when the core meaning is preserved."}
{"id": "2506.14549", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14549", "abs": "https://arxiv.org/abs/2506.14549", "authors": ["Yong Liu", "Wenpeng Xiao", "Qianqian Wang", "Junlin Chen", "Shiyin Wang", "Yitong Wang", "Xinglong Wu", "Yansong Tang"], "title": "DreamLight: Towards Harmonious and Consistent Image Relighting", "comment": null, "summary": "We introduce a model named DreamLight for universal image relighting in this work, which can seamlessly composite subjects into a new background while maintaining aesthetic uniformity in terms of lighting and color tone. The background can be specified by natural images (image-based relighting) or generated from unlimited text prompts (text-based relighting). Existing studies primarily focus on image-based relighting, while with scant exploration into text-based scenarios. Some works employ intricate disentanglement pipeline designs relying on environment maps to provide relevant information, which grapples with the expensive data cost required for intrinsic decomposition and light source. Other methods take this task as an image translation problem and perform pixel-level transformation with autoencoder architecture. While these methods have achieved decent harmonization effects, they struggle to generate realistic and natural light interaction effects between the foreground and background. To alleviate these challenges, we reorganize the input data into a unified format and leverage the semantic prior provided by the pretrained diffusion model to facilitate the generation of natural results. Moreover, we propose a Position-Guided Light Adapter (PGLA) that condenses light information from different directions in the background into designed light query embeddings, and modulates the foreground with direction-biased masked attention. In addition, we present a post-processing module named Spectral Foreground Fixer (SFF) to adaptively reorganize different frequency components of subject and relighted background, which helps enhance the consistency of foreground appearance. Extensive comparisons and user study demonstrate that our DreamLight achieves remarkable relighting performance."}
{"id": "2506.14625", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14625", "abs": "https://arxiv.org/abs/2506.14625", "authors": ["Chenchen Yuan", "Zheyu Zhang", "Shuo Yang", "Bardh Prenkaj", "Gjergji Kasneci"], "title": "Probabilistic Aggregation and Targeted Embedding Optimization for Collective Moral Reasoning in Large Language Models", "comment": "18 pages", "summary": "Large Language Models (LLMs) have shown impressive moral reasoning abilities. Yet they often diverge when confronted with complex, multi-factor moral dilemmas. To address these discrepancies, we propose a framework that synthesizes multiple LLMs' moral judgments into a collectively formulated moral judgment, realigning models that deviate significantly from this consensus. Our aggregation mechanism fuses continuous moral acceptability scores (beyond binary labels) into a collective probability, weighting contributions by model reliability. For misaligned models, a targeted embedding-optimization procedure fine-tunes token embeddings for moral philosophical theories, minimizing JS divergence to the consensus while preserving semantic integrity. Experiments on a large-scale social moral dilemma dataset show our approach builds robust consensus and improves individual model fidelity. These findings highlight the value of data-driven moral alignment across multiple models and its potential for safer, more consistent AI systems."}
{"id": "2506.14560", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.14560", "abs": "https://arxiv.org/abs/2506.14560", "authors": ["David Butler", "Adrian Hilton", "Gustavo Carneiro"], "title": "Risk Estimation of Knee Osteoarthritis Progression via Predictive Multi-task Modelling from Efficient Diffusion Model using X-ray Images", "comment": null, "summary": "Medical imaging plays a crucial role in assessing knee osteoarthritis (OA) risk by enabling early detection and disease monitoring. Recent machine learning methods have improved risk estimation (i.e., predicting the likelihood of disease progression) and predictive modelling (i.e., the forecasting of future outcomes based on current data) using medical images, but clinical adoption remains limited due to their lack of interpretability. Existing approaches that generate future images for risk estimation are complex and impractical. Additionally, previous methods fail to localize anatomical knee landmarks, limiting interpretability. We address these gaps with a new interpretable machine learning method to estimate the risk of knee OA progression via multi-task predictive modelling that classifies future knee OA severity and predicts anatomical knee landmarks from efficiently generated high-quality future images. Such image generation is achieved by leveraging a diffusion model in a class-conditioned latent space to forecast disease progression, offering a visual representation of how particular health conditions may evolve. Applied to the Osteoarthritis Initiative dataset, our approach improves the state-of-the-art (SOTA) by 2\\%, achieving an AUC of 0.71 in predicting knee OA progression while offering ~9% faster inference time."}
{"id": "2506.14634", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.14634", "abs": "https://arxiv.org/abs/2506.14634", "authors": ["Leah von der Heyde", "Anna-Carolina Haensch", "Bernd Weiß", "Jessika Daikeler"], "title": "AIn't Nothing But a Survey? Using Large Language Models for Coding German Open-Ended Survey Responses on Survey Motivation", "comment": "to appear in Survey Research Methods", "summary": "The recent development and wider accessibility of LLMs have spurred discussions about how they can be used in survey research, including classifying open-ended survey responses. Due to their linguistic capacities, it is possible that LLMs are an efficient alternative to time-consuming manual coding and the pre-training of supervised machine learning models. As most existing research on this topic has focused on English-language responses relating to non-complex topics or on single LLMs, it is unclear whether its findings generalize and how the quality of these classifications compares to established methods. In this study, we investigate to what extent different LLMs can be used to code open-ended survey responses in other contexts, using German data on reasons for survey participation as an example. We compare several state-of-the-art LLMs and several prompting approaches, and evaluate the LLMs' performance by using human expert codings. Overall performance differs greatly between LLMs, and only a fine-tuned LLM achieves satisfactory levels of predictive performance. Performance differences between prompting approaches are conditional on the LLM used. Finally, LLMs' unequal classification performance across different categories of reasons for survey participation results in different categorical distributions when not using fine-tuning. We discuss the implications of these findings, both for methodological research on coding open-ended responses and for their substantive analysis, and for practitioners processing or substantively analyzing such data. Finally, we highlight the many trade-offs researchers need to consider when choosing automated methods for open-ended response classification in the age of LLMs. In doing so, our study contributes to the growing body of research about the conditions under which LLMs can be efficiently, accurately, and reliably leveraged in survey research."}
{"id": "2506.14583", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14583", "abs": "https://arxiv.org/abs/2506.14583", "authors": ["Krishna Sahukara", "Zineddine Bettouche", "Andreas Fischer"], "title": "Synthetic Data Augmentation for Table Detection: Re-evaluating TableNet's Performance with Automatically Generated Document Images", "comment": null, "summary": "Document pages captured by smartphones or scanners often contain tables, yet manual extraction is slow and error-prone. We introduce an automated LaTeX-based pipeline that synthesizes realistic two-column pages with visually diverse table layouts and aligned ground-truth masks. The generated corpus augments the real-world Marmot benchmark and enables a systematic resolution study of TableNet. Training TableNet on our synthetic data achieves a pixel-wise XOR error of 4.04% on our synthetic test set with a 256x256 input resolution, and 4.33% with 1024x1024. The best performance on the Marmot benchmark is 9.18% (at 256x256), while cutting manual annotation effort through automation."}
{"id": "2506.14641", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.14641", "abs": "https://arxiv.org/abs/2506.14641", "authors": ["Xiang Cheng", "Chengyan Pan", "Minjun Zhao", "Deyang Li", "Fangchao Liu", "Xinyu Zhang", "Xiao Zhang", "Yong Liu"], "title": "Revisiting Chain-of-Thought Prompting: Zero-shot Can Be Stronger than Few-shot", "comment": "19 pages,22 figures", "summary": "In-Context Learning (ICL) is an essential emergent ability of Large Language Models (LLMs), and recent studies introduce Chain-of-Thought (CoT) to exemplars of ICL to enhance the reasoning capability, especially in mathematics tasks. However, given the continuous advancement of model capabilities, it remains unclear whether CoT exemplars still benefit recent, stronger models in such tasks. Through systematic experiments, we find that for recent strong models such as the Qwen2.5 series, adding traditional CoT exemplars does not improve reasoning performance compared to Zero-Shot CoT. Instead, their primary function is to align the output format with human expectations. We further investigate the effectiveness of enhanced CoT exemplars, constructed using answers from advanced models such as \\texttt{Qwen2.5-Max} and \\texttt{DeepSeek-R1}. Experimental results indicate that these enhanced exemplars still fail to improve the model's reasoning performance. Further analysis reveals that models tend to ignore the exemplars and focus primarily on the instructions, leading to no observable gain in reasoning ability. Overall, our findings highlight the limitations of the current ICL+CoT framework in mathematical reasoning, calling for a re-examination of the ICL paradigm and the definition of exemplars."}
{"id": "2506.14596", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14596", "abs": "https://arxiv.org/abs/2506.14596", "authors": ["Ming Xu", "Xu Zhang"], "title": "PoseGRAF: Geometric-Reinforced Adaptive Fusion for Monocular 3D Human Pose Estimation", "comment": null, "summary": "Existing monocular 3D pose estimation methods primarily rely on joint positional features, while overlooking intrinsic directional and angular correlations within the skeleton. As a result, they often produce implausible poses under joint occlusions or rapid motion changes. To address these challenges, we propose the PoseGRAF framework. We first construct a dual graph convolutional structure that separately processes joint and bone graphs, effectively capturing their local dependencies. A Cross-Attention module is then introduced to model interdependencies between bone directions and joint features. Building upon this, a dynamic fusion module is designed to adaptively integrate both feature types by leveraging the relational dependencies between joints and bones. An improved Transformer encoder is further incorporated in a residual manner to generate the final output. Experimental results on the Human3.6M and MPI-INF-3DHP datasets show that our method exceeds state-of-the-art approaches. Additional evaluations on in-the-wild videos further validate its generalizability. The code is publicly available at https://github.com/iCityLab/PoseGRAF."}
{"id": "2506.14645", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.14645", "abs": "https://arxiv.org/abs/2506.14645", "authors": [". Pazzaglia", "V. Vendetti", "L. D. Comencini", "F. Deriu", "V. Modugno"], "title": "Passing the Turing Test in Political Discourse: Fine-Tuning LLMs to Mimic Polarized Social Media Comments", "comment": null, "summary": "The increasing sophistication of large language models (LLMs) has sparked growing concerns regarding their potential role in exacerbating ideological polarization through the automated generation of persuasive and biased content. This study explores the extent to which fine-tuned LLMs can replicate and amplify polarizing discourse within online environments. Using a curated dataset of politically charged discussions extracted from Reddit, we fine-tune an open-source LLM to produce context-aware and ideologically aligned responses. The model's outputs are evaluated through linguistic analysis, sentiment scoring, and human annotation, with particular attention to credibility and rhetorical alignment with the original discourse. The results indicate that, when trained on partisan data, LLMs are capable of producing highly plausible and provocative comments, often indistinguishable from those written by humans. These findings raise significant ethical questions about the use of AI in political discourse, disinformation, and manipulation campaigns. The paper concludes with a discussion of the broader implications for AI governance, platform regulation, and the development of detection tools to mitigate adversarial fine-tuning risks."}
{"id": "2506.14603", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.14603", "abs": "https://arxiv.org/abs/2506.14603", "authors": ["Amirmojtaba Sabour", "Sanja Fidler", "Karsten Kreis"], "title": "Align Your Flow: Scaling Continuous-Time Flow Map Distillation", "comment": "Project page: https://research.nvidia.com/labs/toronto-ai/AlignYourFlow/", "summary": "Diffusion- and flow-based models have emerged as state-of-the-art generative modeling approaches, but they require many sampling steps. Consistency models can distill these models into efficient one-step generators; however, unlike flow- and diffusion-based methods, their performance inevitably degrades when increasing the number of steps, which we show both analytically and empirically. Flow maps generalize these approaches by connecting any two noise levels in a single step and remain effective across all step counts. In this paper, we introduce two new continuous-time objectives for training flow maps, along with additional novel training techniques, generalizing existing consistency and flow matching objectives. We further demonstrate that autoguidance can improve performance, using a low-quality model for guidance during distillation, and an additional boost can be achieved by adversarial finetuning, with minimal loss in sample diversity. We extensively validate our flow map models, called Align Your Flow, on challenging image generation benchmarks and achieve state-of-the-art few-step generation performance on both ImageNet 64x64 and 512x512, using small and efficient neural networks. Finally, we show text-to-image flow map models that outperform all existing non-adversarially trained few-step samplers in text-conditioned synthesis."}
{"id": "2506.14646", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14646", "abs": "https://arxiv.org/abs/2506.14646", "authors": ["Hengyuan Zhang", "Xinrong Chen", "Yingmin Qiu", "Xiao Liang", "Ziyue Li", "Guanyu Wang", "Weiping Li", "Tong Mo", "Wenyue Li", "Hayden Kwok-Hay So", "Ngai Wong"], "title": "GuiLoMo: Allocating Expert Number and Rank for LoRA-MoE via Bilevel Optimization with GuidedSelection Vectors", "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) methods, particularly Low-Rank Adaptation (LoRA), offer an efficient way to adapt large language models with reduced computational costs. However, their performance is limited by the small number of trainable parameters. Recent work combines LoRA with the Mixture-of-Experts (MoE), i.e., LoRA-MoE, to enhance capacity, but two limitations remain in hindering the full exploitation of its potential: 1) the influence of downstream tasks when assigning expert numbers, and 2) the uniform rank assignment across all LoRA experts, which restricts representational diversity. To mitigate these gaps, we propose GuiLoMo, a fine-grained layer-wise expert numbers and ranks allocation strategy with GuidedSelection Vectors (GSVs). GSVs are learned via a prior bilevel optimization process to capture both model- and task-specific needs, and are then used to allocate optimal expert numbers and ranks. Experiments on three backbone models across diverse benchmarks show that GuiLoMo consistently achieves superior or comparable performance to all baselines. Further analysis offers key insights into how expert numbers and ranks vary across layers and tasks, highlighting the benefits of adaptive expert configuration. Our code is available at https://github.com/Liar406/Gui-LoMo.git."}
{"id": "2506.14605", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.14605", "abs": "https://arxiv.org/abs/2506.14605", "authors": ["Giacomo Meanti", "Thomas Ryckeboer", "Michael Arbel", "Julien Mairal"], "title": "Unsupervised Imaging Inverse Problems with Diffusion Distribution Matching", "comment": "Code available at https://github.com/inria-thoth/ddm4ip", "summary": "This work addresses image restoration tasks through the lens of inverse problems using unpaired datasets. In contrast to traditional approaches -- which typically assume full knowledge of the forward model or access to paired degraded and ground-truth images -- the proposed method operates under minimal assumptions and relies only on small, unpaired datasets. This makes it particularly well-suited for real-world scenarios, where the forward model is often unknown or misspecified, and collecting paired data is costly or infeasible. The method leverages conditional flow matching to model the distribution of degraded observations, while simultaneously learning the forward model via a distribution-matching loss that arises naturally from the framework. Empirically, it outperforms both single-image blind and unsupervised approaches on deblurring and non-uniform point spread function (PSF) calibration tasks. It also matches state-of-the-art performance on blind super-resolution. We also showcase the effectiveness of our method with a proof of concept for lens calibration: a real-world application traditionally requiring time-consuming experiments and specialized equipment. In contrast, our approach achieves this with minimal data acquisition effort."}
{"id": "2506.14681", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14681", "abs": "https://arxiv.org/abs/2506.14681", "authors": ["Yuto Harada", "Yusuke Yamauchi", "Yusuke Oda", "Yohei Oseki", "Yusuke Miyao", "Yu Takagi"], "title": "Massive Supervised Fine-tuning Experiments Reveal How Data, Layer, and Training Factors Shape LLM Alignment Quality", "comment": null, "summary": "Supervised fine-tuning (SFT) is a critical step in aligning large language models (LLMs) with human instructions and values, yet many aspects of SFT remain poorly understood. We trained a wide range of base models on a variety of datasets including code generation, mathematical reasoning, and general-domain tasks, resulting in 1,000+ SFT models under controlled conditions. We then identified the dataset properties that matter most and examined the layer-wise modifications introduced by SFT. Our findings reveal that some training-task synergies persist across all models while others vary substantially, emphasizing the importance of model-specific strategies. Moreover, we demonstrate that perplexity consistently predicts SFT effectiveness--often surpassing superficial similarity between trained data and benchmark--and that mid-layer weight changes correlate most strongly with performance gains. We will release these 1,000+ SFT models and benchmark results to accelerate further research."}
{"id": "2506.14629", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14629", "abs": "https://arxiv.org/abs/2506.14629", "authors": ["Md. Adnanul Islam", "Md. Faiyaz Abdullah Sayeedi", "Md. Asaduzzaman Shuvo", "Muhammad Ziaur Rahman", "Shahanur Rahman Bappy", "Raiyan Rahman", "Swakkhar Shatabda"], "title": "VisText-Mosquito: A Multimodal Dataset and Benchmark for AI-Based Mosquito Breeding Site Detection and Reasoning", "comment": null, "summary": "Mosquito-borne diseases pose a major global health risk, requiring early detection and proactive control of breeding sites to prevent outbreaks. In this paper, we present VisText-Mosquito, a multimodal dataset that integrates visual and textual data to support automated detection, segmentation, and reasoning for mosquito breeding site analysis. The dataset includes 1,828 annotated images for object detection, 142 images for water surface segmentation, and natural language reasoning texts linked to each image. The YOLOv9s model achieves the highest precision of 0.92926 and mAP@50 of 0.92891 for object detection, while YOLOv11n-Seg reaches a segmentation precision of 0.91587 and mAP@50 of 0.79795. For reasoning generation, our fine-tuned BLIP model achieves a final loss of 0.0028, with a BLEU score of 54.7, BERTScore of 0.91, and ROUGE-L of 0.87. This dataset and model framework emphasize the theme \"Prevention is Better than Cure\", showcasing how AI-based detection can proactively address mosquito-borne disease risks. The dataset and implementation code are publicly available at GitHub: https://github.com/adnanul-islam-jisun/VisText-Mosquito"}
{"id": "2506.14702", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.14702", "abs": "https://arxiv.org/abs/2506.14702", "authors": ["Daniel D'souza", "Julia Kreutzer", "Adrien Morisot", "Ahmet Üstün", "Sara Hooker"], "title": "Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time Markers", "comment": null, "summary": "One of the most profound challenges of modern machine learning is performing well on the long-tail of rare and underrepresented features. Large general-purpose models are trained for many tasks, but work best on high-frequency use cases. After training, it is hard to adapt a model to perform well on specific use cases underrepresented in the training corpus. Relying on prompt engineering or few-shot examples to maximize the output quality on a particular test case can be frustrating, as models can be highly sensitive to small changes, react in unpredicted ways or rely on a fixed system prompt for maintaining performance. In this work, we ask: \"Can we optimize our training protocols to both improve controllability and performance on underrepresented use cases at inference time?\" We revisit the divide between training and inference techniques to improve long-tail performance while providing users with a set of control levers the model is trained to be responsive to. We create a detailed taxonomy of data characteristics and task provenance to explicitly control generation attributes and implicitly condition generations at inference time. We fine-tune a base model to infer these markers automatically, which makes them optional at inference time. This principled and flexible approach yields pronounced improvements in performance, especially on examples from the long tail of the training distribution. While we observe an average lift of 5.7% win rates in open-ended generation quality with our markers, we see over 9.1% gains in underrepresented domains. We also observe relative lifts of up to 14.1% on underrepresented tasks like CodeRepair and absolute improvements of 35.3% on length instruction following evaluations."}
{"id": "2506.14642", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14642", "abs": "https://arxiv.org/abs/2506.14642", "authors": ["Yuke Xing", "Jiarui Wang", "Peizhi Niu", "Wenjie Huang", "Guangtao Zhai", "Yiling Xu"], "title": "3DGS-IEval-15K: A Large-scale Image Quality Evaluation Database for 3D Gaussian-Splatting", "comment": null, "summary": "3D Gaussian Splatting (3DGS) has emerged as a promising approach for novel view synthesis, offering real-time rendering with high visual fidelity. However, its substantial storage requirements present significant challenges for practical applications. While recent state-of-the-art (SOTA) 3DGS methods increasingly incorporate dedicated compression modules, there is a lack of a comprehensive framework to evaluate their perceptual impact. Therefore we present 3DGS-IEval-15K, the first large-scale image quality assessment (IQA) dataset specifically designed for compressed 3DGS representations. Our dataset encompasses 15,200 images rendered from 10 real-world scenes through 6 representative 3DGS algorithms at 20 strategically selected viewpoints, with different compression levels leading to various distortion effects. Through controlled subjective experiments, we collect human perception data from 60 viewers. We validate dataset quality through scene diversity and MOS distribution analysis, and establish a comprehensive benchmark with 30 representative IQA metrics covering diverse types. As the largest-scale 3DGS quality assessment dataset to date, our work provides a foundation for developing 3DGS specialized IQA metrics, and offers essential data for investigating view-dependent quality distribution patterns unique to 3DGS. The database is publicly available at https://github.com/YukeXing/3DGS-IEval-15K."}
{"id": "2506.14704", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14704", "abs": "https://arxiv.org/abs/2506.14704", "authors": ["Anton Changalidis", "Aki Härmä"], "title": "Capacity Matters: a Proof-of-Concept for Transformer Memorization on Real-World Data", "comment": "This work has been accepted for publication at the First Workshop on Large Language Model Memorization (L2M2) at ACL 2025, Vienna, Austria", "summary": "This paper studies how the model architecture and data configurations influence the empirical memorization capacity of generative transformers. The models are trained using synthetic text datasets derived from the Systematized Nomenclature of Medicine (SNOMED) knowledge graph: triplets, representing static connections, and sequences, simulating complex relation patterns. The results show that embedding size is the primary determinant of learning speed and capacity, while additional layers provide limited benefits and may hinder performance on simpler datasets. Activation functions play a crucial role, and Softmax demonstrates greater stability and capacity. Furthermore, increasing the complexity of the data set seems to improve the final memorization. These insights improve our understanding of transformer memory mechanisms and provide a framework for optimizing model design with structured real-world data."}
{"id": "2506.14667", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14667", "abs": "https://arxiv.org/abs/2506.14667", "authors": ["Matt Poyser", "Toby P. Breckon"], "title": "DDS-NAS: Dynamic Data Selection within Neural Architecture Search via On-line Hard Example Mining applied to Image Classification", "comment": "27 single-column pages, 8 figures, to be published in Pattern Recognition", "summary": "In order to address the scalability challenge within Neural Architecture Search (NAS), we speed up NAS training via dynamic hard example mining within a curriculum learning framework. By utilizing an autoencoder that enforces an image similarity embedding in latent space, we construct an efficient kd-tree structure to order images by furthest neighbour dissimilarity in a low-dimensional embedding. From a given query image from our subsample dataset, we can identify the most dissimilar image within the global dataset in logarithmic time. Via curriculum learning, we then dynamically re-formulate an unbiased subsample dataset for NAS optimisation, upon which the current NAS solution architecture performs poorly. We show that our DDS-NAS framework speeds up gradient-based NAS strategies by up to 27x without loss in performance. By maximising the contribution of each image sample during training, we reduce the duration of a NAS training cycle and the number of iterations required for convergence."}
{"id": "2506.14731", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14731", "abs": "https://arxiv.org/abs/2506.14731", "authors": ["Ring Team", "Bin Hu", "Cai Chen", "Deng Zhao", "Ding Liu", "Dingnan Jin", "Feng Zhu", "Hao Dai", "Hongzhi Luan", "Jia Guo", "Jiaming Liu", "Jiewei Wu", "Jun Mei", "Jun Zhou", "Junbo Zhao", "Junwu Xiong", "Kaihong Zhang", "Kuan Xu", "Lei Liang", "Liang Jiang", "Liangcheng Fu", "Longfei Zheng", "Qiang Gao", "Qing Cui", "Quan Wan", "Shaomian Zheng", "Shuaicheng Li", "Tongkai Yang", "Wang Ren", "Xiaodong Yan", "Xiaopei Wan", "Xiaoyun Feng", "Xin Zhao", "Xinxing Yang", "Xinyu Kong", "Xuemin Yang", "Yang Li", "Yingting Wu", "Yongkang Liu", "Zhankai Xu", "Zhenduo Zhang", "Zhenglei Zhou", "Zhenyu Huang", "Zhiqiang Zhang", "Zihao Wang", "Zujie Wen"], "title": "Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning for LLMs", "comment": "Technical Report", "summary": "We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model optimized via reinforcement learning (RL) to achieve efficient and robust reasoning capabilities. Built upon the publicly available Ling-lite model, a 16.8 billion parameter model with 2.75 billion activated parameters, our approach matches the performance of state-of-the-art (SOTA) small-scale reasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench, GPQA-Diamond) while activating only one-third of the parameters required by comparable models. To accomplish this, we introduce a joint training pipeline integrating distillation with RL, revealing undocumented challenges in MoE RL training. First, we identify optimization instability during RL training, and we propose Constrained Contextual Computation Policy Optimization(C3PO), a novel approach that enhances training stability and improves computational throughput via algorithm-system co-design methodology. Second, we empirically demonstrate that selecting distillation checkpoints based on entropy loss for RL training, rather than validation metrics, yields superior performance-efficiency trade-offs in subsequent RL training. Finally, we develop a two-stage training paradigm to harmonize multi-domain data integration, addressing domain conflicts that arise in training with mixed dataset. We will release the model, dataset, and code."}
{"id": "2506.14674", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14674", "abs": "https://arxiv.org/abs/2506.14674", "authors": ["Ling Li", "Yao Zhou", "Yuxuan Liang", "Fugee Tsung", "Jiaheng Wei"], "title": "Recognition through Reasoning: Reinforcing Image Geo-localization with Large Vision-Language Models", "comment": null, "summary": "Previous methods for image geo-localization have typically treated the task as either classification or retrieval, often relying on black-box decisions that lack interpretability. The rise of large vision-language models (LVLMs) has enabled a rethinking of geo-localization as a reasoning-driven task grounded in visual cues. However, two major challenges persist. On the data side, existing reasoning-focused datasets are primarily based on street-view imagery, offering limited scene diversity and constrained viewpoints. On the modeling side, current approaches predominantly rely on supervised fine-tuning, which yields only marginal improvements in reasoning capabilities. To address these challenges, we propose a novel pipeline that constructs a reasoning-oriented geo-localization dataset, MP16-Reason, using diverse social media images. We introduce GLOBE, Group-relative policy optimization for Locatability assessment and Optimized visual-clue reasoning, yielding Bi-objective geo-Enhancement for the VLM in recognition and reasoning. GLOBE incorporates task-specific rewards that jointly enhance locatability assessment, visual clue reasoning, and geolocation accuracy. Both qualitative and quantitative results demonstrate that GLOBE outperforms state-of-the-art open-source LVLMs on geo-localization tasks, particularly in diverse visual scenes, while also generating more insightful and interpretable reasoning trajectories."}
{"id": "2506.14758", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14758", "abs": "https://arxiv.org/abs/2506.14758", "authors": ["Daixuan Cheng", "Shaohan Huang", "Xuekai Zhu", "Bo Dai", "Wayne Xin Zhao", "Zhenliang Zhang", "Furu Wei"], "title": "Reasoning with Exploration: An Entropy Perspective", "comment": null, "summary": "Balancing exploration and exploitation is a central goal in reinforcement learning (RL). Despite recent advances in enhancing language model (LM) reasoning, most methods lean toward exploitation, and increasingly encounter performance plateaus. In this work, we revisit entropy -- a signal of exploration in RL -- and examine its relationship to exploratory reasoning in LMs. Through empirical analysis, we uncover strong positive correlations between high-entropy regions and three types of exploratory reasoning actions: (1) pivotal tokens that determine or connect logical steps, (2) reflective actions such as self-verification and correction, and (3) rare behaviors under-explored by the base LMs. Motivated by this, we introduce a minimal modification to standard RL with only one line of code: augmenting the advantage function with an entropy-based term. Unlike traditional maximum-entropy methods which encourage exploration by promoting uncertainty, we encourage exploration by promoting longer and deeper reasoning chains. Notably, our method achieves significant gains on the Pass@K metric -- an upper-bound estimator of LM reasoning capabilities -- even when evaluated with extremely large K values, pushing the boundaries of LM reasoning."}
{"id": "2506.14686", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14686", "abs": "https://arxiv.org/abs/2506.14686", "authors": ["Xi Chen", "Hengshuang Zhao"], "title": "FocalClick-XL: Towards Unified and High-quality Interactive Segmentation", "comment": null, "summary": "Interactive segmentation enables users to extract binary masks of target objects through simple interactions such as clicks, scribbles, and boxes. However, existing methods often support only limited interaction forms and struggle to capture fine details. In this paper, we revisit the classical coarse-to-fine design of FocalClick and introduce significant extensions. Inspired by its multi-stage strategy, we propose a novel pipeline, FocalClick-XL, to address these challenges simultaneously. Following the emerging trend of large-scale pretraining, we decompose interactive segmentation into meta-tasks that capture different levels of information -- context, object, and detail -- assigning a dedicated subnet to each level.This decomposition allows each subnet to undergo scaled pretraining with independent data and supervision, maximizing its effectiveness. To enhance flexibility, we share context- and detail-level information across different interaction forms as common knowledge while introducing a prompting layer at the object level to encode specific interaction types. As a result, FocalClick-XL achieves state-of-the-art performance on click-based benchmarks and demonstrates remarkable adaptability to diverse interaction formats, including boxes, scribbles, and coarse masks. Beyond binary mask generation, it is also capable of predicting alpha mattes with fine-grained details, making it a versatile and powerful tool for interactive segmentation."}
{"id": "2506.14761", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14761", "abs": "https://arxiv.org/abs/2506.14761", "authors": ["Mathurin Videau", "Badr Youbi Idrissi", "Alessandro Leite", "Marc Schoenauer", "Olivier Teytaud", "David Lopez-Paz"], "title": "From Bytes to Ideas: Language Modeling with Autoregressive U-Nets", "comment": null, "summary": "Tokenization imposes a fixed granularity on the input text, freezing how a language model operates on data and how far in the future it predicts. Byte Pair Encoding (BPE) and similar schemes split text once, build a static vocabulary, and leave the model stuck with that choice. We relax this rigidity by introducing an autoregressive U-Net that learns to embed its own tokens as it trains. The network reads raw bytes, pools them into words, then pairs of words, then up to 4 words, giving it a multi-scale view of the sequence. At deeper stages, the model must predict further into the future -- anticipating the next few words rather than the next byte -- so deeper stages focus on broader semantic patterns while earlier stages handle fine details. When carefully tuning and controlling pretraining compute, shallow hierarchies tie strong BPE baselines, and deeper hierarchies have a promising trend. Because tokenization now lives inside the model, the same system can handle character-level tasks and carry knowledge across low-resource languages."}
{"id": "2506.14696", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14696", "abs": "https://arxiv.org/abs/2506.14696", "authors": ["Dahang Wan", "Rongsheng Lu", "Yang Fang", "Xianli Lang", "Shuangbao Shu", "Jingjing Chen", "Siyuan Shen", "Ting Xu", "Zecong Ye"], "title": "YOLOv11-RGBT: Towards a Comprehensive Single-Stage Multispectral Object Detection Framework", "comment": "28 pages, 8 figures", "summary": "Multispectral object detection, which integrates information from multiple bands, can enhance detection accuracy and environmental adaptability, holding great application potential across various fields. Although existing methods have made progress in cross-modal interaction, low-light conditions, and model lightweight, there are still challenges like the lack of a unified single-stage framework, difficulty in balancing performance and fusion strategy, and unreasonable modality weight allocation. To address these, based on the YOLOv11 framework, we present YOLOv11-RGBT, a new comprehensive multimodal object detection framework. We designed six multispectral fusion modes and successfully applied them to models from YOLOv3 to YOLOv12 and RT-DETR. After reevaluating the importance of the two modalities, we proposed a P3 mid-fusion strategy and multispectral controllable fine-tuning (MCF) strategy for multispectral models. These improvements optimize feature fusion, reduce redundancy and mismatches, and boost overall model performance. Experiments show our framework excels on three major open-source multispectral object detection datasets, like LLVIP and FLIR. Particularly, the multispectral controllable fine-tuning strategy significantly enhanced model adaptability and robustness. On the FLIR dataset, it consistently improved YOLOv11 models' mAP by 3.41%-5.65%, reaching a maximum of 47.61%, verifying the framework and strategies' effectiveness. The code is available at: https://github.com/wandahangFY/YOLOv11-RGBT."}
{"id": "2506.14767", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.14767", "abs": "https://arxiv.org/abs/2506.14767", "authors": ["Li-Wei Chen", "Takuya Higuchi", "Zakaria Aldeneh", "Ahmed Hussen Abdelaziz", "Alexander Rudnicky"], "title": "A Variational Framework for Improving Naturalness in Generative Spoken Language Models", "comment": "International Conference on Machine Learning (ICML) 2025", "summary": "The success of large language models in text processing has inspired their adaptation to speech modeling. However, since speech is continuous and complex, it is often discretized for autoregressive modeling. Speech tokens derived from self-supervised models (known as semantic tokens) typically focus on the linguistic aspects of speech but neglect prosodic information. As a result, models trained on these tokens can generate speech with reduced naturalness. Existing approaches try to fix this by adding pitch features to the semantic tokens. However, pitch alone cannot fully represent the range of paralinguistic attributes, and selecting the right features requires careful hand-engineering. To overcome this, we propose an end-to-end variational approach that automatically learns to encode these continuous speech attributes to enhance the semantic tokens. Our approach eliminates the need for manual extraction and selection of paralinguistic features. Moreover, it produces preferred speech continuations according to human raters. Code, samples and models are available at https://github.com/b04901014/vae-gslm."}
{"id": "2506.14706", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14706", "abs": "https://arxiv.org/abs/2506.14706", "authors": ["Ni Ou", "Zhuo Chen", "Xinru Zhang", "Junzheng Wang"], "title": "Iterative Camera-LiDAR Extrinsic Optimization via Surrogate Diffusion", "comment": "7 pages, 4 figures, accepted by IROS 2025", "summary": "Cameras and LiDAR are essential sensors for autonomous vehicles. The fusion of camera and LiDAR data addresses the limitations of individual sensors but relies on precise extrinsic calibration. Recently, numerous end-to-end calibration methods have been proposed; however, most predict extrinsic parameters in a single step and lack iterative optimization capabilities. To address the increasing demand for higher accuracy, we propose a versatile iterative framework based on surrogate diffusion. This framework can enhance the performance of any calibration method without requiring architectural modifications. Specifically, the initial extrinsic parameters undergo iterative refinement through a denoising process, in which the original calibration method serves as a surrogate denoiser to estimate the final extrinsics at each step. For comparative analysis, we selected four state-of-the-art calibration methods as surrogate denoisers and compared the results of our diffusion process with those of two other iterative approaches. Extensive experiments demonstrate that when integrated with our diffusion model, all calibration methods achieve higher accuracy, improved robustness, and greater stability compared to other iterative techniques and their single-step counterparts."}
{"id": "2506.14142", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14142", "abs": "https://arxiv.org/abs/2506.14142", "authors": ["Wenting Chen", "Yi Dong", "Zhaojun Ding", "Yucheng Shi", "Yifan Zhou", "Fang Zeng", "Yijun Luo", "Tianyu Lin", "Yihang Su", "Yichen Wu", "Kai Zhang", "Zhen Xiang", "Tianming Liu", "Ninghao Liu", "Lichao Sun", "Yixuan Yuan", "Xiang Li"], "title": "RadFabric: Agentic AI System with Reasoning Capability for Radiology", "comment": "4 figures, 2 tables", "summary": "Chest X ray (CXR) imaging remains a critical diagnostic tool for thoracic conditions, but current automated systems face limitations in pathology coverage, diagnostic accuracy, and integration of visual and textual reasoning. To address these gaps, we propose RadFabric, a multi agent, multimodal reasoning framework that unifies visual and textual analysis for comprehensive CXR interpretation. RadFabric is built on the Model Context Protocol (MCP), enabling modularity, interoperability, and scalability for seamless integration of new diagnostic agents. The system employs specialized CXR agents for pathology detection, an Anatomical Interpretation Agent to map visual findings to precise anatomical structures, and a Reasoning Agent powered by large multimodal reasoning models to synthesize visual, anatomical, and clinical data into transparent and evidence based diagnoses. RadFabric achieves significant performance improvements, with near-perfect detection of challenging pathologies like fractures (1.000 accuracy) and superior overall diagnostic accuracy (0.799) compared to traditional systems (0.229 to 0.527). By integrating cross modal feature alignment and preference-driven reasoning, RadFabric advances AI-driven radiology toward transparent, anatomically precise, and clinically actionable CXR analysis."}
{"id": "2506.14709", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.14709", "abs": "https://arxiv.org/abs/2506.14709", "authors": ["Kunal Swami", "Debtanu Gupta", "Amrit Kumar Muduli", "Chirag Jaiswal", "Pankaj Kumar Bajpai"], "title": "DiFuse-Net: RGB and Dual-Pixel Depth Estimation using Window Bi-directional Parallax Attention and Cross-modal Transfer Learning", "comment": "Accepted in IROS 2025", "summary": "Depth estimation is crucial for intelligent systems, enabling applications from autonomous navigation to augmented reality. While traditional stereo and active depth sensors have limitations in cost, power, and robustness, dual-pixel (DP) technology, ubiquitous in modern cameras, offers a compelling alternative. This paper introduces DiFuse-Net, a novel modality decoupled network design for disentangled RGB and DP based depth estimation. DiFuse-Net features a window bi-directional parallax attention mechanism (WBiPAM) specifically designed to capture the subtle DP disparity cues unique to smartphone cameras with small aperture. A separate encoder extracts contextual information from the RGB image, and these features are fused to enhance depth prediction. We also propose a Cross-modal Transfer Learning (CmTL) mechanism to utilize large-scale RGB-D datasets in the literature to cope with the limitations of obtaining large-scale RGB-DP-D dataset. Our evaluation and comparison of the proposed method demonstrates its superiority over the DP and stereo-based baseline methods. Additionally, we contribute a new, high-quality, real-world RGB-DP-D training dataset, named Dual-Camera Dual-Pixel (DCDP) dataset, created using our novel symmetric stereo camera hardware setup, stereo calibration and rectification protocol, and AI stereo disparity estimation method."}
{"id": "2506.14629", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14629", "abs": "https://arxiv.org/abs/2506.14629", "authors": ["Md. Adnanul Islam", "Md. Faiyaz Abdullah Sayeedi", "Md. Asaduzzaman Shuvo", "Muhammad Ziaur Rahman", "Shahanur Rahman Bappy", "Raiyan Rahman", "Swakkhar Shatabda"], "title": "VisText-Mosquito: A Multimodal Dataset and Benchmark for AI-Based Mosquito Breeding Site Detection and Reasoning", "comment": null, "summary": "Mosquito-borne diseases pose a major global health risk, requiring early detection and proactive control of breeding sites to prevent outbreaks. In this paper, we present VisText-Mosquito, a multimodal dataset that integrates visual and textual data to support automated detection, segmentation, and reasoning for mosquito breeding site analysis. The dataset includes 1,828 annotated images for object detection, 142 images for water surface segmentation, and natural language reasoning texts linked to each image. The YOLOv9s model achieves the highest precision of 0.92926 and mAP@50 of 0.92891 for object detection, while YOLOv11n-Seg reaches a segmentation precision of 0.91587 and mAP@50 of 0.79795. For reasoning generation, our fine-tuned BLIP model achieves a final loss of 0.0028, with a BLEU score of 54.7, BERTScore of 0.91, and ROUGE-L of 0.87. This dataset and model framework emphasize the theme \"Prevention is Better than Cure\", showcasing how AI-based detection can proactively address mosquito-borne disease risks. The dataset and implementation code are publicly available at GitHub: https://github.com/adnanul-islam-jisun/VisText-Mosquito"}
{"id": "2506.14730", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14730", "abs": "https://arxiv.org/abs/2506.14730", "authors": ["Corey Scher", "Jamon Van Den Hoek"], "title": "Active InSAR monitoring of building damage in Gaza during the Israel-Hamas War", "comment": null, "summary": "Aerial bombardment of the Gaza Strip beginning October 7, 2023 is one of the most intense bombing campaigns of the twenty-first century, driving widespread urban damage. Characterizing damage over a geographically dynamic and protracted armed conflict requires active monitoring. Synthetic aperture radar (SAR) has precedence for mapping disaster-induced damage with bi-temporal methods but applications to active monitoring during sustained crises are limited. Using interferometric SAR data from Sentinel-1, we apply a long temporal-arc coherent change detection (LT-CCD) approach to track weekly damage trends over the first year of the 2023- Israel-Hamas War. We detect 92.5% of damage labels in reference data from the United Nations with a negligible (1.2%) false positive rate. The temporal fidelity of our approach reveals rapidly increasing damage during the first three months of the war focused in northern Gaza, a notable pause in damage during a temporary ceasefire, and surges of new damage as conflict hot-spots shift from north to south. Three-fifths (191,263) of all buildings are damaged or destroyed by the end of the study. With massive need for timely data on damage in armed conflict zones, our low-cost and low-latency approach enables rapid uptake of damage information at humanitarian and journalistic organizations."}
{"id": "2506.14766", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14766", "abs": "https://arxiv.org/abs/2506.14766", "authors": ["Yujun Wang", "Jinhe Bi", "Yunpu Ma", "Soeren Pirk"], "title": "ASCD: Attention-Steerable Contrastive Decoding for Reducing Hallucination in MLLM", "comment": "15 pages, 7 figures", "summary": "Multimodal Large Language Model (MLLM) often suffer from hallucinations. They over-rely on partial cues and generate incorrect responses. Recently, methods like Visual Contrastive Decoding (VCD) and Instruction Contrastive Decoding (ICD) have been proposed to mitigate hallucinations by contrasting predictions from perturbed or negatively prefixed inputs against original outputs. In this work, we uncover that methods like VCD and ICD fundamentally influence internal attention dynamics of the model. This observation suggests that their effectiveness may not stem merely from surface-level modifications to logits but from deeper shifts in attention distribution. Inspired by this insight, we propose an attention-steerable contrastive decoding framework that directly intervenes in attention mechanisms of the model to offer a more principled approach to mitigating hallucinations. Our experiments across multiple MLLM architectures and diverse decoding methods demonstrate that our approach significantly reduces hallucinations and improves the performance on benchmarks such as POPE, CHAIR, and MMHal-Bench, while simultaneously enhancing performance on standard VQA benchmarks."}
{"id": "2506.14742", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14742", "abs": "https://arxiv.org/abs/2506.14742", "authors": ["Ziqiao Peng", "Wentao Hu", "Junyuan Ma", "Xiangyu Zhu", "Xiaomei Zhang", "Hao Zhao", "Hui Tian", "Jun He", "Hongyan Liu", "Zhaoxin Fan"], "title": "SyncTalk++: High-Fidelity and Efficient Synchronized Talking Heads Synthesis Using Gaussian Splatting", "comment": null, "summary": "Achieving high synchronization in the synthesis of realistic, speech-driven talking head videos presents a significant challenge. A lifelike talking head requires synchronized coordination of subject identity, lip movements, facial expressions, and head poses. The absence of these synchronizations is a fundamental flaw, leading to unrealistic results. To address the critical issue of synchronization, identified as the ''devil'' in creating realistic talking heads, we introduce SyncTalk++, which features a Dynamic Portrait Renderer with Gaussian Splatting to ensure consistent subject identity preservation and a Face-Sync Controller that aligns lip movements with speech while innovatively using a 3D facial blendshape model to reconstruct accurate facial expressions. To ensure natural head movements, we propose a Head-Sync Stabilizer, which optimizes head poses for greater stability. Additionally, SyncTalk++ enhances robustness to out-of-distribution (OOD) audio by incorporating an Expression Generator and a Torso Restorer, which generate speech-matched facial expressions and seamless torso regions. Our approach maintains consistency and continuity in visual details across frames and significantly improves rendering speed and quality, achieving up to 101 frames per second. Extensive experiments and user studies demonstrate that SyncTalk++ outperforms state-of-the-art methods in synchronization and realism. We recommend watching the supplementary video: https://ziqiaopeng.github.io/synctalk++."}
{"id": "2506.14753", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.14753", "abs": "https://arxiv.org/abs/2506.14753", "authors": ["Qinchan", "Li", "Kenneth Chen", "Changyue", "Su", "Wittawat Jitkrittum", "Qi Sun", "Patsorn Sangkloy"], "title": "Cost-Aware Routing for Efficient Text-To-Image Generation", "comment": null, "summary": "Diffusion models are well known for their ability to generate a high-fidelity image for an input prompt through an iterative denoising process. Unfortunately, the high fidelity also comes at a high computational cost due the inherently sequential generative process. In this work, we seek to optimally balance quality and computational cost, and propose a framework to allow the amount of computation to vary for each prompt, depending on its complexity. Each prompt is automatically routed to the most appropriate text-to-image generation function, which may correspond to a distinct number of denoising steps of a diffusion model, or a disparate, independent text-to-image model. Unlike uniform cost reduction techniques (e.g., distillation, model quantization), our approach achieves the optimal trade-off by learning to reserve expensive choices (e.g., 100+ denoising steps) only for a few complex prompts, and employ more economical choices (e.g., small distilled model) for less sophisticated prompts. We empirically demonstrate on COCO and DiffusionDB that by learning to route to nine already-trained text-to-image models, our approach is able to deliver an average quality that is higher than that achievable by any of these models alone."}
{"id": "2506.14765", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14765", "abs": "https://arxiv.org/abs/2506.14765", "authors": ["Nikolaos Dionelis", "Jente Bosmans", "Riccardo Musto", "Giancarlo Paoletti", "Simone Sarti", "Giacomo Cascarano", "Casper Fibaek", "Luke Camilleri", "Bertrand Le Saux", "Nicolas Longépé"], "title": "Scaling-Up the Pretraining of the Earth Observation Foundation Model PhilEO to the MajorTOM Dataset", "comment": "6 pages, 9 figures, 1 table, 29 references", "summary": "Today, Earth Observation (EO) satellites generate massive volumes of data, with the Copernicus Sentinel-2 constellation alone producing approximately 1.6TB per day. To fully exploit this information, it is essential to pretrain EO Foundation Models (FMs) on large unlabeled datasets, enabling efficient fine-tuning for several different downstream tasks with minimal labeled data. In this work, we present the scaling-up of our recently proposed EO Foundation Model, PhilEO Geo-Aware U-Net, on the unlabeled 23TB dataset MajorTOM, which covers the vast majority of the Earth's surface, as well as on the specialized subset FastTOM 2TB that does not include oceans and ice. We develop and study various PhilEO model variants with different numbers of parameters and architectures. Finally, we fine-tune the models on the PhilEO Bench for road density estimation, building density pixel-wise regression, and land cover semantic segmentation, and we evaluate the performance. Our results demonstrate that for all n-shots for road density regression, the PhilEO 44M MajorTOM 23TB model outperforms PhilEO Globe 0.5TB 44M. We also show that for most n-shots for road density estimation and building density regression, PhilEO 200M FastTOM outperforms all the other models. The effectiveness of both dataset and model scaling is validated using the PhilEO Bench. We also study the impact of architecture scaling, transitioning from U-Net Convolutional Neural Networks (CNN) to Vision Transformers (ViT)."}
{"id": "2506.14766", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14766", "abs": "https://arxiv.org/abs/2506.14766", "authors": ["Yujun Wang", "Jinhe Bi", "Yunpu Ma", "Soeren Pirk"], "title": "ASCD: Attention-Steerable Contrastive Decoding for Reducing Hallucination in MLLM", "comment": "15 pages, 7 figures", "summary": "Multimodal Large Language Model (MLLM) often suffer from hallucinations. They over-rely on partial cues and generate incorrect responses. Recently, methods like Visual Contrastive Decoding (VCD) and Instruction Contrastive Decoding (ICD) have been proposed to mitigate hallucinations by contrasting predictions from perturbed or negatively prefixed inputs against original outputs. In this work, we uncover that methods like VCD and ICD fundamentally influence internal attention dynamics of the model. This observation suggests that their effectiveness may not stem merely from surface-level modifications to logits but from deeper shifts in attention distribution. Inspired by this insight, we propose an attention-steerable contrastive decoding framework that directly intervenes in attention mechanisms of the model to offer a more principled approach to mitigating hallucinations. Our experiments across multiple MLLM architectures and diverse decoding methods demonstrate that our approach significantly reduces hallucinations and improves the performance on benchmarks such as POPE, CHAIR, and MMHal-Bench, while simultaneously enhancing performance on standard VQA benchmarks."}
{"id": "2506.14769", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.14769", "abs": "https://arxiv.org/abs/2506.14769", "authors": ["Jiahua Ma", "Yiran Qin", "Yixiong Li", "Xuanqi Liao", "Yulan Guo", "Ruimao Zhang"], "title": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal Diffusion", "comment": null, "summary": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating expert demonstrations through action diffusion. However, in practical applications, hardware limitations often degrade data quality, while real-time constraints restrict model inference to instantaneous state and scene observations. These limitations seriously reduce the efficacy of learning from expert demonstrations, resulting in failures in object localization, grasp planning, and long-horizon task execution. To address these challenges, we propose Causal Diffusion Policy (CDP), a novel transformer-based diffusion model that enhances action prediction by conditioning on historical action sequences, thereby enabling more coherent and context-aware visuomotor policy learning. To further mitigate the computational cost associated with autoregressive inference, a caching mechanism is also introduced to store attention key-value pairs from previous timesteps, substantially reducing redundant computations during execution. Extensive experiments in both simulated and real-world environments, spanning diverse 2D and 3D manipulation tasks, demonstrate that CDP uniquely leverages historical action sequences to achieve significantly higher accuracy than existing methods. Moreover, even when faced with degraded input observation quality, CDP maintains remarkable precision by reasoning through temporal continuity, which highlights its practical robustness for robotic control under realistic, imperfect conditions."}
{"id": "2506.13888", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.13888", "abs": "https://arxiv.org/abs/2506.13888", "authors": ["Jipeng Zhang", "Kehao Miao", "Renjie Pi", "Zhaowei Wang", "Runtao Liu", "Rui Pan", "Tong Zhang"], "title": "VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative Training", "comment": null, "summary": "Reinforcement Fine-Tuning (RFT) with verifiable rewards has advanced large language models but remains underexplored for Vision-Language (VL) models. The Vision-Language Reward Model (VL-RM) is key to aligning VL models by providing structured feedback, yet training effective VL-RMs faces two major challenges. First, the bootstrapping dilemma arises as high-quality training data depends on already strong VL models, creating a cycle where self-generated supervision reinforces existing biases. Second, modality bias and negative example amplification occur when VL models hallucinate incorrect visual attributes, leading to flawed preference data that further misguides training. To address these issues, we propose an iterative training framework leveraging vision experts, Chain-of-Thought (CoT) rationales, and Margin-based Rejection Sampling. Our approach refines preference datasets, enhances structured critiques, and iteratively improves reasoning. Experiments across VL-RM benchmarks demonstrate superior performance in hallucination detection and multimodal reasoning, advancing VL model alignment with reinforcement learning."}
